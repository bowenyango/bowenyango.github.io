[{"id":0,"href":"/docs/data-store-and-management/caching/","title":"Caching","section":"Docs","content":"Caching\n"},{"id":1,"href":"/docs/block-chain/","title":"Block Chain","section":"Docs","content":"Block Chain\n"},{"id":2,"href":"/docs/book-reading/system-design-alex-xu/","title":"System Design Alex Xu","section":"Book Reading","content":" Chapter One: SCALE FROM ZERO TO MILLIONS OF USERS # Cache Eviction Policy # Eviction Policy: Once the cache is full, any requests to add items to the cache might cause existing items to be removed. This is called cache eviction. Least-recently-used (LRU) is the most popular cache eviction policy. Other eviction policies, such as the Least Frequently Used (LFU) or First in First Out (FIFO), can be adopted to satisfy different use cases.\nTODO: Implementation? # Dynamic Content Caching in CDN # TODO\nhttps://aws.amazon.com/cloudfront/dynamic-content/?nc1=h_ls\nStateless architecture VS Stateful architecture # Stateful architecture: The issue is that every request from the same client must be routed to the same server. This can be done with sticky sessions in most load balancers.\nTODO: Is there any real-world use case using stateful server.\nStateless architecture: In this stateless architecture, HTTP requests from users can be sent to any web servers, which fetch state data from a shared data store. State data is stored in a shared data store and kept out of web servers.\nStore Session Data in a NoSQL DB QUESTION: NoSQL is easier to scale than Traditional SQL? # How the database is synchronized in different data center in different physical regions? # https://netflixtechblog.com/active-active-for-multi-regional-resiliency-c47719f6685b\nDatabase Sharding # Each shard shares the same schema, though the actual data on each shard is unique to the shard.\nSimple Hash Sharding: Hash(Partition Key) % NumOfShard -\u0026gt; Different Shard\nNew Challenges Introduced:\nResharding data: Consistent hashing Celebrity problem: Some shards may be accessed more frequently than others Join and de-normalization: A common workaround is to denormalize the database so that queries can be performed in a single table. Reference: [1] Hypertext Transfer Protocol: https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol [2] Should you go Beyond Relational Databases?: https://blog.teamtreehouse.com/should-you-go-beyond-relational-databases [3] Replication: https://en.wikipedia.org/wiki/Replication_(computing) [4] Multi-master replication: https://en.wikipedia.org/wiki/Multi-master_replication [5] NDB Cluster Replication: Multi-Master and Circular Replication: https://dev.mysql.com/doc/refman/5.7/en/mysql-cluster-replication-multi-master.html [6] Caching Strategies and How to Choose the Right One: https://codeahoy.com/2017/08/11/caching-strategies-and-how-to-choose-the-right-one/ [7] R. Nishtala, \u0026#34;Facebook, Scaling Memcache at,\u0026#34; 10th USENIX Symposium on Networked Systems Design and Implementation (NSDI ’13). [8] Single point of failure: https://en.wikipedia.org/wiki/Single_point_of_failure [9] Amazon CloudFront Dynamic Content Delivery: https://aws.amazon.com/cloudfront/dynamic-content/ [10] Configure Sticky Sessions for Your Classic Load Balancer: https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html [11] Active-Active for Multi-Regional Resiliency: https://netflixtechblog.com/active-active-for-multi-regional-resiliency-c47719f6685b [12] Amazon EC2 High Memory Instances: https://aws.amazon.com/ec2/instance-types/high-memory/ [13] What it takes to run Stack Overflow: http://nickcraver.com/blog/2013/11/22/what-it-takes-to-run-stack-overflow [14] What The Heck Are You Actually Using NoSQL For: http://highscalability.com/blog/2010/12/6/what-the-heck-are-you-actually-using-nosqlfor.html Chapter Two: BACK-OF-THE-ENVELOPE ESTIMATION # Latency # Reference: https://colin-scott.github.io/personal_website/research/interactive_latency.html\nService Level Agreement(SLA) # Make Clear Assumptions during Interviews # QPS peak QPS storage cache number of servers CHAPTER 3: A FRAMEWORK FOR SYSTEM DESIGN INTERVIEWS # What is expected in a system design interview? # Technical skills The ability to cooperate The ability to ask good questions Real world trade-offs A 4-step process for effective system design interview # Step 1 - Understand the problem and establish design scope # Slow down. Think deeply and ask questions to clarify requirements and assumptions. What Features? -\u0026gt; What microservice we need + Database Schema + Data Limitation + Data Flow + API endpoints(Spec) What Size? (How many users and what frequency) -\u0026gt; Algorithm Complexity + TPS + Storage + Latency Level of Consistency? -\u0026gt; Weak or Strong Current technology stack? -\u0026gt; If there is something you can leverage to simplify the design Write down your assumptions on the whiteboard Step 2 - Propose high-level design and get buy-in # Come up with an initial blueprint for the design. Ask for feedback. Treat your interviewer as a teammate and work together. Many good interviewers love to talk and get involved. Draw box diagrams with key components on the whiteboard or paper. This might include clients (mobile/web), APIs, web servers, data stores, cache, CDN, message queue, etc. Do back-of-the-envelope calculations to evaluate if your blueprint fits the scale constraints. Think out loud. Communicate with your interviewer if back-of-the-envelope is necessary before diving into it. Go through a few concrete use cases to discover edge cases Step 3 - Design deep dive # You shall work with the interviewer to identify and prioritize components in the architecture. Different interviewers may have different preferences on different systems. For URL shortener, it is interesting to dive into the hash function design that converts a long URL to a short one. For a chat system, how to reduce latency and how to support online/offline status are two interesting topics. For an e-commerce platform, it is interesting to dive into inventory management and the order processing workflow. For a social media platform, how to handle real-time updates for the feed and notifications, and how to implement data partitioning and sharding are two interesting topics. For a video streaming service, it is interesting to dive into streaming protocols and techniques for reducing latency. For a ride-sharing service, how to handle real-time ride tracking and how to implement the matching algorithm and surge pricing are two interesting topics. For an online marketplace, it is interesting to dive into search and recommendation algorithms and fraud detection and prevention. For a cloud storage service, how to ensure data redundancy and replication and how to manage efficient file storage and retrieval are two interesting topics. For a content management system (CMS), it is interesting to dive into user roles and permissions and performance optimization for content delivery. Try not to get into unnecessary details. Step 4 - Wrap up # In this final step, the interviewer might ask you a few follow-up questions or give you the freedom to discuss other additional points.\nThe interviewer might want you to identify the system bottlenecks and discuss potential improvements. It could be useful to give the interviewer a recap of your design. Error cases (server failure, network loss, etc.) are interesting to talk about. Operation issues are worth mentioning. How do you monitor metrics and error logs? How to roll out the system? How to handle the next scale curve is also an interesting topic. Dos # Always ask for clarification. Do not assume your assumption is correct. Understand the requirements of the problem. There is neither the right answer nor the best answer. A solution designed to solve the problems of a young startup is different from that of an established company with millions of users. Make sure you understand the requirements. Let the interviewer know what you are thinking. Communicate with your interview. Suggest multiple approaches if possible. Once you agree with your interviewer on the blueprint, go into details on each component. Design the most critical components first. Bounce ideas off the interviewer. A good interviewer works with you as a teammate. Never give up. Don\u0026rsquo;ts # Don\u0026rsquo;t be unprepared for typical interview questions. Don’t jump into a solution without clarifying the requirements and assumptions. Don’t go into too much detail on a single component in the beginning. Give the high-level design first then drills down. If you get stuck, don\u0026rsquo;t hesitate to ask for hints. Again, communicate. Don\u0026rsquo;t think in silence. Don’t think your interview is done once you give the design. You are not done until your interviewer says you are done. Ask for feedback early and often. CHAPTER 4: DESIGN A RATE LIMITER # The benefits of using an API rate limiter:\nPrevent resource starvation caused by Denial of Service (DoS) attack. Reduce cost. Limiting excess requests means fewer servers and allocating more resources to high priority APIs. Prevent servers from being overloaded. Step 1 - Understand the problem and establish design scope # What kind of rate limiter are we going to design? Is it a client-side rate limiter (written in JS and can be bypassed by users who manipulate the code) or server-side API rate limiter? Does the rate limiter throttle API requests based on IP, the user ID, or other properties? What is the scale of the system? Is it built for a startup or a big company with a large user base? Will the system work in a distributed environment? Is the rate limiter a separate service or should it be implemented in application code? Do we need to inform users who are throttled? Summary of the requirements:\nAccurately limit excessive requests. Low latency. The rate limiter should not slow down HTTP response time. Use as little memory as possible. Distributed rate limiting. The rate limiter can be shared across multiple servers or processes. Exception handling. Show clear exceptions to users when their requests are throttled. High fault tolerance. If there are any problems with the rate limiter (for example, a cache server goes offline), it does not affect the entire system. Step 2 - Propose high-level design and get buy-in # Where to implement the rate limiter: Client Side(not reliable) Server Side(Inside API Servers) Microservice in the middle: rate limiting is usually implemented within a component called API gateway So where should the rater limiter be implemented, on the server-side or in a gateway? There is no absolute answer.\nWhat technology stacks to use: Cache service: Redis/Memcache Rate Limiting Rules: QPS by account, IP Algorithm for rate limiting: Token bucket Bucket size: the maximum number of tokens allowed in the bucket Refill rate: number of tokens put into the bucket every second How many buckets do we need? Improvement: Lazy Refilling(Refill when receiving new requests) + Automatic Cleanup Better in scenarios requiring instant responses. AWS API Gateway: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html Stripe: https://stripe.com/blog/rate-limiters Leaking bucket Bucket size: it is equal to the queue size. The queue holds the requests to be processed at a fixed rate. Outflow rate: it defines how many requests can be processed at a fixed rate, usually in seconds. But the requests at the end of the queue will be delayed. Better in scenarios requiring predictable processing rate. Shopify: https://shopify.dev/docs/api/usage/rate-limits Fixed window counter A counter used in each fixed time window(per minute, pet hour\u0026hellip;) Pros: simplicity and ease of implementation Cons: uneven request distribution near the boundary of time windows. Widely used in a SaaS platform subscription service.(10000 API calls per month) Better for time-sensitive scenarios like subscription, game, voting systems, etc. Sliding window log Rate limiting implemented by this algorithm is very accurate. Require lots of memory and computing for storing and comparing the timestamps of each request So at the cost of more memory and processing power, this algorithm is used for Financial Systems, Authentication Systems, which are not high-frequency but requires accuracy. Sliding window counter Combination of fixed window counter and sliding window log: Requests in current window + requests in the previous window * overlap percentage of the rolling window and previous window Memory efficient and better burst resolving than fixed window counter when facing the edge of the time window issues An approximation of the actual rate because it assumes requests in the previous window are evenly distributed Cloudfare: 0.003% of requests are wrongly allowed or rate limited among 400 million requests High-level design graph Step 3 - Design deep dive # The high-level design(Rate limiting as the middleware using Redis as the counter storage) does not answer the following questions:\nHow are rate limiting rules created? Where are the rules stored?\nRules are generally written in configuration files and saved on disk. domain: auth descriptors: - key: auth_type Value: login rate_limit: unit: minute requests_per_unit: 5 Afterwards, should the rules be stored in memory or in redis? Loading Rules into Memory: Environment Variables or In-Memory Storage: When your application starts, you can load the rate limit rules from the YAML file into memory (such as environment variables or a configuration object). Advantage: Fast access, minimal latency, and no dependency on external services during rule evaluation. Disadvantage: Requires application restart to update rules. Caching Rules in Redis: Redis Cache: You can load the rate limit rules into Redis when your application starts. The application can then fetch these rules from Redis when needed. Advantage: Centralized storage for rules, making it easier to update rules without restarting the application. This is especially useful in distributed systems where multiple instances of your application need to share the same rules. Disadvantage: Slightly increased latency compared to in-memory access, but still very fast due to Redis\u0026rsquo;s performance. How to handle requests that are rate limited?\nIn case a request is rate limited, APIs return a HTTP response code 429 (too many requests) to the client. Sometimes we need to keep those requests to be processed later. Rate limiter HTTP headers\nX-Ratelimit-Remaining: The remaining number of allowed requests within the window. X-Ratelimit-Limit: It indicates how many calls the client can make per time window. X-Ratelimit-Retry-After: The number of seconds to wait until you can make a request again without being throttled. Detailed design: Rate limiter in a distributed environment\nRace condition Locks are the most obvious solution for solving race condition. However, locks will significantly slow down the system. Two strategies are commonly used to solve the problem: Lua script reference: https://gist.github.com/ptarjan/e38f45f2dfe601419ca3af937fff574d#request-rate-limiter Sorted sets data structure in Redis reference: https://engineering.classdojo.com/blog/2015/02/06/rolling-rate-limiter/ Synchronization issue Use centralized data stores like Redis Step 4 - Wrap up # Hard vs soft rate limiting Hard: The number of requests cannot exceed the threshold. Soft: Requests can exceed the threshold for a short period. Rate limiting at different levels: at the application level (HTTP: layer 7). by IP addresses using Iptables (IP: layer 3). Reference: https://blog.programster.org/rate-limit-requests-with-iptables Expansion # How does Redis solve the consistency issue in distributed mode?\nRedis Consistency Model\nEventual Consistency: Redis, by default, does not provide strong consistency. Instead, it operates under an eventual consistency model. This means that after a write operation, there might be a delay before all replicas have the same data as the master. During this period, read operations from replicas might return outdated data. Master-Slave (Primary-Replica) Replication:\nMechanism: In Redis, a master (primary) node handles all write operations. One or more slave (replica) nodes replicate data from the master. Replication is typically asynchronous, meaning that there is a delay before the replicas catch up with the master. Consistency: Because replication is asynchronous, it is possible for replicas to lag behind the master, leading to a window where reads from replicas might return stale data. Redis Cluster:\nSharding: Redis Cluster allows Redis to scale horizontally by partitioning (sharding) the dataset across multiple master nodes. Each master in the cluster is responsible for a subset of the keyspace, determined by consistent hashing. Groups: Each shard in a Redis Cluster has one master and several replicas (slaves). This master-replica setup within each shard provides redundancy and improves fault tolerance. Key Distribution: When a key is stored or accessed, the Redis Cluster calculates which shard (master-replica group) is responsible for that key using consistent hashing. The key is then sent to the appropriate master node within the cluster. Redis Sentinel:\nMonitoring and Failover: Redis Sentinel is responsible for monitoring the health of the master nodes. If a master node fails, Sentinel coordinates the election of a new master from one of the replicas. This process involves promoting the most up-to-date replica to the master role. High Availability: Sentinel helps to ensure high availability by automatically handling failovers, making the Redis deployment more resilient to failures. CHAPTER 5: DESIGN CONSISTENT HASHING # Consistent hashing is a special kind of hashing such that when a hash table is re-sized and consistent hashing is used, only k/n keys need to be remapped on average, where k is the number of keys, and n is the number of slots. In contrast, in most traditional hash tables, a change in the number of array slots causes nearly all keys to be remapped\nTraditional Hashing: hash(node_id) % node_number Consistent Hashing: hash(node_id) Virtual Node in Consistent Hashing: hash(node_id + str(vnode_id)) Small Clusters (3-10 Nodes): Consider using 100-200 vnodes per node. Medium Clusters (10-50 Nodes): 50-150 vnodes per node are usually sufficient Large Clusters (50+ Nodes): 20-100 vnodes per node might be appropriate. The standard deviation that measures how data are spread out is between 5% (200 virtual nodes) and 10% (100 virtual nodes) of the mean. However, more spaces are needed to store data about virtual nodes. This is a tradeoff, and we can tune the number of virtual nodes to fit our system requirements. Find affected range Add a new node onto the ring. The affected range starts from the newly added node(current) and moves anticlockwise around the ring until a server is found (previous). Thus, keys located between current and previous(which originaly belong to next, the node next to current) need to be redistributed to current.\nRemove a node from the ring. The affected range starts from the newly added node(current) and moves anticlockwise around the ring until a server is found (previous). Thus, keys located between current and previous(which originaly belong to current) need to be redistributed to next.\nBenefits of consistent hashing Minimized keys are redistributed when servers are added or removed.\nIt is easy to scale horizontally because data are more evenly distributed.\nMitigate hotspot key problem. Excessive access to a specific shard could cause server overload. Consistent hashing helps to mitigate the problem by distributing the data more evenly.\nCHAPTER 6: DESIGN A KEY-VALUE STORE # 1. Requirements # The size of a key-value pair is small: less than 10 KB. Ability to store big data. High availability: The system responds quickly, even during failures. High scalability: The system can be scaled to support large data set. Automatic scaling: The addition/deletion of servers should be automatic based on traffic. Tunable consistency. Low latency. 2. What features are expected in CAP(AP or CP)? # Consistency: consistency means all clients see the same data at the same time no matter which node they connect to.\nAvailability: availability means any client which requests data gets a response even if some of the nodes are down.\nPartition Tolerance: a partition indicates a communication break between two nodes. Partition tolerance means the system continues to operate despite network partitions.\nNOTE: Since network failure is unavoidable, a distributed system must tolerate network partition. Thus, a CA system cannot exist in real-world applications.\nCP(Strong Consistency): If we choose consistency over availability (CP system), we must block all write operations to n1 and n2 to avoid data inconsistency among these three servers when n3 is offline, which makes the system unavailable.\nExamples:\nPaxos Raft Zookeeper Etcd AP(Eventual Consistency): If we choose availability over consistency (AP system), the system keeps accepting reads, even though it might return stale data. For writes, n1 and n2 will keep accepting writes, and data will be synced to n3 when the network partition is resolved.\nExamples:\nCassandra DNS (Domain Name System) 3. System components # Core components and techniques used to build a key-value store:\nComponent 1: Data partition # Challenges in partitioning the data: Distribute data across multiple servers evenly. Minimize data movement when nodes are added or removed. Solution: Consistent hashing Automatic scaling: servers could be added and removed automatically depending on the load. Heterogeneity: the number of virtual nodes for a server is proportional to the server capacity. Component 2: Data replication # To achieve high availability and reliability, data must be replicated asynchronously over N servers, where N is a configurable parameter. These N servers are chosen using the following logic: after a key is mapped to a position on the hash ring, walk clockwise from that position and choose the first N servers on the ring to store data copies. With virtual nodes, the first N nodes on the ring may be owned by fewer than N physical servers. To avoid this issue, we only choose unique servers while performing the clockwise walk logic. Component 3: Consistency # N = The number of replicas W = A write quorum of size W. When a client performs a write operation, it is sent to at least W nodes. The operation is considered successful only when at least W nodes confirm the write. R = A read quorum of size R. When a client reads data, it queries at least R nodes. The read operation is successful only if at least R nodes return the same version of the data. Question: How to configure N, W, and R to fit our use cases? Here are some of the possible setups: # If R = 1 and W = N, the system is optimized for a fast read. Usage Example: Financial Transaction Processing The system needs to ensure that the write operation is strongly consistent across multiple nodes to avoid issues like double spending or incorrect balances. TODO: Future Investigation If W = 1 and R = N, the system is optimized for fast write. Usage Example: Content Delivery Network (CDN) for Static Content The system can afford to have a lower W since updates are less frequent, but R needs to be high to ensure users get the most up-to-date content. TODO: Future Investigation If W = N/2 + 1 and R = N/2 + 1, see Paxos and Raft. If W + R \u0026gt; N, strong consistency is guaranteed. If W + R \u0026lt;= N, strong consistency is not guaranteed. Consistency models # Strong consistency: any read operation returns a value corresponding to the result of the most updated write data item. A client never sees out-of-date data. Weak consistency: subsequent read operations may not see the most updated value. Eventual consistency: this is a specific form of weak consistency. Given enough time, all updates are propagated, and all replicas are consistent. Component 4: Inconsistency resolution: Versioning # vector clock: a [server, version] pair associated with a data item Using vector clocks, it is easy to tell that a version X is an ancestor (i.e. no conflict) of version Y if the version counters for each participant in the vector clock of Y is greater than or equal to the ones in version X. For example, the vector clock D([s0, 1], [s1, 1])] is an ancestor of D([s0, 1], [s1, 2]). Therefore, no conflict is recorded. You can tell that a version X is a sibling (i.e., a conflict exists) of Y if there is any participant in Y\u0026rsquo;s vector clock who has a counter that is less than its corresponding counter in X. For example, the following two vector clocks indicate there is a conflict: D([s0, 1], [s1, 2]) and D([s0, 2], [s1, 1]). Downsides: First, vector clocks add complexity to the client because it needs to implement conflict resolution logic. Second, the [server: version] pairs in the vector clock could grow rapidly. However, based on Dynamo paper, Amazon has not yet encountered this problem in production; Component 5: Failures # Failure detection\nall-to-all multicasting: Inefficient\ndecentralized failure detection methods: Gossip Protocol\nEach node keeps a list of other nodes with a heartbeat counter, which it increases periodically to show it\u0026rsquo;s active.\nNodes randomly share their heartbeat info with a few other nodes, which then pass it on, spreading the information throughout the network.\nWhen a node notices that another node\u0026rsquo;s heartbeat hasn\u0026rsquo;t increased for a while, it assumes that node is offline.\nQuestion: How is the node list of each node determined? # Answer: In the gossip protocol, the list of nodes that each node communicates with is typically determined randomly, not manually. # Each node in the system maintains a membership list of all other nodes, and when it needs to send a heartbeat or other information, it randomly selects a small subset of nodes from this list to communicate with. This selection process is dynamic and happens automatically at each communication interval. # Question: Is there any chance that a healthy node is treated as unhealthy by mistake? # Yes, there is a possibility, albeit a small one, that a network partition or random selection could lead to a situation where a healthy node is incorrectly marked as unhealthy in the gossip protocol. # Handling temporary failures: Sloppy Quorum and Hinted Handoff(The Quorum number is not changed, still W and R)\nWhen some nodes are unreachable due to failures or network issues, sloppy quorum steps in to ensure operations can still proceed:\nWrite Operations: If the required nodes for a write quorum are unavailable, the system temporarily writes the data to any available nodes, even if they are not part of the original set responsible for the data. These nodes are often referred to as “hinted” nodes.\nRead Operations: Similarly, read operations can be satisfied by querying any subset of nodes that hold the relevant data, ensuring that the operation completes even if not all original nodes are reachable.\nThe data is stored on available nodes, and a “hint” is recorded. This hint indicates that the data should be transferred to the original responsible nodes once they become available again.\nThese hinted writes ensure that the data is not lost and will eventually reach its intended location.\nHigher Availability: By allowing operations to proceed with any available nodes, sloppy quorum significantly improves the system’s availability during partial failures.\nTemporary Inconsistency: The system may temporarily have inconsistencies since not all nodes are updated immediately. However, it ensures that these inconsistencies are resolved over time, maintaining eventual consistency.\nPotential for Data Loss: If hinted nodes fail before the data can be transferred to the original nodes, there is a risk of data loss unless additional measures (e.g., replication of hints) are taken.\nReference: https://www.geeksforgeeks.org/what-is-sloppy-quorum-and-hinted-handoff/\nHandling permanent failures: Anti-Entropy Protocols\nMerkle tree: Inconsistency Detection\nTree-Like Structure: A Merkle tree is a binary tree, where each leaf node represents a hash of a block of data, and each non-leaf (or internal) node is a hash of its child nodes. Leaves(Hash(data)): The leaf nodes contain the hash values of the actual data blocks. Non-Leaf Nodes(Hash(Hash(Child1) + Hash(Child2))): The internal nodes are created by taking the hash of the concatenation of their child nodes\u0026rsquo; hash values. This process continues up the tree until you reach the root node. It\u0026rsquo;s widely used in systems where data integrity is critical, to ensure that data hasn\u0026rsquo;t been tampered with or to identify which part has been modified. TODO: So what\u0026rsquo;s the solution to handle permanent failures? (NOT MENTIONED IN THE BOOK)\nDetection of Failure Node Replacement Data Rebalancing Data Replication Consensus Protocols: In some cases, consensus protocols like Raft or Paxos may be used to coordinate the recovery process, ensuring that the system agrees on the state of the data after recovery. Component 6: System architecture diagram # Clients communicate with the key-value store through simple APIs: get(key) and put(key, value).\nA coordinator is a node that acts as a proxy between the client and the key-value store.\nNodes are distributed on a ring using consistent hashing.\nThe system is completely decentralized so adding and moving nodes can be automatic.\nData is replicated at multiple nodes.\nThere is no single point of failure as every node has the same set of responsibilities.\nComponent 7: Write path # The write request is persisted on a commit log file.\nData is saved in the memory cache.\nWhen the memory cache is full or reaches a predefined threshold, data is flushed to SSTable on disk.\nTODO: What is a SSTable?\nComponent 8: Read path # The system first checks if data is in memory. If not, go to step 2.\nIf data is not in memory, the system checks the bloom filter. 3. The bloom filter is used to figure out which SSTables might contain the key.\nSSTables return the result of the data set.\nThe result of the data set is returned to the client.\nReference: # Cassandra Architecture: https://cassandra.apache.org/doc/stable/cassandra/architecture/ SStable: https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/ CHAPTER 7: DESIGN A UNIQUE ID GENERATOR IN DISTRIBUTED SYSTEMS # Requirements: # IDs must be unique. IDs are numerical values only. IDs fit into 64-bit. IDs are ordered by date. Ability to generate over 10,000 unique IDs per second. Possible Solutions: # Multi-master replication Instead of increasing the next ID by 1, we increase it by k, where k is the number of database servers in use.\nUUID No coordination required but is 128 bits long and non-numeric.\nTicket Server Use a centralized auto increment feature in a single database/redis server (Ticket Server)\nTwitter snowflake approach\nDivide an ID into different sections:\nSign bit: 1 bit. It will always be 0. This is reserved for future uses. It can potentially be used to distinguish between signed and unsigned numbers.\nTimestamp: 41 bits. Milliseconds since the epoch or custom epoch.\nDatacenter ID: 5 bits, which gives us 2 ^ 5 = 32 datacenters.\nMachine ID: 5 bits, which gives us 2 ^ 5 = 32 machines per datacenter.\nSequence number: 12 bits. For every ID generated on that machine/process, the sequence number is incremented by 1. The number is reset to 0 every millisecond.\nCustom Algorithm with Distributed Coordination Systems(Zookeeper, etcd, etc)\nMore Discussion: # Clock synchronization Network Time Protocol is the most popular solution to this problem TODO: All the solution listed in the books are silly and not that useful, if you have more time think of something can be used in the real world. CHAPTER 8: DESIGN A URL SHORTENER # 301 redirect. A 301 redirect shows that the requested URL is “permanently” moved to the long URL. Since it is permanently redirected, the browser caches the response, and subsequent requests for the same URL will not be sent to the URL shortening service. Instead, requests are redirected to the long URL server directly.\nReduce the server load 302 redirect. A 302 redirect means that the URL is “temporarily” moved to the long URL, meaning that subsequent requests for the same URL will be sent to the URL shortening service first. Then, they are redirected to the long URL server.\nGood for analytics Requirements: # Write operation: 100 million URLs are generated per day. Write operation per second: 100 million / 24 /3600 = 1160 Read operation: Assuming ratio of read operation to write operation is 10:1, read operation per second: 1160 * 10 = 11,600 Assuming the URL shortener service will run for 10 years, this means we must support 100 million * 365 * 10 = 365 billion records. Assume average URL length is 100. Storage requirement over 10 years: 365 billion * 100 bytes * 10 years = 365 TB Hash function: # The hashValue consists of characters from [0-9, a-z, A-Z], containing 10 + 26 + 26 = 62 possible characters. To figure out the length of hashValue, find the smallest n such that 62^n ≥ 365 billion.\nHash Function 1: hash + collision resolution\nA straightforward solution is to use well-known hash functions like CRC32, MD5, or SHA-1. Even the shortest hash value (from CRC32) is too long (more than 7 characters). How can we make it shorter? The first approach is to collect the first 7 characters of a hash value; however, this method can lead to hash collisions. To resolve hash collisions, we can recursively append a new predefined string until no more collision is discovered Can use bloom filter + Redis to accelarate DB searching Hash Function 2: base 62 conversion\nUse a unique incremental ID generator Convert the decimal int to base 62 People can guess the URL path, so it has potential security issues CHAPTER 9: DESIGN A WEB CRAWLER # A crawler is used for many purposes:\nSearch engine indexing: This is the most common use case. A crawler collects web pages to create a local index for search engines. For example, Googlebot is the web crawler behind the Google search engine.\nWeb archiving: This is the process of collecting information from the web to preserve data for future uses. For instance, many national libraries run crawlers to archive web sites. Notable examples are the US Library of Congress and the EU web archive.\nWeb mining: The explosive growth of the web presents an unprecedented opportunity for data mining. Web mining helps to discover useful knowledge from the internet. For example, top financial firms use crawlers to download shareholder meetings and annual reports to learn key company initiatives.\nWeb monitoring: The crawlers help to monitor copyright and trademark infringements over the Internet. For example, Digimarc utilizes crawlers to discover pirated works and reports.\nThe basic algorithm of a web crawler is simple # Given a set of URLs, download all the web pages addressed by the URLs.\nExtract URLs from these web pages\nAdd new URLs to the list of URLs to be downloaded. Repeat these 3 steps.\nAsk Questions: # What is the main purpose of the crawler? Is it used for search engine indexing, data mining, or something else?\nRequirement: Search engine indexing.\nHow many web pages does the web crawler collect per month?\nRequirement: 1 billion pages.\nWhat content types are included? HTML only or other content types such as PDFs and images as well?\nRequirement: HTML only.\nShall we consider newly added or edited web pages?\nRequirement: Yes, we should consider the newly added or edited web pages.\nDo we need to store HTML pages crawled from the web?\nRequirement: Yes, up to 5 years\nHow do we handle web pages with duplicate content?\nRequirement: Pages with duplicate content should be ignored.\nCharacteristics of the system:\nScalability: Web crawling should be extremely efficient using parallelization. Robustness: Bad HTML, unresponsive servers, crashes, malicious links. The crawler must handle all those edge cases. Politeness: The crawler should not make too many requests to a website within a short time interval. Extensibility: The system is flexible so that minimal changes are needed to support new content types. For example, if we want to crawl image files in the future, we should not need to redesign the entire system. Scale Estimation # QPS: 1,000,000,000 / 30 days / 24 hours / 3600 seconds = ~400 pages per second. Assume the average web page size is 2000k. 1-billion-page x 2000k = 2 PB storage per month. Assuming data are stored for five years, 2 PB * 12 months * 5 years = 120 PB. A 120 PB storage is needed to store five-year content. Initial Design # Seed URLs\nA web crawler uses seed URLs as a starting point for the crawl process.\nTo crawl the entire web, we need to be creative in selecting seed URLs. A good seed URL serves as a good starting point that a crawler can utilize to traverse as many links as possible.\nThe general strategy is to divide the entire URL space into smaller ones. The first proposed approach is based on locality as different countries may have different popular websites. Another way is to choose seed URLs based on topics.\nURL Frontier\nMost modern web crawlers split the crawl state into two: to be downloaded and already downloaded. The component that stores URLs to be downloaded is called the URL Frontier. You can refer to this as a First-in-First-out (FIFO) queue.\nHTML Downloader\nThe HTML downloader downloads web pages from the internet. Those URLs are provided by the URL Frontier.\nDNS Resolver(But we don\u0026rsquo;t need to design this right)\nTo download a web page, a URL must be translated into an IP address. The HTML Downloader calls the DNS Resolver to get the corresponding IP address for the URL.\nContent Parser\nAfter a web page is downloaded, it must be parsed and validated because malformed web pages could provoke problems and waste storage space. Implementing a content parser in a crawl server will slow down the crawling process. Thus, the content parser is a separate component.\nContent Duplicate Checker\nAn efficient way to accomplish this task is to compare the hash values of the two web pages.\nContent Storage\nIt is a storage system for storing HTML content.\nMost of the content is stored on disk because the data set is too big to fit in memory.\nPopular content is kept in memory to reduce latency.\nURL Extractor\nURL Extractor parses and extracts links from HTML pages. Figure below shows an example of a link extraction process. Relative paths are converted to absolute URLs by adding a prefix.\nURL Filter\nThe URL filter excludes certain content types, file extensions, error links and URLs in “blacklisted” sites.\nURL Duplicate Checker\nIt helps to avoid adding the same URL multiple times as this can increase server load and cause potential infinite loops.\nBloom filter and hash table are common techniques to implement the Deplicate Checker component.\nURL Storage\nURL Storage stores already visited URLs.\nWeb crawler workflow # Design deep dive # DFS vs BFS # You can think of the web as a directed graph where web pages serve as nodes and hyperlinks (URLs) as edges. The crawl process can be seen as traversing a directed graph from one web page to others. Two common graph traversal algorithms are DFS and BFS. However, DFS is usually not a good choice because the depth of DFS can be very deep.\nBFS is commonly used by web crawlers and is implemented by a first-in-first-out (FIFO) queue. In a FIFO queue, URLs are dequeued in the order they are enqueued. However, this implementation has two problems:\nImpoliteness: Most links from the same web page are linked back to the same host. When the crawler tries to download web pages in parallel, servers will be flooded with requests. This is considered as “impolite”.\nStandard BFS does not take the priority of a URL into consideration. The web is large and not every page has the same level of quality and importance. Therefore, we may want to prioritize URLs according to their page ranks, web traffic, update frequency, etc.\nURL frontier # URL frontier helps to address these problems. A URL frontier is a data structure that stores URLs to be downloaded. The URL frontier is an important component to ensure politeness, URL prioritization, and freshness.\nPoliteness\nThe general idea of enforcing politeness is to download one page at a time from the same host. A delay can be added between two download tasks. The politeness constraint is implemented by maintain a mapping from website hostnames to download (worker) threads.\nIn the book, author introduces a method which requires a separate queue containing URLs from a single host for each hostname. It also requires One Worker per Queue. But I think this method is inefficient with resources.\nPossible Improvements:\nWorker Pool Custom DelayedQueue Priority\nPrioritize URLs based on usefulness, which can be measured by PageRank, website traffic, update frequency, etc. “Prioritizer” is the component that handles URL prioritization.\nPrioritizer: It takes URLs as input and computes the priorities.\nQueue f1 to fn: Each queue has an assigned priority. Queues with high priority are selected with higher probability.\nQueue selector: Randomly choose a queue with a bias towards queues with higher priority.\nFreshness\nRecrawl based on web pages’ update history. Prioritize URLs and recrawl important pages first and more frequently. Storage for URL Frontier\nThe majority of URLs are stored on disk Maintain buffers in memory for enqueue/dequeue operations. Data in the buffer is periodically written to the disk. Robots Exclusion Protocol The Robots Exclusion Protocol (REP), commonly referred to as the robots.txt file, is a standard used by websites to communicate with web crawlers and other automated agents about which parts of the site should or should not be accessed. It is a way for site owners to control how search engines and other web crawlers interact with their site.\nDisallow Directive: The Disallow directive specifies which parts of the site crawlers should not access. Allow Directive: The Allow directive specifies parts of the site that crawlers are allowed to access. Sitemap Directive: The robots.txt file can also include a link to the site\u0026rsquo;s XML sitemap, which helps crawlers understand the structure of the site and locate the most important pages for indexing. No Enforcement Mechanism: It’s important to note that the Robots Exclusion Protocol is purely advisory. Compliant crawlers (such as search engines like Google or Bing) follow the rules, but non-compliant crawlers, such as some malicious bots, can ignore the robots.txt file and crawl the site anyway. No Indexing Control: The robots.txt file only controls crawling, not indexing. Pages that are disallowed in the robots.txt file may still be indexed if they are linked to from other sites. User-agent: * Disallow: /private/ Allow: /public/ Sitemap: https://www.example.com/sitemap.xml Performance optimization\nDistributed crawl\nCache DNS Resolver\nDNS Resolver is a bottleneck for crawlers because DNS requests might take time due to the synchronous nature of many DNS interfaces. DNS response time ranges from 10ms to 200ms. Once a request to DNS is carried out by a crawler thread, other threads are blocked until the first request is completed. Our DNS cache keeps the domain name to IP address mapping and is updated periodically by cron jobs.\nWhy Are DNS Requests Synchronous? # Historically, many DNS interfaces were implemented synchronously for simplicity. Early DNS libraries and network protocols often operated in a blocking, synchronous fashion because that was how many networking systems were designed. Some reasons include:\nLegacy Design: Synchronous operations are easier to implement, especially in systems where concurrency and non-blocking I/O were less emphasized. Blocking I/O: DNS resolution was typically done through blocking I/O calls, meaning the program would wait until the DNS response came back before moving forward. Single Request-Response Nature: DNS queries are usually simple request-response pairs, so they were often implemented as blocking calls, with the assumption that the response would be relatively fast. Modern Asynchronous DNS Services # Java: Uses the InetAddress class for synchronous DNS and libraries like DNSJava for more advanced queries. Asynchronous options include Netty DNS Resolver and Vert.x. Python: Supports DNS resolution through the socket module for synchronous calls and libraries like dnspython for advanced queries. Asynchronous options include aiohttp and aiodns. Go: Has the net package for synchronous DNS resolution and advanced libraries like miekg/dns. Asynchronous DNS can be handled using goroutines. Locality\nShort timeout\nRobustness\nBesides performance optimization, robustness is also an important consideration. We present a few approaches to improve the system robustness:\nConsistent hashing: This helps to distribute loads among downloaders.\nSave crawl states and data: To guard against failures, crawl states and data are written to a storage system. A disrupted crawl can be restarted easily by loading saved states and data.\nException handling: Errors are inevitable and common in a large-scale system. The crawler must handle exceptions gracefully without crashing the system.\nData validation: This is an important measure to prevent system errors.\nExtensibility\nPNG Downloader module: it is plugged-in to download PNG files. Web Monitor module: prevent copyright and trademark infringements. Detect and avoid problematic content\nRedundant content Spider traps A spider trap is a web page that causes a crawler in an infinite loop. For instance, an infinite deep directory structure is listed as follows: www.spidertrapexample.com/foo/bar/foo/bar/foo/bar/… Such spider traps can be avoided by setting a maximal length for URLs. However, no onesize-fits-all solution exists to detect spider traps. Websites containing spider traps are easy to identify due to an unusually large number of web pages discovered on such websites. It is hard to develop automatic algorithms to avoid spider traps; however, a user can manually verify and identify a spider trap, and either exclude those websites from the crawler or apply some customized URL filters.\nExpansion\nServer-side rendering: Numerous websites use scripts like JavaScript, AJAX, etc to generate links on the fly. If we download and parse web pages directly, we will not be able to retrieve dynamically generated links. To solve this problem, we perform server-side rendering (also called dynamic rendering) first before parsing a page CHAPTER 10: DESIGN A NOTIFICATION SYSTEM # Ask questions first: # Ask questions first:\nIs it a real-time system?\nRequirements: It is a soft real-time system. We want a user to receive notifications as soon as possible. However, if the system is under a high workload, a slight delay is acceptable.\nWhat triggers notifications?\nRequirements: Notifications can be triggered by client applications. They can also be scheduled on the server-side.\nWill users be able to opt-out?\nRequirements: Yes, users who choose to opt-out will no longer receive notifications.\nHow many notifications are sent out each day?\nRequirements: 10 million mobile push notifications, 1 million SMS messages, and 5 million emails.\nHigh-level design # It is structured as follows:\nDifferent types of notifications\nContact info gathering flow\nNotification sending/receiving flow\nDifferent types of notifications # iOS push notification\nProvider A provider builds and sends notification requests to Apple Push Notification Service (APNS). To construct a push notification, the provider provides the following data:\nDevice token: This is a unique identifier used for sending push notifications. Payload: This is a JSON dictionary that contains a notification’s payload. { \u0026#34;aps\u0026#34;: { \u0026#34;alert\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Game Request\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;Bob wants to play chess\u0026#34;, \u0026#34;action-loc-key\u0026#34;: \u0026#34;PLAY\u0026#34; }, \u0026#34;badge\u0026#34;: 5 } } APNS: This is a remote service provided by Apple to propagate push notifications to iOS devices.\niOS Device: It is the end client, which receives push notifications.\nAndroid push notification\nInstead of using APNs, Firebase Cloud Messaging (FCM) is commonly used to send push notifications to android devices.\nSMS message\nFor SMS messages, third party SMS services like Twilio [1], Nexmo [2], and many others are commonly used.\nEmail\nAlthough companies can set up their own email servers, many of them opt for commercial email services. Sendgrid and Mailchimp are among the most popular email services, which offer a better delivery rate and data analytics.\nContact info gathering flow # To send notifications, we need to gather mobile device tokens, phone numbers, or email addresses. When a user installs our app or signs up for the first time, API servers collect user contact info and store it in the database.\nNotification sending/receiving flow # Service 1 to N\nThey represent different services that send notifications via APIs provided by notification servers.\nNotification servers\nProvide APIs for services to send notifications. Those APIs are only accessible internally or by verified clients to prevent spams.\nCarry out basic validations to verify emails, phone numbers, etc.\nQuery the database or cache to fetch data needed to render a notification.\nPut notification data to message queues for parallel processing.\nDeep Dive # Reliability # How to prevent data loss?\nOne of the most important requirements in a notification system is that it cannot lose data. Notifications can usually be delayed or re-ordered, but never lost. To satisfy this requirement, the notification system persists notification data in a database and implements a retry mechanism. The notification log database is included for data persistence\nWill recipients receive a notification exactly once?\nThe short answer is no. Although notification is delivered exactly once most of the time, the distributed nature could result in duplicate notifications. To reduce the duplication occurrence, we introduce a dedupe mechanism and handle each failure case carefully. Here is a simple dedupe logic:\nWhen a notification event first arrives, we check if it is seen before by checking the event ID. If it is seen before, it is discarded. Otherwise, we will send out the notification.\nReference: https://bravenewgeek.com/you-cannot-have-exactly-once-delivery/\nAdditional components and considerations # Notification setting\nMany websites and apps give users fine-grained control over notification settings.\nRate limiting\nRetry mechanism\nWhen a third-party service fails to send a notification, the notification will be added to the message queue for retrying. If the problem persists, an alert will be sent out to developers.\nSecurity in push notifications\nFor iOS or Android apps, appKey and appSecret are used to secure push notification APIs. Only authenticated or verified clients are allowed to send push notifications using our APIs.\nMonitor queued notifications\nA key metric to monitor is the total number of queued notifications. If the number is large, the notification events are not processed fast enough by workers. To avoid delay in the notification delivery, more workers are needed.\nEvents tracking\nTODO: How to trigger an event when a user view a notification but didn\u0026rsquo;t do other operations?\nCHAPTER 11: DESIGN A NEWS FEED SYSTEM # Ask Questions First # Is this a mobile app? Or a web app? Or both?\nRequirements: Both\nWhat are the important features?\nRequirements: A user can publish a post and see her friends’ posts on the news feed page.\nIs the news feed sorted by reverse chronological order or any particular order such as topic scores?\nRequirements: To keep things simple, let us assume the feed is sorted by reverse chronological order.\nHow many friends can a user have?\nRequirements: 5000\nWhat is the traffic volume?\nRequirements: 10 million DAU\nCan feed contain images, videos, or just text?\nRequriements: It can contain media files, including both images and videos.\nHigh Level Design # The design is divided into two flows: feed publishing and news feed building.\nFeed publishing: when a user publishes a post, corresponding data is written into cache and database. A post is populated to her friends’ news feed. Newsfeed building: for simplicity, let us assume the news feed is built by aggregating friends’ posts in reverse chronological order. Newsfeed APIs\nFeed publishing API To publish a post, a HTTP POST request will be sent to the server: POST /v1/me/feed\ncontent: content is the text of the post. auth_token: used to authenticate API requests. Newsfeed retrieval API The API to retrieve news feed: GET /v1/me/feed\nauth_token: used to authenticate API requests. Feed publishing\nPost service: persist post in the database and cache. Fanout service: push new content to friends’ news feed. Newsfeed data is stored in the cache for fast retrieval. Notification service: inform friends that new content is available and send out push notifications. Newsfeed building Newsfeed service: news feed service fetches news feed from the cache. Newsfeed cache: store news feed IDs needed to render the news feed. Deep Dive # Feed publishing # Fanout service: # Fanout is the process of delivering a post to all friends. Two types of fanout models are: fanout on write (also called push model) and fanout on read (also called pull model).\nFanout on write. With this approach, news feed is pre-computed during write time. A new post is delivered to friends’ cache immediately after it is published.\nPros:\nThe news feed is generated in real-time and can be pushed to friends immediately.\nFetching news feed is fast because the news feed is pre-computed during write time.\nCons:\nIf a user has many friends, fetching the friend list and generating news feeds for all of them are slow and time consuming. It is called hotkey problem.\nFor inactive users or those rarely log in, pre-computing news feeds waste computing resources.\nFanout on read. The news feed is generated during read time. This is an on-demand model. Recent posts are pulled when a user loads her home page.\nPros:\nFor inactive users or those who rarely log in, fanout on read works better because it will not waste computing resources on them.\nData is not pushed to friends so there is no hotkey problem.\nCons:\nFetching the news feed is slow as the news feed is not pre-computed. Difference between a notification system and a content feeding system # 1. Primary Purpose # Notification System:\nGoal: Alert users in real-time about events such as new posts, interactions (likes, comments, retweets), or updates. Example: Sending a push notification when a celebrity posts a new tweet, even if the user is not actively using the app. Content Feeding System:\nGoal: Serve users with the actual content they want to see, such as posts from people they follow, organized in a timeline or feed. Example: Loading and displaying a user’s news feed when they open the app. 2. Mechanism # Notification System:\nPush-Based: Notifications are sent from the server directly to the user\u0026rsquo;s device as soon as an event happens, regardless of whether the app is running or not. Real-Time: Designed to deliver immediate alerts and updates, triggering device notifications (e.g., banner, sound) to get the user’s attention. Content Feeding System:\nHybrid (Push/Pull) Model: Push Model (Fanout on Write): Pre-computes and stores content in the user’s feed at the time of the post, so it\u0026rsquo;s ready when they open the app. Pull Model (Fanout on Read): Retrieves content on demand when the user opens the app, fetching recent posts dynamically. 3. Resource Usage # Notification System:\nLightweight: Only metadata about the event (e.g., \u0026ldquo;User X posted a new tweet\u0026rdquo;) is sent, which consumes minimal resources and bandwidth. Low Latency: Focuses on real-time delivery with minimal delay to ensure users receive alerts promptly. Content Feeding System:\nHeavier: Involves delivering large amounts of content (e.g., posts, images, videos), requiring more storage, bandwidth, and processing power, especially if it pre-computes and caches the content for users. Latency Can Vary: Latency depends on whether content is pushed and precomputed (fast retrieval) or pulled on demand (slower due to real-time fetching). 4. User Interaction # Notification System:\nPassive Engagement: Users are alerted to new content or interactions without needing to open the app first. They can decide whether to open the app and engage with the content based on the notification. Content Feeding System:\nActive Engagement: Users must open the app and interact with the feed to view and consume the content. The system is designed to deliver a seamless experience when the user engages directly with the app. 5. Independence # Notification System:\nIndependent of Content Loading: The notification system operates separately from the content feeding system, meaning notifications are delivered even if the content itself is not yet loaded or fetched. Content Feeding System:\nDependent on Feed Model: The way content is delivered depends on whether the system uses a push or pull model, influencing how quickly and efficiently content is loaded when the user interacts with the feed. 6. Examples # Notification System:\nPush notifications for new posts, comments, or likes that pop up on your phone even if the app is closed. Content Feeding System:\nThe posts you see in your timeline when you open apps like Twitter, Instagram, or Facebook, either pre-computed and ready to view or fetched on demand. Summary: # Notification System: Push-based, real-time, lightweight alerts sent to users regardless of whether the app is open or closed. Content Feeding System: Responsible for delivering and rendering the actual content in a user’s feed, using either a push model (precomputed) or a pull model (on-demand). We adopt a hybrid approach to get benefits of both approaches and avoid pitfalls in them. Since fetching the news feed fast is crucial, we use a push model for the majority of users. For celebrities or users who have many friends/followers, we let followers pull news content on-demand to avoid system overload. Consistent hashing is a useful technique to mitigate the hotkey problem as it helps to distribute requests/data more evenly.\n1. Pushing Content to Others\u0026rsquo; News Feeds: # In social media applications, each user typically has a personal news feed list, which contains posts from the accounts they follow. When you \u0026ldquo;push content\u0026rdquo; to a follower’s feed, it means that when a user creates a post, that post is immediately inserted into the news feed lists of all their followers. These personal news feed lists are usually stored in a database or cache (e.g., Redis) and are retrieved whenever the user requests their feed. For example, if Alice posts something, the system inserts Alice\u0026rsquo;s post into the personal feeds of Bob, Carol, and all her other followers. Do feeds grow infinitely? Not usually. These lists are not stored infinitely; instead, social media platforms implement mechanisms to expire older content from the cache or database to avoid excessive storage growth. For example: TTL (Time to Live): Older posts might be automatically removed after a certain time period, such as after 30 days. Fixed-size lists: Only the most recent posts are kept in the user\u0026rsquo;s feed (e.g., the last 100 posts), and older ones are deleted. Archival: Older content may be archived to cheaper, slower storage for long-term access but is not kept in the active cache. Cleanup policies: Periodic cleanup processes may be scheduled to remove inactive or outdated posts from news feeds. 2. Does Fanout on Write Require Users to Log In? # No, Fanout on Write does not require users to log in to receive new posts in their feed. When a user creates a post, the system pushes that post to their followers’ news feed whether they are logged in or not. The primary benefit of this approach is that when users do log in, their news feeds are already pre-populated with the latest posts. This makes fetching the feed instantaneous, as the posts are already sitting in the cache or database. Why not trigger a fetch request when users log in?\nPre-computation Advantage: Fanout on Write pre-computes the feed during the post creation process (write time), so users experience minimal latency when loading their feed. If you were to trigger a fetch request upon login (which is closer to a Fanout on Read model), the system would need to retrieve posts from all the accounts the user follows, sort them, and then render the feed in real-time, which introduces higher latency. Instant Experience: Fanout on Write is designed for a real-time, instant experience, ensuring that even if a user has many followers or follows many people, their news feed is ready and waiting as soon as they log in. Fanout Workers in Detail # Fanout workers play a crucial role in distributing posts to users\u0026rsquo; news feeds. They act as the processing layer between the message queue and the news feed cache. Here\u0026rsquo;s a step-by-step breakdown of how they work:\n1. Receiving the Task # Fanout workers are subscribed to a message queue (e.g., RabbitMQ, Kafka). After a new post is published, the list of friends (or followers) and the post ID are sent to the message queue. The workers fetch the task from the queue, which typically includes the post ID and a list of user IDs (friends or followers). 2. Processing the Fanout # The fanout worker processes each task by iterating through the list of user IDs. For each user ID, the worker creates an entry in the news feed cache linking the post to that user. This is usually stored as a \u0026lt;post_id, user_id\u0026gt; pair, which acts as a reference to the post rather than storing the full content (to save memory). 3. Efficient Storage # Instead of storing the complete post object and user object, only IDs are stored in the cache. This drastically reduces memory usage. The actual post data (e.g., text, images) is retrieved later from a separate data store when a user views the post. The worker appends the new \u0026lt;post_id, user_id\u0026gt; pair to the news feed cache for each follower of the original poster. 4. Cache Management # To manage memory usage, the news feed cache is typically bounded by a configurable limit. For example, each user\u0026rsquo;s feed might store only the most recent 100 posts. Older posts may be removed or archived to prevent excessive memory consumption. The workers ensure that the cache maintains a fixed size by either removing the oldest posts when new ones are added or by setting a time-to-live (TTL) policy for how long posts are kept. 5. Parallel Processing # In large-scale systems, there may be multiple fanout workers operating in parallel. This allows the system to handle large volumes of posts efficiently, even if the poster has millions of followers. Tasks are typically distributed across the workers, allowing them to independently process different batches of followers, which speeds up the fanout process. 6. Fault Tolerance # Fanout workers are designed to be fault-tolerant. If a worker fails during the task, the message queue can reassign the task to another worker to ensure that no posts are lost. This ensures that all followers eventually receive the post, even if a worker fails. Summary # Fanout workers take the list of user IDs and post ID from the message queue. They efficiently store only the IDs in the cache to keep memory usage low. They maintain the news feed cache within a fixed size and process tasks in parallel to handle scale. Fault tolerance mechanisms ensure that tasks are completed even if individual workers fail. Newsfeed retrieval # A user sends a request to retrieve her news feed. The request looks like this: /v1/me/feed\nThe load balancer redistributes requests to web servers.\nWeb servers call the news feed service to fetch news feeds.\nNews feed service gets a list post IDs from the news feed cache.\nA user’s news feed is more than just a list of feed IDs. It contains username, profile picture, post content, post image, etc. Thus, the news feed service fetches the complete user and post objects from caches (user cache and post cache) to construct the fully hydrated news feed.\nThe fully hydrated news feed is returned in JSON format back to the client for rendering.\nExpansion # 1. Are Posts Strictly Chronological? # No, the posts that a user receives on platforms like Twitter and Instagram are not always strictly chronological. These platforms often use algorithms to determine the order in which posts are presented. Factors such as engagement, relevance, and user interactions are prioritized over strict time order. While some real-time posts (like from close friends) might appear in a timely manner, other posts (like from less engaged accounts or celebrities) may be pulled and shown based on what the platform considers most relevant to the user. 2. When Should You Trigger Fanout on Read? # Triggering Fanout on Read: In a hybrid model, fanout on read should be triggered when the user logs in and requests their feed but the cache contains only partial data from fanout on write. Return Fanout on Write Data First vs. Combining Data: Option 1: Return Fanout on Write Data First: You could return the fanout on write data (e.g., posts from close friends) immediately to reduce latency and allow users to see some content right away. Meanwhile, you would trigger fanout on read in the background and fetch additional posts (e.g., from celebrities) asynchronously. Once the additional posts are retrieved, you could either update the feed in real-time or load more content as the user scrolls. Option 2: Wait and Combine Data: Alternatively, you could wait until both fanout on write and fanout on read data are combined before sending the complete feed to the user. This ensures a more seamless experience but could introduce additional latency, as the user will have to wait until all data has been fetched and merged. The choice depends on your user experience goals. If you prioritize instant loading, returning fanout on write data first is preferable. If you want a more complete feed from the start, you may opt to wait for fanout on read.\n3. How Should the Server Retrieve Missing Data Not in the Fanout on Write Cache or DB? # When the server detects that certain posts are missing from the fanout on write cache, it needs to run a fanout on read operation. Here’s how it can retrieve the missing data: Identify Missing Posts: The server first determines which posts are missing. This can be done by checking the last post timestamp or post sequence numbers to identify gaps in the feed. If certain posts are not found in the cache, they are flagged for retrieval. Query the Primary Data Store: For posts that were not cached during fanout on write, the server queries the primary database (where all posts are stored) to fetch recent posts from the accounts the user follows (e.g., celebrities or distant connections). Merge with Cache Data: Once the missing posts are retrieved from the database, they are merged with the cached data from fanout on write, ensuring that the final feed includes all relevant posts. Return Data to the User: Depending on the system design, this data can either be returned immediately once complete or loaded incrementally as the user scrolls through their feed. 4. How Can We Know What Data Should Be Retrieved During the Fanout on Read Phase? # To effectively retrieve the missing data during the fanout on read phase, the system can follow these steps:\nTracking Gaps in the Cache:\nCache Metadata: The system can store metadata in the user’s feed cache, such as last update timestamps, post sequence numbers, or content source tags (e.g., marking whether a post was retrieved via fanout on write or fanout on read). This metadata can be used to track which posts were pushed during fanout on write and identify the content that was not included (such as posts from high-volume users or celebrities). Querying the Relevant Data Sources:\nIdentify Followed Accounts Excluded from Fanout on Write: During the fanout on read phase, the system checks the list of followed accounts that were excluded from fanout on write (e.g., celebrities, users with a large following). These accounts are flagged as requiring additional retrieval. Time-Based Queries: Using the last update timestamp or post sequence number, the system queries the database or a specialized data store to retrieve any new posts from these excluded accounts that were published after the user’s cache was last updated. Dynamic Post Retrieval:\nThe system performs a dynamic retrieval of posts from these accounts (via fanout on read) and integrates them into the user’s feed. This process ensures that the user receives both the real-time posts from close friends and the on-demand posts from celebrities or high-volume users. Pagination and Infinite Scrolling:\nSince many social media platforms use pagination or infinite scrolling, the fanout on read phase can be executed incrementally as the user scrolls down their feed. As new data is needed, the system fetches the next batch of posts from the users excluded from fanout on write, ensuring a continuous and seamless user experience. Prioritization and Filtering:\nDuring fanout on read, the system can prioritize fetching the most recent or most relevant posts from these excluded accounts, based on factors like engagement or user interest. Filtering mechanisms can be used to ensure that only the most relevant posts are pulled into the user’s feed, avoiding information overload. CHAPTER 12: DESIGN A CHAT SYSTEM # "},{"id":3,"href":"/docs/cloud-computing-and-infrastructure/","title":"Cloud Computing and Infrastructure","section":"Docs","content":"Cloud Computing\n"},{"id":4,"href":"/docs/cloud-computing-and-infrastructure/aws/api-gateway/api-endpoint-type/","title":"API Endpoint Type","section":"API Gateway","content":" API endpoint type # Regional APIs are deployed in the current AWS Region. Edge-optimized APIs route requests to the nearest CloudFront Point of Presence. Private APIs are only accessible from VPCs. "},{"id":5,"href":"/docs/cloud-computing-and-infrastructure/aws/api-gateway/domain-name/","title":"Domain Name","section":"API Gateway","content":" Why Can\u0026rsquo;t I Create a Route53 Record Pointing Directly to the API Invoke URL? # The API Invoke URL provided by AWS API Gateway is a domain managed by AWS. This URL usually follows a pattern like {api-id}.execute-api.{region}.amazonaws.com. When you use a service like Route 53 to manage your DNS, creating a CNAME record that directly points to this invoke URL is not recommended for a couple of reasons:\nSSL/TLS Certificate Mismatch: The SSL/TLS certificate on the API Gateway\u0026rsquo;s default URL is issued to that specific AWS domain and not to your custom domain. Directly pointing a Route 53 record to the API Gateway\u0026rsquo;s invoke URL can lead to SSL/TLS mismatches and security warnings in browsers or API clients.\nCustomization and Control: Using a custom domain name through API Gateway allows for greater control and customization. For example, you can set up custom paths for your API routes, use your own SSL/TLS certificates, and have a URL that\u0026rsquo;s branded for your organization or service.\nWhat is the meaning of the Domain Name in API Gateway and can I create a Route 53 Record with a Different Domain Name # The domain name in API Gateway refers to the custom domain that you want to use for your API, different from the default AWS-provided URL. You can set up a custom domain in API Gateway and then create a CNAME or A Alias record in Route 53 that points to this custom domain.\nWhen you set up a custom domain in API Gateway, AWS generates a new endpoint (a CloudFront distribution) for this domain. This is what you point your DNS record (in Route 53 or any other DNS service) to, not the original invoke URL.\nYou cannot use a Route 53 record with a completely different domain name to point to the AWS-generated domain name (the one for the custom domain in API Gateway). The domain name you use in Route 53 should match the custom domain you\u0026rsquo;ve set up in API Gateway.\nEven though I still don\u0026rsquo;t know why, but when I access the cloudfront distribution URL directly, it raises Not Found error. When I tried to create a rent route53 record with a different domain pointing to the cloudfront URL, it raises Forbidden error. So I assume there is a domain mapping mechanism behind the scene, which is done by the API gateway.\n"},{"id":6,"href":"/docs/cloud-computing-and-infrastructure/aws/api-gateway/header/","title":"Header","section":"API Gateway","content":" Forward Original Host from the Original Request # Today I met this issue, I tried to realize a request workflow like this:\nUser Request -\u0026gt; Route53 DNS(CloudFront Distribution of the API Gateway Custom Domain) -\u0026gt; API Gateway -\u0026gt; via HTTP request -\u0026gt; Public Application Load Balancer -\u0026gt; ECS instance\ncurl -H \u0026#34;Host: notification.dev.scrawlrapi.com\u0026#34; https://notification.dev.scrawlrapi.com/status When I tried to send request like this, the ECS instance indeed received the request but with a modified host header.\n{ \u0026#34;time\u0026#34;: \u0026#34;2024-03-25T23:05:22+00:00\u0026#34;, \u0026#34;remote_addr\u0026#34;: \u0026#34;10.0.0.187\u0026#34;, \u0026#34;remote_user\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ssl_protocol_cipher\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;body_bytes_sent\u0026#34;: \u0026#34;55\u0026#34;, \u0026#34;request_time\u0026#34;: \u0026#34;0.000\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;404\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;GET / HTTP/1.1\u0026#34;, \u0026#34;request_method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;http_referrer\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;http_x_forwarded_for\u0026#34;: \u0026#34;44.234.29.173\u0026#34;, \u0026#34;http_cf_ray\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;dualstack.bowen-test-public-alb-1535001582.us-west-2.elb.amazonaws.com\u0026#34;, \u0026#34;server_name\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;upstream_address\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;upstream_status\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;upstream_response_time\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;upstream_response_length\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;upstream_cache_status\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;http_user_agent\u0026#34;: \u0026#34;curl/8.1.2\u0026#34; } So the host is the HTTP address that I set in the API Gateway Integration\nCan I create a public NLB in front of the private ALB? # Yes you can, but now AWS only supports directing encrypted data to ALB from NLB. You cannot create a target group with Application Load Balancer using TLS protocol(Only TCP is supported). Which means you can not decrpyt the incoming requests over HTTPS in NLB and send the unencrypted data to the ALB.\nHowever, you can just simply listen on the port 443 using TCP protocol and then direct the encrypted data to the ALB. And then terminate the TLS connection in the HTTPS listeners in the ALB.\nERRORS # Invalid mapping expression specified: Validation Result: warnings : [], errors : [Operations on header x-forwarded-for are restricted]\nAPI Gateway Headers\n{ \u0026#34;cookie\u0026#34;: [ \u0026#34;_ga=GA1.1.643138662.1711125483; _ga_QT3LLYY7TG=GS1.1.1724445288.9.0.1724445288.0.0.0\u0026#34; ], \u0026#34;via\u0026#34;: [ \u0026#34;HTTP/1.1 AmazonAPIGateway\u0026#34; ], \u0026#34;forwarded\u0026#34;: [ \u0026#34;for=207.81.250.185;host=geosociatal.uat.scrawlr.com;proto=https\u0026#34; ], \u0026#34;priority\u0026#34;: [ \u0026#34;u=0, i\u0026#34; ], \u0026#34;accept-language\u0026#34;: [ \u0026#34;zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7\u0026#34; ], \u0026#34;accept-encoding\u0026#34;: [ \u0026#34;gzip, deflate, br, zstd\u0026#34; ], \u0026#34;sec-fetch-dest\u0026#34;: [ \u0026#34;document\u0026#34; ], \u0026#34;sec-fetch-user\u0026#34;: [ \u0026#34;?1\u0026#34; ], \u0026#34;sec-fetch-mode\u0026#34;: [ \u0026#34;navigate\u0026#34; ], \u0026#34;sec-fetch-site\u0026#34;: [ \u0026#34;none\u0026#34; ], \u0026#34;accept\u0026#34;: [ \u0026#34;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\u0026#34; ], \u0026#34;upgrade-insecure-requests\u0026#34;: [ \u0026#34;1\u0026#34; ], \u0026#34;sec-ch-ua-platform\u0026#34;: [ \u0026#34;\\\u0026#34;macOS\\\u0026#34;\u0026#34; ], \u0026#34;sec-ch-ua-mobile\u0026#34;: [ \u0026#34;?0\u0026#34; ], \u0026#34;sec-ch-ua\u0026#34;: [ \u0026#34;\\\u0026#34;Not/A)Brand\\\u0026#34;;v=\\\u0026#34;8\\\u0026#34;, \\\u0026#34;Chromium\\\u0026#34;;v=\\\u0026#34;126\\\u0026#34;, \\\u0026#34;Google Chrome\\\u0026#34;;v=\\\u0026#34;126\\\u0026#34;\u0026#34; ], \u0026#34;cache-control\u0026#34;: [ \u0026#34;max-age=0\u0026#34; ], \u0026#34;user-agent\u0026#34;: [ \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\u0026#34; ], \u0026#34;content-length\u0026#34;: [ \u0026#34;0\u0026#34; ], \u0026#34;x-amzn-trace-id\u0026#34;: [ \u0026#34;Self=1-66d8d99b-3a444ca8714c1d0130b2cdec;Root=1-66d8d99b-79b4667132cf44bc552d3de3\u0026#34; ], \u0026#34;host\u0026#34;: [ \u0026#34;geosociatal.uat.scrawlr.com\u0026#34; ], \u0026#34;x-forwarded-port\u0026#34;: [ \u0026#34;443\u0026#34; ], \u0026#34;x-forwarded-proto\u0026#34;: [ \u0026#34;https\u0026#34; ], \u0026#34;x-forwarded-for\u0026#34;: [ \u0026#34;10.0.1.70\u0026#34; ] } Is there any way to align the header of HTTP API gateway and Application Load Balancer to use forwarded or x-forwarded-for? # "},{"id":7,"href":"/docs/cloud-computing-and-infrastructure/aws/api-gateway/payload-limitation/","title":"Payload Limitation","section":"API Gateway","content":" API Gateway has a hard limitation of payload as 10MB # Reference: # https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html\n"},{"id":8,"href":"/docs/cloud-computing-and-infrastructure/aws/api-gateway/redirect-http-to-https/","title":"Redirect HTTP to HTTPS","section":"API Gateway","content":" How to redirect API gateway http to https? # When use API Gateway HTTP API with custom domains(regional), it only works on HTTPS and not on HTTP.\nFor example, if you have a custom domain like www.example.com, and in your reout53 DNS record you redirect that domain to the involke URL of API gateway, it works fine when a user access you website via https://www.example.com. However, because the api gateway doesn\u0026rsquo;t support http requests, it will raise ERR_CONNECTION_REFUSED error.\nSo, what should we do if we want to redirect users who enter the URL in the browser search bar manually with http? Cloudfront is one and the only solution according to this link.\nReference: # https://www.reddit.com/r/aws/comments/y30uee/api_gateway_http_to_https_redirection/ https://blog.hyper.io/using-cloudfront-to-redirect-http-to-https-on-apigateway/\n"},{"id":9,"href":"/docs/cloud-computing-and-infrastructure/aws/cloudfront/","title":"Cloudfront","section":"Aws","content":" QUESTION: # Is it a common practice for major corporations to use an intermediary domain layer in conjunction with Amazon CloudFront, rather than directly linking their public domain to a CloudFront URL? For instance, rather than associating the domain \u0026lsquo;dev.example.com\u0026rsquo; directly with a CloudFront distribution, a company might first link it to an alternate domain like \u0026lsquo;dev.aws.additional.com\u0026rsquo;, which then redirects to the actual CloudFront service. What are the reasons and technologies involved in setting up \u0026lsquo;dev.example.com\u0026rsquo; to work in this manner?\nANSWER: # Large organizations often deploy an intermediate domain layer to manage web traffic through services like Amazon CloudFront for enhanced control, security, and efficiency. Here\u0026rsquo;s how they do it:\nDNS Configuration: They configure their DNS with Amazon Route 53 (or other DNS services) to point a subdomain (like \u0026lsquo;dev.aws.additional.com\u0026rsquo;) to their CloudFront distribution URL. This subdomain acts as the intermediary.\nRedirection and Traffic Management: With this setup, they can then redirect traffic from their main domain (\u0026lsquo;dev.example.com\u0026rsquo;) to the intermediary subdomain. This redirection can be handled through various methods, such as HTTP redirection, URL rewriting, or other DNS-level configurations.\nEnhanced Security: The intermediary layer can host security measures like WAFs that screen incoming traffic for threats before passing legitimate requests to CloudFront, thereby safeguarding the backend infrastructure.\nOperational Flexibility: By separating the public domain from the CloudFront URL, companies gain the flexibility to switch between different CDN services or configurations without altering the public-facing DNS records.\nPerformance Enhancements: The intermediate layer provides a location to implement additional performance optimization techniques on top of what CloudFront offers, such as custom caching rules, compression, or even pre-processing requests.\nService Integration: They can integrate additional AWS services or third-party solutions at this layer to enhance functionality. For example, they might use AWS Lambda@Edge to run custom code closer to users for personalization or to manage traffic routing logic.\nCompliance and Data Processing: Some companies have specific compliance requirements that dictate how and where data is processed. An intermediary layer can help ensure that these requirements are met before data is distributed through the CDN.\nCost Management and Analytics: The intermediate layer can also be a strategic point for logging and analyzing traffic patterns, which can inform cost management strategies and help optimize resource allocation.\nBy employing an intermediate layer, companies can create a robust and flexible architecture that can scale with their needs, protect their resources, and provide a better experience for end-users.\nAdditional Feature # Creating the Intermediate Layer with Realistic Examples:\nSubdomain and DNS Configuration: A company could set up a subdomain like intermediate.example.com which is pointed to their CloudFront distribution. This is done by creating a CNAME record in the DNS configuration that associates intermediate.example.com with the CloudFront distribution\u0026rsquo;s domain (e.g., d1234.cloudfront.net).\nSecurity and Operations: On intermediate.example.com, they might deploy an AWS WAF to filter traffic, ensuring that only legitimate requests are forwarded to CloudFront. They could also set up AWS Shield for DDoS protection.\nCustom Logic with Lambda@Edge: They can use Lambda@Edge for executing custom code that manipulates the request/response right at the edge locations. For example, they could write functions to handle A/B testing, where some users are served different versions of the site for testing purposes.\nTraffic Routing: They might use Amazon Route 53 routing policies such as geolocation or latency-based routing at the intermediate.example.com level to direct traffic to the nearest or most efficient CloudFront distribution, potentially across multiple regions.\nSSL/TLS Certificate: They can apply an SSL/TLS certificate to intermediate.example.com to handle HTTPS requests, which may be different from the certificate used on the public-facing domain.\nLoad Balancing: If they are using multiple CDNs or have a complex infrastructure, they might set up AWS Global Accelerator or Elastic Load Balancing (ELB) to further control the flow of traffic before it hits CloudFront.\n"},{"id":10,"href":"/docs/cloud-computing-and-infrastructure/aws/cloudfront/features/","title":"Features","section":"Cloudfront","content":" What can the CloudFront do as a CDN? # In reality, CloudFront is a highly versatile Content Delivery Network (CDN) service that can distribute both static and dynamic content. Here are some key points to clarify its capabilities:\nMultiple Origin Sources:\nCloudFront can deliver content from various sources, not just S3 buckets. These sources include Amazon EC2 instances, Elastic Load Balancers, and, notably, Amazon API Gateway.\nDynamic Content Delivery:\nCloudFront can also be used to deliver dynamic content. It can forward different types of requests (GET, POST, PUT, DELETE, etc.) to the origin, which is very useful for APIs.\nCaching Mechanisms:\nWhile it\u0026rsquo;s known for caching content at edge locations for faster delivery, CloudFront provides options to control how content is cached. For APIs, you might configure CloudFront to not cache the response, effectively making it a pass-through proxy.\nSecurity and SSL/TLS Support:\nCloudFront offers SSL/TLS support and can work with AWS Certificate Manager to handle HTTPS connections, which is essential for secure data transfer and SEO.\nCustomization and Optimization:\nIt allows for URL path-based routing, query string parameters forwarding, and can even modify requests and responses. This makes it suitable for complex applications, including APIs.\nGlobal Content Delivery:\nLike other CDNs, CloudFront delivers content from the nearest edge location to the user, reducing latency. This is beneficial for both static and dynamic content.\nHTTP to HTTPS Redirection:\nAs mentioned earlier, CloudFront can handle the redirection from HTTP to HTTPS, enhancing the security of your application.\n"},{"id":11,"href":"/docs/cloud-computing-and-infrastructure/aws/cloudfront/response-header/","title":"Response Header","section":"Cloudfront","content":" Problem: # Google is indexing your staging and dev environments, which could result in duplicate content across environments. This indexing can harm the SEO of your production website by placing it in \u0026ldquo;supplemental results\u0026rdquo; (lower priority in search rankings). To avoid this, you want to add the X-Robots-Tag: noindex, nofollow header to your website hosted on S3 + CloudFront.\nSolution: # Create a Response Headers Policy in CloudFront:\nOpen your CloudFront distribution in the AWS console. Go to the Behaviors tab and edit the relevant cache behavior (e.g., /* for all content). Add a Response Headers Policy by creating a custom policy that adds the header: X-Robots-Tag: noindex, nofollow Ensure to select Overwrite if the header already exists. Attach the Policy to the Cache Behavior:\nApply the newly created Response Headers Policy to the desired cache behavior (e.g., the default /* path). Invalidate the CloudFront Cache:\nOptionally, create a cache invalidation for /* to ensure that the new header is applied immediately. Outcome: # This will ensure that your staging and dev environments are no longer indexed by search engines, preserving the SEO ranking of your production site.\n"},{"id":12,"href":"/docs/cloud-computing-and-infrastructure/aws/ecs/capacity-groups/","title":"Capacity Groups","section":"Ecs","content":" Automatically manage Amazon ECS capacity with cluster auto scaling # Amazon ECS can manage the scaling of Amazon EC2 instances that are registered to your cluster. This is referred to as Amazon ECS cluster auto scaling. You turn on managed scaling when you create the Amazon ECS Auto Scaling group capacity provider. Then, you set a target percentage (the targetCapacity) for the instance utilization in this Auto Scaling group. Amazon ECS creates two custom CloudWatch metrics and a target tracking scaling policy for your Auto Scaling group. Amazon ECS then manages the scale-in and scale-out actions based on the resource utilization that your tasks use.\nFor each Auto Scaling group capacity provider that\u0026rsquo;s associated with a cluster, Amazon ECS creates and manages the following resources:\nA low metric value CloudWatch alarm\nA high metric value CloudWatch alarm\nA target tracking scaling policy\nReference: # https://aws.amazon.com/blogs/containers/deep-dive-on-amazon-ecs-cluster-auto-scaling/ https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cluster-auto-scaling.html "},{"id":13,"href":"/docs/cloud-computing-and-infrastructure/aws/ecs/ec2_group/","title":"Ec2 Group","section":"Ecs","content":" ECS - EC2 Architecture # Comparison with Fargate # The challenge of running EC2 instances is that they are servers. You have to patch them and keep them updated. You have to manage logging, manage ssh keys, manage what happens when you run out of disk space, etc. Your business doesn\u0026rsquo;t really get any value out of managing those things specifically, but historically we\u0026rsquo;ve accepted that we have to do those things if we want to run our app.\nWhat Fargate gives you is the ability to run your app, in a container, without having to manage or care about any of that stuff mentioned above. That\u0026rsquo;s the value proposition that Fargate gives you. Run the app that your business cares about, and don\u0026rsquo;t care about anything else.\nBut this isn\u0026rsquo;t Lambda, it\u0026rsquo;s still a container that\u0026rsquo;s running (and costing) all the time, even if nobody is using it. If paying for potentially idle compute time is a deal-breaker for you, then you\u0026rsquo;ll want to look into refactoring your application to run on Lambda. This is probably not a small task, and in some cases not a possible task at all.\nAs for always-running services like web server, managing ECS with EC2 instances is about 20% cheaper than using ECS with Fargate.\nReference: https://www.clickittech.com/devops/fargate-pricing/\nComparing AL2 and Amazon Linux 2023 # Refenrence: https://docs.aws.amazon.com/linux/al2023/ug/compare-with-al2.html\n"},{"id":14,"href":"/docs/cloud-computing-and-infrastructure/aws/ecs/ecs/","title":"Ecs","section":"Ecs","content":" Exploring Ordered Placement Strategies in Container Orchestration # In the realm of container orchestration and management, particularly in cloud environments, choosing the right placement strategy for containers can significantly impact the performance and cost-efficiency of your infrastructure. Placement strategies define how containers (or tasks) are distributed across a cluster of machines, ensuring optimal resource usage and fault tolerance. In this post, we’ll explore three common strategies: binpack, random, and spread.\n1. Binpack Placement Strategy # Type: binpack Purpose: The Binpack strategy focuses on resource optimization, packing containers onto the least number of instances while utilizing as much CPU and memory as possible. By minimizing unused resources on individual instances, you can reduce the overall number of running instances, cutting costs. Field: Valid values for the field attribute in the binpack strategy are memory and cpu. This indicates the strategy will focus on packing containers based on either available memory or CPU usage. Use Case: Ideal for applications that can tolerate high resource utilization on fewer instances. It’s a cost-saving strategy when minimizing the number of running instances is a priority. 2. Random Placement Strategy # Type: random Purpose: As the name suggests, this strategy randomly places containers across the available instances. There is no specific logic to optimize for resources or balance across instances. Field: The field attribute is not required for this strategy. Use Case: The Random strategy works well when there are no strong requirements regarding resource distribution or utilization. It offers a simple way to spread containers without over-complicating the placement logic. 3. Spread Placement Strategy # Type: spread Purpose: The Spread strategy evenly distributes containers across all available instances, ensuring no single instance becomes a bottleneck. Field: In the spread strategy, the field attribute accepts values like instanceId, host, or any custom attribute applied to container instances. Use Case: This strategy is perfect for ensuring fault tolerance and load balancing across the cluster. By spreading containers evenly, it helps prevent any single instance from becoming overwhelmed, improving the reliability of your application. Summary of Placement Strategies: # Binpack: Packs containers tightly on the least number of instances, optimizing for resource utilization and cost savings. Random: Distributes containers with no specific logic or pattern, useful for scenarios without strict resource constraints. Spread: Ensures even distribution across instances, improving fault tolerance and load balancing. Managing ECS Cluster Capacity Providers with Terraform # When working with Amazon ECS (Elastic Container Service) in Terraform, managing capacity providers for ECS clusters can sometimes present challenges, particularly when you need to update or attach multiple providers. A common resource in this setup is aws_ecs_cluster_capacity_providers.\nKey Consideration: # You should only have one aws_ecs_cluster_capacity_providers resource per ECS cluster. If you want to attach multiple capacity providers to a single cluster, you can modify the capacity_providers parameter in the existing resource definition, adding new providers to the set.\nCommon Issue: # If multiple aws_ecs_cluster_capacity_providers resources are defined for a single ECS cluster, you might encounter this error:\nResourceInUseException: The specified capacity provider is in use and cannot be removed. To avoid this, ensure that you are updating the existing resource instead of creating new ones.\nThis blog post covers critical strategies for container placement and best practices when working with ECS cluster capacity providers. Understanding these concepts can help improve the efficiency of your containerized workloads and avoid common pitfalls in cluster management.\n"},{"id":15,"href":"/docs/cloud-computing-and-infrastructure/aws/ecs/fargate/","title":"Fargate","section":"Ecs","content":" ECS - Fargate Architecture # Pricing Structure # Fargate pricing is based on five independently configurable dimensions:\nvCPU Memory Operating System CPU Architecture System Resources "},{"id":16,"href":"/docs/cloud-computing-and-infrastructure/aws/ecs/how-to-deploy-stateful-application/","title":"How to Deploy Stateful Application","section":"Ecs","content":" How to deploy stateful application in ECS? # Amazon Elastic Container Service (ECS) doesn\u0026rsquo;t have a direct equivalent to StatefulSets in Kubernetes. However, ECS has various features and strategies that can be used to manage stateful applications, although with different mechanisms compared to Kubernetes.\nECS Concepts Comparable to StatefulSets: # Task Definitions: In ECS, you define your application using task definitions. While they don\u0026rsquo;t manage state in the same way as StatefulSets, task definitions are the closest you get to defining the desired state of your application in ECS.\nService Placement Strategies and Constraints: ECS allows you to use placement strategies and constraints to exert some control over how tasks are placed. This can be somewhat similar to how StatefulSets manage pod placement, but it\u0026rsquo;s less prescriptive about instance identity and order.\nElastic File System (EFS) Integration: For persistent storage, ECS tasks can be integrated with AWS Elastic File System (EFS), which can mimic the persistent volume feature of StatefulSets to some extent.\nSticky Sessions with Load Balancers: For maintaining user session state, ECS can utilize sticky sessions with load balancers, which is different from how StatefulSets manage state but can achieve a similar goal for certain types of applications.\nConclusion: # While ECS provides mechanisms for handling state and maintaining persistence, it does not have a direct analogue to Kubernetes\u0026rsquo; StatefulSets. The management of stateful applications in ECS relies more on a combination of task definitions, storage solutions like EFS, and load balancing strategies.\nECS\u0026rsquo;s approach is more about leveraging AWS\u0026rsquo;s cloud infrastructure and services to manage state, rather than a specific orchestrated container management feature like StatefulSets.\n"},{"id":17,"href":"/docs/cloud-computing-and-infrastructure/aws/ecs/network_modes/","title":"Network Modes","section":"Ecs","content":" ECS Network Modes # In AWS ECS (Elastic Container Service), the networking section refers to the configuration of how the containers within a task communicate with each other and with the outside world. There are several network modes available in ECS, each offering different capabilities and levels of isolation. Understanding these modes is crucial for designing your ECS deployment according to your application\u0026rsquo;s networking requirements. Here\u0026rsquo;s a breakdown of the different network modes:\nNone: In this mode, the container doesn\u0026rsquo;t have its own network stack. It is effectively isolated from the outside network and cannot make any inbound or outbound network calls. This mode is rarely used, except in specialized circumstances where network isolation is desired.\nBridge: This is the default network mode for ECS tasks. In bridge mode, Docker creates a network bridge on the host system and assigns each container its own network namespace and IP address. Containers can communicate with each other over this bridge. This mode provides a balance between isolation and network functionality.\nHost: When you choose the host mode, the container shares the network stack of the EC2 instance on which it\u0026rsquo;s running. This means that the container\u0026rsquo;s network interfaces and ports are directly mapped to the EC2 instance\u0026rsquo;s interfaces and ports. This mode is typically used when you need high-performance networking or when the container needs to open a large number of ports.\nAWSVPC: This mode gives each ECS task its own Elastic Network Interface (ENI) and a private IP address from the VPC, providing the task with full networking features, such as VPC security groups, subnets, and route tables. This mode offers the highest level of network isolation and is useful for tasks that require a high degree of network control and security.\nFargate: When using AWS Fargate to launch ECS tasks, each task gets its own ENI and IP address, similar to the AWSVPC mode. However, Fargate abstracts away the underlying EC2 instances, so you don\u0026rsquo;t have to manage them. This mode is ideal for users who want a serverless container experience.\nTask Level Network Setting # The AWSVPC network mode in ECS provides network configuration at the task level, which is distinct from the network configuration of the EC2 instances (Capacity Providers) in your ECS cluster. Here’s why it’s useful:\nTask-Level Network Isolation: In AWSVPC mode, each ECS task is assigned its own Elastic Network Interface (ENI) with a private IP address from your VPC. This means that each task has its own network environment, which is independent of the underlying EC2 instance\u0026rsquo;s network configuration. This setup is particularly beneficial for enhanced network isolation and security at the task level.\nSecurity and Control: With AWSVPC, you can apply VPC security groups and network ACLs directly to individual tasks. This level of granularity is crucial for implementing strict security rules and controlling access to and from the tasks.\nSimplified Port Management: Since each task has its own network interface, you don\u0026rsquo;t have to worry about port conflicts between tasks on the same instance. This simplifies the management of network ports, especially for applications that open a large number of ports.\nDirect VPC Integration: Tasks in AWSVPC mode can directly interact with other AWS services configured within the same VPC. This integration is beneficial for applications that need to communicate with databases, caches, or other AWS resources within a private network.\nUse Case Specificity: While the EC2 instances in your cluster might have their network configurations, the AWSVPC mode allows specific ECS tasks to have network settings tailored to their particular needs, which might differ from the general configuration of the EC2 instances.\n"},{"id":18,"href":"/docs/cloud-computing-and-infrastructure/aws/ecs/sdk/","title":"Sdk","section":"Ecs","content":" Enter ECS Container # aws ecs execute-command \u0026ndash;region us-west-2 \u0026ndash;cluster arn:aws:ecs:us-west-2:999999999999:cluster/XXXXXXXXXXXXX \u0026ndash;task 00000000000000000000000000000000 \u0026ndash;container XXXXXXXXXXXXX \u0026ndash;command \u0026ldquo;bash\u0026rdquo; \u0026ndash;interactive\n"},{"id":19,"href":"/docs/cloud-computing-and-infrastructure/aws/efs/","title":"Efs","section":"Aws","content":" What is an EFS? # Elastic File System (EFS) in Amazon Web Services (AWS) is a cloud-based file storage service for applications and workloads that run in the AWS Cloud. It\u0026rsquo;s designed to be easy to use and offers a simple interface that allows you to create and configure file systems quickly. Here are some key features of EFS:\nScalability: EFS automatically scales without needing to provision storage capacity or performance. This means it can grow and shrink automatically as you add and remove files, making it a good choice for applications with fluctuating storage needs.\nHighly Available and Durable: EFS is designed to be highly available and durable. It stores data across multiple Availability Zones (AZs) in an AWS Region, ensuring high availability and reliability.\nShared File Access: EFS supports the Network File System (NFS) protocol, which means that multiple EC2 instances and other AWS services can access your file system simultaneously, making it suitable for scenarios where shared file storage is needed.\nPerformance Modes: EFS offers two performance modes - General Purpose, which is the default and is suitable for a broad range of applications, and Max I/O, which is optimized for applications requiring higher levels of I/O throughput.\nLifecycle Management: EFS can automatically move your files to lower-cost storage classes when they are not accessed for a period of time. This feature helps in optimizing costs according to access patterns.\nSecurity: EFS integrates with AWS Identity and Access Management (IAM) for access control and also supports POSIX permissions. It offers encryption of data at rest and in transit.\nIntegration with AWS Services: EFS integrates seamlessly with other AWS services like Amazon EC2, AWS Lambda, and container services such as Amazon ECS and EKS, allowing for a wide range of cloud-based applications and use cases.\nEFS is often used for use cases such as content management, data analytics, media processing workflows, and home directories, where shared file storage is necessary. It\u0026rsquo;s particularly advantageous in situations where you need a file system that can automatically adjust to fluctuating storage demands without manual intervention.\nChoice between using S3 and EFS? # Amazon EFS # Use EFS when:\nFile System Interface: You need a traditional file system interface and file system semantics (like POSIX). This is essential for applications that are designed for a file system hierarchy.\nShared Access: Your application requires shared access to files, where multiple instances or services need to read and write to the same file system simultaneously.\nLow Latency: Your application needs low-latency access to files. EFS is particularly suited for use cases where millisecond-level latencies are important.\nAutomatically Scaling Storage: You require a storage solution that automatically scales with your usage, without needing to manage the scaling process.\nIntegration with EC2 and other AWS Services: You need tight integration with AWS compute services like EC2, AWS Lambda, and container services such as ECS and EKS.\nTypical use cases for EFS include web serving and content management, data analytics, application testing and development environments, and database backups.\nAmazon S3 # Use S3 when:\nObject Storage: Your application works with data as objects (not files), and you require capabilities like metadata tagging, versioning, and lifecycle policies.\nDurability and Scalability: You need high durability (S3 provides 99.999999999% durability) and the ability to store and manage a massive amount of data.\nData Distribution: You need to distribute content with URLs (S3 can host static websites and distribute large files).\nArchiving and Backup: For use cases involving data archiving, backup, and disaster recovery where the immediate availability of data is less critical.\nEvent-Driven Computing: If you are using AWS Lambda for serverless computing, S3 can act as a trigger for Lambda functions.\nCost-Effective Storage: If you need a more cost-effective solution for storing large amounts of data. S3 offers various storage classes for different use cases, such as S3 Standard, S3 Infrequent Access, and S3 Glacier for long-term archival.\nTypical use cases for S3 include storing and serving static resources for web applications, data lakes, big data analytics, and backup and disaster recovery solutions.\nSummary # Use EFS: When you need a file system for shared, low-latency access, and for applications built for a file system structure. Use S3: For object storage with massive scalability, for static web hosting, and for cost-effective storage with various access patterns. Both EFS and S3 are highly reliable and integrate well with other AWS services, but the choice largely depends on whether you need file-level or object-level storage and the specific access patterns of your application.\nReal-world usecase # Amazon S3 Use Cases # Website Hosting: A company hosts a static website on AWS. All the website\u0026rsquo;s static assets like HTML, CSS, JavaScript files, and images are stored in an S3 bucket. S3 is ideal for this because it can serve these files directly over the web, handle high levels of traffic, and is cost-effective for storing web content.\nData Lakes and Big Data Analytics: A business uses S3 to store massive datasets collected from various sources. The data is then processed and analyzed using big data processing services like Amazon EMR (Elastic MapReduce) or Redshift. S3 is suitable for this due to its scalability, data durability, and integration with these analytics services.\nBackup and Disaster Recovery: A company backs up critical data to S3 because it offers high durability, ensuring data safety. In case of a disaster, the data can be quickly retrieved and restored. S3\u0026rsquo;s versioning and lifecycle policies also help in managing the backups efficiently.\nMedia Hosting: A media company uses S3 to store and distribute large video files. S3 is used due to its ability to handle large files and high throughput, alongside integration with CDN services like Amazon CloudFront for efficient distribution.\nAmazon EFS Use Cases # Shared File Storage for Compute Instances: A company uses a cluster of EC2 instances for processing complex scientific simulations. These instances need shared, fast access to a common dataset. EFS is used here as it provides a common file system that can be mounted across all EC2 instances, allowing for efficient data sharing and high-performance read/write operations.\nContent Management Systems: A content management system (CMS) deployed on AWS requires a shared file system to store and manage web content, templates, and media files. EFS is used because it allows multiple servers (like web and application servers) to access and serve the same content.\nSoftware Development and Testing: For software development and testing environments, multiple developers need to access and modify application code and configurations. EFS provides a shared storage solution where files can be consistently and concurrently accessed and updated.\nContainer Storage: An application deployed using container services like AWS ECS or Kubernetes on AWS requires persistent, shared storage for containers. EFS is suitable as it can be easily integrated as shared file storage for containers, allowing them to read, write, and persist data consistently.\nSummary # Use S3: For static content hosting, large-scale data storage, backups, and scenarios needing high durability and scalability. It\u0026rsquo;s ideal for \u0026ldquo;write once, read many\u0026rdquo; scenarios. Use EFS: For applications needing a shared file system, low-latency file access, and consistent performance across multiple EC2 instances or containers. It\u0026rsquo;s suited for \u0026ldquo;read and write many times\u0026rdquo; scenarios. In what scenario that ECS fits better than S3? # Scenario: Collaborative Application Development Environment # Imagine a software development company working on a large-scale application. This project involves multiple teams of developers, testers, and DevOps engineers who need to collaborate efficiently. They require a shared environment where code, libraries, and other development artifacts are stored and can be accessed and modified concurrently.\nRequirements of This Scenario:\nFile Locking and Concurrent Access: Developers need to edit code files simultaneously. A file system supports file locking mechanisms to prevent conflicts, ensuring that only one user at a time can modify a particular file.\nLow-Latency Access: The development environment demands low-latency access to files for compiling code, running tests, and performing continuous integration/continuous deployment (CI/CD) tasks. File-system storage provides faster read/write operations compared to object storage.\nPOSIX-Compliance: Many development tools and applications rely on POSIX-compliant file systems for operations like seeking specific positions in files, renaming files, and directory manipulations. POSIX compliance is not something S3 provides.\nHierarchical File Structure: Developers are accustomed to a hierarchical file structure (directories and subdirectories), which is a natural way to organize project files. File-system storage aligns well with this requirement.\nWhy S3 is Not Suitable Here:\nS3 is an object storage solution, not a file system. It does not support file locking or allow for in-place edits. Each time a file is modified, it needs to be uploaded as a new object. Accessing data in S3 can have higher latency compared to file-system storage. This could slow down development processes that require frequent read/write operations. S3 is not POSIX-compliant, which can limit its compatibility with certain development tools or scripts that expect a file system environment. S3\u0026rsquo;s flat namespace structure might not be ideal for organizing files in the way required by development teams. In this scenario, a solution like Amazon EFS would be more suitable due to its POSIX-compliant file system, low-latency characteristics, and support for concurrent access with proper file locking mechanisms. It provides an environment similar to traditional on-premises file servers, which is a key requirement for collaborative software development and deployment environments.\nEFS Usage Example with Code # Step 1: Create an EFS File System # You can create an EFS file system using the AWS Management Console or the AWS CLI. For example, using the AWS CLI:\naws efs create-file-system --creation-token MyEfsFileSystem This command creates a new file system. You\u0026rsquo;ll get a File System ID (e.g., fs-12345678) in response.\nStep 2: Create Mount Targets # After creating the file system, you need to create mount targets in your VPC:\naws efs create-mount-target --file-system-id fs-12345678 --subnet-id subnet-abcdefgh --security-groups sg-12345678 This command creates a mount target in a specific subnet and security group.\nStep 3: Mount the File System on EC2 Instances # Now, you can mount the file system on your EC2 instances. First, ensure that the amazon-efs-utils package is installed on your EC2 instance:\nFor Amazon Linux or RHEL:\nsudo yum install -y amazon-efs-utils For Ubuntu:\nsudo apt-get install -y amazon-efs-utils Then, you can mount the file system:\nsudo mount -t efs fs-12345678:/ /mnt/efs This command mounts the EFS file system to a directory on your EC2 instance (here /mnt/efs).\nExample Use Cases with Code # Shared Data for Web Servers: If you\u0026rsquo;re running multiple web servers in an Auto Scaling group, you can use EFS to store shared assets like images or stylesheets. The mount command (sudo mount -t efs fs-12345678:/ /mnt/efs) would be part of your EC2 instance\u0026rsquo;s initialization script.\nStoring Application Logs: You can configure your applications to write logs directly to a directory mounted on EFS. For instance, configure your logging framework to write logs to /mnt/efs/logs.\nPersistent Storage for Containers: If you\u0026rsquo;re using AWS ECS or Kubernetes on AWS (EKS), you can mount an EFS file system to your containers for persistent storage. In Kubernetes, you\u0026rsquo;d use a PersistentVolume (PV) and PersistentVolumeClaim (PVC) referencing your EFS.\nKubernetes PV example:\napiVersion: v1 kind: PersistentVolume metadata: name: efs-pv spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: efs csi: driver: efs.csi.aws.com volumeHandle: [FileSystemId]::[AccessPointId] This YAML file defines a PersistentVolume that points to your EFS file system.\nRemember, these examples assume you\u0026rsquo;ve set up your AWS environment, including your VPC, security groups, and IAM roles. Additionally, always ensure your security groups and network ACLs allow the necessary traffic to and from your EFS mount points.\n"},{"id":20,"href":"/docs/cloud-computing-and-infrastructure/aws/eks/","title":"Eks","section":"Aws","content":" Workloads # PodTemplates: Templates for creating pods. Generally, you don\u0026rsquo;t create these directly but define them in other resources like Deployments. Pods: Basic execution units in Kubernetes. Create with kubectl run or define in a YAML file and use kubectl apply. ReplicaSets: Ensure a specified number of pod replicas are running. Created as part of Deployments. Deployments: Manage stateless applications, handle ReplicaSets. Create with a Deployment YAML file. StatefulSets: Like Deployments but for stateful applications. Create with a StatefulSet YAML file. DaemonSets: Ensure all (or some) Nodes run a copy of a Pod. Created with a DaemonSet YAML file. Jobs: Run a task once to completion. Create with a Job YAML file. CronJobs: Schedule Jobs to run at specific times. Create with a CronJob YAML file. Workload Support # PriorityClasses: Define the importance of Pods. Created with a PriorityClass YAML file. HorizontalPodAutoscalers: Automatically scale the number of Pods. Created with an HPA YAML file. Cluster Architecture # Cluster: Represents the Kubernetes cluster itself. Nodes: Worker machines in Kubernetes. Managed through AWS EKS. Namespaces: Logical separation within your cluster. Create with kubectl create namespace. API Extensions # APIServices: Extend Kubernetes API. Created as part of API extension server setup. Leases: Lightweight resource to coordinate across a cluster. Often managed internally by Kubernetes. RuntimeClasses: Define different container runtimes. Create with a RuntimeClass YAML file. FlowSchemas and PriorityLevelConfigurations: Manage API request prioritization. Service and Networking # Services: Abstract way to expose applications. Create with a Service YAML file. Endpoints: IP addresses to reach a Service. Often managed by Kubernetes itself. EndpointSlices: Scalable version of Endpoints. Managed by Kubernetes. Ingresses: Manage external access to services. Create with an Ingress YAML file. IngressClasses: Define types of Ingresses. Config and Secrets # ConfigMaps: Key-value pairs for configuration. Create with a ConfigMap YAML file. Secrets: Store sensitive data. Create with a Secret YAML file. Storage # PersistentVolumeClaims (PVCs): Request storage resources. Created by users. PersistentVolumes (PVs): Represents storage in the cluster. Created by admins. StorageClasses: Define types of storage offered. Created by admins. VolumeAttachments: Represents attachment of a volume to a node. CSIDrivers and CSINodes: Represent Container Storage Interface (CSI) drivers and nodes. Authentication and Authorization # ServiceAccounts: Accounts for processes in pods. Create with a ServiceAccount YAML file. ClusterRoles and ClusterRoleBindings: Define permissions across a cluster. Roles and RoleBindings: Define permissions in a namespace. Policy # LimitRanges: Impose constraints on resource use. ResourceQuotas: Enforce limits across a namespace. NetworkPolicies: Define network access rules. PodDisruptionBudgets: Limit disruptions to applications. Extensions # CustomResourceDefinitions (CRDs): Extend Kubernetes resources. MutatingWebhookConfigurations and ValidatingWebhookConfigurations: Manage admission webhooks. Each of these resources is typically defined in a YAML file and created with kubectl apply -f [filename]. The specific structure of the YAML file varies based on the resource type. You can find templates and examples for these in the Kubernetes documentation or online resources tailored to EKS.\n"},{"id":21,"href":"/docs/cloud-computing-and-infrastructure/aws/eks/cluster/","title":"Cluster","section":"Eks","content":" Cluster in Amazon Elastic Kubernetes Service (EKS) # Overview # Amazon Elastic Kubernetes Service (EKS) is a managed service that makes it easy to run Kubernetes on AWS without needing to install and operate your own Kubernetes control plane or worker nodes.\nWhat is a Cluster? # In the context of Amazon EKS, a cluster refers to the Kubernetes control plane and the compute infrastructure it manages. The control plane runs in an account managed by AWS, and the user manages the compute infrastructure.\nComponents of an EKS Cluster # Control Plane:\nManages the Kubernetes API server and the backend persistence layer for the cluster’s state. It is managed by AWS and is highly available across multiple Availability Zones. Worker Nodes:\nThese are EC2 instances that run your application containers. You can manage these instances, or you can use AWS Fargate for a serverless option where you don\u0026rsquo;t have to manage servers. Kubernetes Resources:\nSuch as pods, deployments, and services, are configured and deployed in the cluster. Key Features # High Availability: Control plane runs across multiple AWS availability zones, ensuring high availability. Security: Integrates with AWS security services like IAM for fine-grained access control. Scalability: Supports large-scale container deployments. Compatibility: Fully compatible with standard Kubernetes, ensuring easy migration of any standard Kubernetes application. Use Cases # Running containerized applications at scale. Microservices architecture deployment. Integrating with AWS services and features for enhanced functionality. EKS simplifies the process of running Kubernetes on AWS by abstracting away the complexity of setting up and managing the Kubernetes control plane and nodes.\n"},{"id":22,"href":"/docs/cloud-computing-and-infrastructure/aws/eks/deployment/","title":"Deployment","section":"Eks","content":" Deployment in Kubernetes (K8s) # What is a Deployment? # A Deployment in Kubernetes is a resource object that provides declarative updates to applications. It allows you to describe an application’s life cycle, such as which images to use for the app, the number of pods there should be, and the way to update them, among other aspects.\nKey Features of Deployment # Replica Management: Deployments allow you to specify how many replicas (copies) of a pod should be running. It ensures that the desired number of pods are always available.\nUpdate Handling: Deployments can manage updates to your application. You can define new images or configurations, and Kubernetes will update the pods in a controlled way (rolling update).\nSelf-healing: If a pod fails, the Deployment will replace it to ensure the desired number of pods is maintained.\nScaling: Deployments can be easily scaled up or down. This can be done manually or automatically based on specific metrics (e.g., CPU usage).\nCreating a Deployment # To create a Deployment, you define a YAML file with the desired state of the deployment. This includes details like the image to use, ports, replicas, and more.\napiVersion: apps/v1 kind: Deployment metadata: name: example-deployment spec: replicas: 3 selector: matchLabels: app: example template: metadata: labels: app: example spec: containers: - name: example image: example/image:v1 ports: - containerPort: 80 Comparison: Kubernetes Deployment vs. Amazon ECS Service # Kubernetes Deployment # Definition # A Deployment in Kubernetes is a resource object that provides declarative updates to applications. It manages the deployment and scaling of a set of Pods and ensures that the desired state of the application is maintained. Key Features # Automated Scaling and Management: Automatically manages the number of pod replicas based on configurations. Rolling Updates and Rollbacks: Supports automated updates and rollbacks for applications. Self-Healing: Automatically replaces failed pods to ensure the desired number of pods is always running. Declarative Configuration: Uses YAML or JSON for configuration. Use Cases # Ideal for stateless applications where scaling and automated management are important. Useful for implementing continuous integration and continuous deployment (CI/CD) strategies. Amazon ECS Service # Definition # An ECS Service enables you to run and maintain a specified number of instances of a task definition simultaneously in an Amazon ECS cluster. Key Features # Integration with AWS Infrastructure: Seamlessly integrates with AWS load balancers, Auto Scaling, and other AWS services. Service Types: Supports both long-running services and one-off batch jobs. Service Discovery: Utilizes AWS Cloud Map for service discovery, allowing applications to quickly and easily discover endpoints for connected AWS services. Role-Based Access Control: Tightly integrated with AWS IAM for granular control and security. Use Cases # Suitable for both web applications and batch processing jobs in the AWS ecosystem. Best suited for applications that are tightly integrated with other AWS services. Key Differences # Platform Specificity: Kubernetes Deployment is platform-agnostic and can run on any Kubernetes cluster, while Amazon ECS Service is specific to the AWS ecosystem. Configuration and Management: Kubernetes uses declarative YAML/JSON configurations, whereas ECS services can be configured via the AWS Management Console or using AWS CLI and SDKs. Load Balancing: Kubernetes deployments need manual setup for load balancing, whereas ECS services have direct integration with AWS Elastic Load Balancing. Service Discovery: Kubernetes requires additional setup for service discovery, whereas ECS services can utilize AWS Cloud Map for easier discovery and integration. In conclusion, while both Kubernetes Deployments and Amazon ECS Services manage containerized applications, they differ in terms of platform dependency, configuration approaches, and integration with other services and infrastructure.\n"},{"id":23,"href":"/docs/cloud-computing-and-infrastructure/aws/eks/pod/","title":"Pod","section":"Eks","content":" Pod in Amazon Elastic Kubernetes Service (EKS) # Overview # A pod in Amazon Elastic Kubernetes Service (EKS) is the smallest and most basic deployable object in Kubernetes. It represents a single instance of a process running in your cluster.\nUnderstanding Pods # Fundamental Unit of Deployment: Pods are the basic building blocks of Kubernetes applications. Containers Within Pods: Each pod can contain one or more containers, usually with closely related functionality. Shared Resources: Containers in a pod share the same network IP, port space, and storage. Key Characteristics # Atomic Unit: Pods are the atomic unit on the Kubernetes platform. Ephemeral Nature: They are typically ephemeral and disposable, meaning they can be easily created, destroyed, and replaced. Replication: Managed by higher-level Kubernetes constructs like Deployments or ReplicaSets. Components of a Pod # Containers: The application component, running the actual code. Volumes: Attachable storage accessible by the containers. Networking: Unique cluster IP address for interacting with other pods or services. Spec: The specification to define the pod’s contents and behavior. Pod Lifecycle # Pending: The pod has been created but is not yet running. Running: At least one container in the pod is running. Succeeded/Failed: All containers in the pod have terminated, with a success or failure status. Unknown: The state of the pod cannot be determined. Use Cases # Running Single-container Applications: Simplest use case, with one container per pod. Co-located, Co-managed Helper Containers: Such as log watchers, data loaders, etc., alongside the main application container. Pods as Management Units: For scaling, deployment, and replication. Conclusion # Pods are the basic, deployable units in EKS that allow you to run and manage applications in a Kubernetes environment. They are crucial for understanding how Kubernetes orchestrates containerized applications.\nPod in EKS VS Task in ECS # A pod in Kubernetes is conceptually similar to a task in Amazon ECS (Elastic Container Service). Both are fundamental units that represent a set of running containers in their respective environments. However, there are some key differences and nuances:\nContainer Grouping:\nIn Kubernetes, a pod can contain one or more containers. These containers are tightly coupled and share resources like network and storage. In ECS, a task is a grouping of containers that are deployed on the same ECS container instance or AWS Fargate. Each container within a task can use different resources and settings. Resource Sharing:\nKubernetes pods share network and storage resources, and containers in the same pod can communicate over localhost. In ECS, containers within a task can share resources and communicate with each other, but they are more independent compared to Kubernetes pods. Scheduling and Management:\nKubernetes handles the scheduling and lifecycle of pods, providing more complex orchestration capabilities like auto-scaling, rolling updates, and self-healing. ECS tasks are managed by ECS and can be scheduled on either ECS container instances (EC2 instances part of an ECS cluster) or using Fargate, which abstracts the underlying server infrastructure. Use Cases:\nIn both systems, the idea is to group containers that need to work closely together. However, Kubernetes tends to be more flexible and feature-rich, suitable for a wide range of use cases and complex scenarios. ECS is often seen as simpler to use and integrates tightly with other AWS services. It\u0026rsquo;s preferred in environments that are heavily invested in AWS. In summary, while both Kubernetes pods and ECS tasks represent a model for running and managing containers, they differ in terms of their features, flexibility, and integration with their respective ecosystems (Kubernetes and AWS).\n"},{"id":24,"href":"/docs/cloud-computing-and-infrastructure/aws/eks/podtemplate/","title":"Pod Template","section":"Eks","content":" PodTemplates in Amazon Elastic Kubernetes Service (EKS) # Overview # PodTemplates are specifications for creating Pods within Kubernetes, and by extension, in Amazon Elastic Kubernetes Service (EKS). They are templates that describe the desired state of a pod.\nUnderstanding PodTemplates # A PodTemplate is embedded in workload resources like Deployments, StatefulSets, and Jobs. The template contains specifications for pod creation, such as containers, volumes, and labels.\nStructure of a PodTemplate # apiVersion: version kind: ResourceType metadata: name: name spec: template: metadata: labels: key: value spec: containers: - name: containerName image: containerImage ports: - containerPort: port "},{"id":25,"href":"/docs/cloud-computing-and-infrastructure/aws/eks/replicasets/","title":"Replica Sets","section":"Eks","content":" ReplicaSets in Amazon Elastic Kubernetes Service (EKS) # Overview # ReplicaSets are a fundamental workload management resource in Kubernetes, and by extension, in Amazon Elastic Kubernetes Service (EKS). They ensure that a specified number of pod replicas are running at any given time.\nPurpose of ReplicaSets # Maintain Stability: ReplicaSets maintain a stable set of replica Pods running at any given time. Self-Healing: If a pod fails, the ReplicaSet replaces it to maintain the desired state. Scaling: Automatically scales the number of pods in response to changes in configuration or environment. How ReplicaSets Work # Defining ReplicaSets: A ReplicaSet is defined with fields, including the number of replicas and a pod template which describes the pods to manage.\nLabel Selector: Uses a label selector to identify the pods it should manage.\nReplicaSet Spec Example # apiVersion: apps/v1 kind: ReplicaSet metadata: name: example-replicaset spec: replicas: 3 selector: matchLabels: app: example-app template: metadata: labels: app: example-app spec: containers: - name: example-container image: nginx "},{"id":26,"href":"/docs/cloud-computing-and-infrastructure/aws/eks/statefulsets/","title":"Stateful Sets","section":"Eks","content":" 1. What is statefulSet? # StatefulSets in Kubernetes (K8s) is a workload API object used for managing stateful applications. It manages the deployment and scaling of a set of Pods and provides guarantees about the ordering and uniqueness of these Pods. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. This feature is particularly important for stateful applications that require stable, unique network identifiers, stable persistent storage, and ordered, graceful deployment and scaling.\nHere are some key aspects of StatefulSets:\nStable, Unique Network Identifiers: Each Pod in a StatefulSet derives its hostname from the name of the StatefulSet and the ordinal of the Pod. The pattern is \u0026lt;statefulset name\u0026gt;-\u0026lt;ordinal\u0026gt;. This means that each Pod’s name is predictable and maintains its identity across rescheduling.\nStable, Persistent Storage: StatefulSets can use PersistentVolumes to provide stable storage through restarts and rescheduling of Pods. When using StatefulSets, you can assign a unique PersistentVolume for each Pod in the set.\nOrdered, Graceful Deployment and Scaling: StatefulSets respect the order of deployment, scaling, and termination. For example, when Pods are being deployed, they are created sequentially, in order from {0..N-1}. When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.\nOrdered, Automated Rolling Updates: When the Pod template is updated, the Pods are terminated and recreated in a controlled and predictable way. By default, this follows the same ordering rules as during scaling.\nApplications suited for StatefulSets include databases like MySQL and PostgreSQL, clustered applications like Cassandra, and any other application that requires one or more of these properties. StatefulSets make it easier to deploy and manage such applications in a Kubernetes environment while providing the necessary guarantees for stateful workloads.\n2. StatefulSets VS Deployments # Similarities # Manage Pods: Both StatefulSets and Deployments are higher-level abstractions that manage the lifecycle of pods. Scalability: They both support scaling operations, allowing you to increase or decrease the number of pod replicas. Update Strategy: Both support rolling updates, ensuring that application changes can be deployed with minimal downtime. Declarative Configuration: You define both StatefulSets and Deployments using YAML configurations in Kubernetes. Self-Healing: They both ensure that the specified number of pods are always running and replace pods that fail. Differences # State Management: StatefulSets: Designed for stateful applications. Each pod in a StatefulSet is given a stable and unique network identifier that is maintained across rescheduling. Deployments: Suited for stateless applications. Pods in a Deployment are interchangeable, with no unique identity. Storage: StatefulSets: Commonly used with persistent storage. Each pod can be associated with its own PersistentVolume, ensuring data persistence. Deployments: Often use ephemeral or shared storage, as pods don’t need to maintain state. Pod Identity: StatefulSets: Pods have a persistent identity. The identity sticks to the pod, regardless of which node it\u0026rsquo;s scheduled on. Deployments: Pods are anonymous; a new pod is indistinguishable from the replaced ones. Scaling and Updates: StatefulSets: Scaling and updates are ordered and sequential. For example, pods are created and terminated in a specific order. Deployments: Scaling and updates can happen in parallel and do not guarantee any order. Use Cases: StatefulSets: Ideal for applications like databases, clustered applications, and other scenarios where the order and identity are important. Deployments: Best for stateless applications like web servers, front-end interfaces, and REST APIs where scaling and load balancing are primary concerns. Conclusion # Choose StatefulSets for applications where the identity, order, and state persistence of each pod are crucial. Opt for Deployments when dealing with stateless applications that require fast scaling and dynamic load balancing, where pod identity is not important. 3. Sticky Session in ECS VS StatefulSet in EKS # Sticky Sessions in ECS with Load Balancers # Sticky Sessions in Amazon Elastic Container Service (ECS) when used with load balancers, are a method to ensure that a user\u0026rsquo;s session requests are directed to the same backend instance. This is particularly useful in scenarios where session state is stored locally on the instance. Here\u0026rsquo;s how they work in ECS:\nEnabling Sticky Sessions: Sticky sessions are enabled through load balancer settings in the ECS service configuration. This can be done with both Application Load Balancers (ALB) and Classic Load Balancers. Session Affinity: When a client makes a request, the load balancer sends the request to a target (like an ECS task). With sticky sessions enabled, the load balancer creates a session cookie. Future requests from the same client are directed to the same target based on this cookie, maintaining session affinity. Duration of Stickiness: You can configure the duration for which the load balancer should maintain the stickiness. After this period, the session may be directed to a different target. Equivalent in Kubernetes StatefulSets # In Kubernetes, StatefulSets provide a different approach to maintaining state. While they don\u0026rsquo;t directly correspond to sticky sessions in load balancers, they offer a way to manage stateful applications:\nStable Identity: Each pod in a StatefulSet has a stable hostname and network identity. This means that even if a pod is rescheduled, it retains its identity, and any stateful connections or data can be consistently accessed. Persistent Storage: StatefulSets can be used with PersistentVolumeClaims to ensure that each pod has a dedicated and persistent storage attached to it. This way, even if the pod is rescheduled to a different node, its state remains intact. Ordered Deployment and Scaling: Pods in a StatefulSet are created and deleted in a specific order, which can be crucial for stateful applications where the order of scaling or updates matters. Key Differences # Nature of State Management:\nECS with Sticky Sessions: Focuses on routing client requests to the same backend instance for session consistency. Kubernetes StatefulSets: Focuses on maintaining a consistent identity and storage for pods, suitable for applications like databases or clustered applications. Use Cases:\nECS is typically used for applications where the session state is lightweight and can be stored temporarily on the instance. StatefulSets are used for applications where data persistence and order are crucial, such as database applications. In summary, sticky sessions in ECS are about directing traffic to the same instance for session consistency, whereas StatefulSets in Kubernetes offer a more integrated solution for running applications that require stable identity, persistent storage, and ordered operations.\n"},{"id":27,"href":"/docs/cloud-computing-and-infrastructure/aws/fsx/","title":"Fsx","section":"Aws","content":" What is Amazon FSx? # Amazon FSx is a set of fully managed file storage services provided by Amazon Web Services (AWS). These services are designed to offer native compatibility with popular file systems and feature-rich file system interfaces, allowing businesses and developers to easily migrate and run their file-based applications in the AWS cloud. As of my last update, Amazon FSx offered two main types of file systems:\nAmazon FSx for Windows File Server: This service provides a fully managed native Microsoft Windows file system, allowing users to easily move their Windows-based applications that rely on file storage to AWS. It supports features such as the Server Message Block (SMB) protocol, Windows NTFS, and Microsoft Active Directory integration.\nAmazon FSx for Lustre: This is a high-performance file system optimized for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. Lustre is a popular file system in compute-intensive environments and offers the ability to process large volumes of data at high speeds.\nKey features of Amazon FSx include:\nFully Managed: Amazon FSx is a fully managed service, meaning AWS handles the heavy lifting of file system maintenance, updates, and scaling.\nHigh Performance: Especially with FSx for Lustre, the service is built for high-performance workloads, providing fast processing capabilities.\nSeamless Integration: It integrates well with other AWS services, making it easier to use alongside a wide range of cloud-based applications and services.\nSecurity and Compliance: Amazon FSx includes features for encryption, access control, and compliance with various standards.\nScalability and Flexibility: Users can scale their storage capacity and performance according to the needs of their applications.\nAmazon FSx is typically used in scenarios that require a robust and scalable file storage solution, such as web-based applications, data processing tasks, content management, and more. It simplifies the management and scaling of file storage, allowing developers to focus more on their applications and less on the underlying infrastructure.\nWhat is the difference between EFS and FSx? # Amazon FSx and Amazon Elastic File System (EFS) are both file storage services offered by AWS, but they serve different needs and offer different features. Understanding the distinctions between them is important when choosing the right solution for your specific use case.\nAmazon FSx: # Compatibility: FSx provides options that are compatible with specific file systems: FSx for Windows File Server and FSx for Lustre. This makes it ideal for applications that require specific Windows or Lustre file system features. Use Cases: FSx for Windows is designed for enterprise applications, media workflows, and other use cases that need native Windows file system features. FSx for Lustre is tailored for high-performance computing, machine learning, and data processing workloads. Features: FSx offers features like SSD storage for high performance, file locking, and integration with AWS services like Direct Connect and VPN. Administration: It offers fully managed experiences with automatic backups, patching, and updates. Amazon Elastic File System (EFS): # Compatibility: EFS is a POSIX-compliant file system service that uses the NFS (Network File System) protocol. It\u0026rsquo;s suitable for Linux-based applications and can be used with a wide range of AWS services. Use Cases: EFS is ideal for scalable, elastic applications and is often used for content management, web serving, data analytics, and other general-purpose file storage needs. Features: It offers features like automatic scaling, pay-for-what-you-use pricing, and high durability and availability. Scalability and Elasticity: EFS automatically scales up or down as you add or remove files, making it a good option for workloads with fluctuating storage needs. In summary: # Amazon FSx is tailored for specific file systems (Windows File Server and Lustre) and is well-suited for specialized use cases requiring these systems\u0026rsquo; specific features. Amazon EFS is more general-purpose, offering scalable file storage for Linux-based applications, and is compatible with a broader range of AWS services. The choice between FSx and EFS largely depends on the specific requirements of your workload, including the operating system, performance needs, and the specific features of the file system that your applications require.\nProvide some real-world scenarios that show the difference between FSx and EFS? # Here are some real-world scenarios that illustrate the differences between Amazon FSx and Amazon EFS, showcasing their unique features and ideal use cases:\nAmazon FSx Scenarios # FSx for Windows File Server: Corporate File Sharing # A large enterprise uses Windows-based applications and needs a file system that seamlessly integrates with Microsoft Active Directory for user authentication and permissions. They deploy FSx for Windows File Server to provide a fully managed, native Windows file sharing environment, ensuring compatibility with their existing Windows infrastructure and simplifying the management of file shares. FSx for Lustre: High-Performance Computing (HPC) # A research institute performs complex scientific simulations that require processing vast amounts of data at high speeds. They use FSx for Lustre because it offers the high performance and low latencies needed for their HPC workloads, allowing them to efficiently run simulations and process large datasets. FSx for Windows File Server: Media Production # A media company requires a file storage solution that supports their Windows-based editing software and enables multiple editors to work on high-resolution video files concurrently. FSx for Windows File Server provides the throughput and IOPS necessary for handling large media files, while also offering native compatibility with their software. Amazon EFS Scenarios # Web Server Scaling # A tech company hosts a popular web application on AWS. As their user base grows, they need a file system that scales automatically with the increasing amount of content. EFS is chosen for its ability to scale on demand, ensuring that their web servers have continuous access to the necessary files without manual intervention for capacity planning. Shared Data in Containerized Environments # An organization uses containerized microservices for their applications. They need a shared file system accessible from multiple Amazon EC2 instances and containers. EFS provides a scalable and easy-to-use solution, allowing containers to access shared data without the complexity of managing the underlying storage infrastructure. Home Directories and User Files # A university sets up a cloud-based environment for students and faculty. They require a solution for storing home directories and user files that can be accessed from various Linux-based systems. EFS is an ideal choice because it\u0026rsquo;s compatible with Linux and offers the POSIX-compliance needed for a wide range of academic applications. In summary, FSx is typically chosen for specific use cases requiring compatibility with Windows or Lustre file systems and for workloads needing high performance and specialized features. In contrast, EFS is more suitable for general-purpose, scalable file storage in Linux-based environments, especially where automatic scaling and elasticity are crucial.\nSo can I say, if I want a file system for Windows environment, I should go for FSx, if I prefer a file system on Linux, then I should choose EFS. # Your summary is mostly accurate but with some nuances:\nFor Windows-based Applications:\nIf you want a file system for Windows-based applications, particularly those that require tight integration with Windows features like Active Directory, NTFS, and SMB protocol, then Amazon FSx for Windows File Server is typically the recommended choice. It\u0026rsquo;s specifically designed for Windows environments and offers a fully managed, native Windows file system experience.\nFor Linux-based Applications:\nIf you want a file system for Linux-based applications, especially those that require a POSIX-compliant file system with NFS support, then Amazon EFS is generally the go-to option. EFS provides a scalable, elastic file system that\u0026rsquo;s well-suited for a wide range of Linux workloads and integrates easily with other AWS services.\nHowever, it\u0026rsquo;s important to note a few things:\nFSx for Windows File Server:\nWhile FSx for Windows File Server is tailored for Windows environments, it doesn\u0026rsquo;t mean Linux systems can\u0026rsquo;t interact with it. Linux systems can mount SMB shares, but the FSx for Windows service is optimized for Windows-specific use cases.\nEFS:\nSimilarly, EFS is primarily used in Linux environments due to its POSIX compliance and NFS support, but it\u0026rsquo;s technically possible for Windows systems to interact with NFS. However, EFS is optimized for Linux use cases.\nIn summary, while the choice between FSx and EFS can often align with the operating system environment (Windows vs. Linux), the decision should also consider specific application requirements, performance needs, and the unique features each service offers.\n"},{"id":28,"href":"/docs/cloud-computing-and-infrastructure/aws/iam/all-actions-resources-conditions/","title":"All Actions Resources Conditions","section":"Iam","content":" Reference: # https://docs.aws.amazon.com/service-authorization/latest/reference/reference_policies_actions-resources-contextkeys.html\n"},{"id":29,"href":"/docs/cloud-computing-and-infrastructure/aws/iam/definition/","title":"Definition","section":"Iam","content":" How to create a IAM role with proper policy using terraform? # Trust Policy: Defines who can assume the role. Role: Defines the role itself. Permissions Policy: Defines what the role can do by specifying actions and resources. Policy Attachment: Attaches the permissions policy to the role. # Step 1: Define the trust policy data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;assume_role\u0026#34; { statement { effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Service\u0026#34; identifiers = [\u0026#34;lambda.amazonaws.com\u0026#34;] } actions = [\u0026#34;sts:AssumeRole\u0026#34;] } } # Step 2: Define the role resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;iam_for_lambda\u0026#34; { name = \u0026#34;iam_for_lambda\u0026#34; assume_role_policy = data.aws_iam_policy_document.assume_role.json } # Step 3: Define the permissions policy data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;lambda_policy\u0026#34; { statement { effect = \u0026#34;Allow\u0026#34; actions = [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ] resources = [\u0026#34;arn:aws:logs:*:*:*\u0026#34;] } } # Step 4: Attach the policy to the role resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;lambda_policy\u0026#34; { name = \u0026#34;lambda_policy\u0026#34; policy = data.aws_iam_policy_document.lambda_policy.json } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;lambda_policy_attachment\u0026#34; { role = aws_iam_role.iam_for_lambda.name policy_arn = aws_iam_policy.lambda_policy.arn } "},{"id":30,"href":"/docs/cloud-computing-and-infrastructure/aws/iam/iam-policy/","title":"Iam Policy","section":"Iam","content":" Difference between Trust Policy and Permission Policy # In AWS IAM, there are two main types of policies that are associated with roles:\nTrust Policy: Specifies who or what can assume the role. Permissions Policy: Specifies what actions the role can perform and on which resources. Trust Policy # Purpose: Defines the entities that are trusted to assume the role. Attachment: Each role can have only one trust policy. Example: Allowing AWS Lambda to assume a role. Permissions Policy # Purpose: Defines the permissions (actions on resources) that are granted to the role. Attachment: Each role can have multiple permissions policies attached to it. Types: Managed Policies: Can be AWS managed or customer managed and can be reused across multiple roles. Inline Policies: Defined directly within the role and cannot be reused. "},{"id":31,"href":"/docs/cloud-computing-and-infrastructure/aws/kms/key-rotation/","title":"Key Rotation","section":"Kms","content":" Key Rotation in KMS # KMS Key Use in RDS/Secrets Manager: The KMS key encrypts the RDS credentials before they are stored in Secrets Manager.\nAccess Control: Access to the decrypted secret is controlled by AWS Identity and Access Management (IAM) policies. You need appropriate permissions both to access the secret in Secrets Manager and to use the associated KMS key for decryption.\nTransparency for the User: The decryption process is designed to be transparent to the user. When you request a secret, you don’t need to manually decrypt it—the service handles everything behind the scenes. This design simplifies the retrieval process while ensuring that the secret remains encrypted at rest.\nKey Rotation: When a KMS key is rotated, it creates a new key version. Existing data remains encrypted with the old key version, and new encryption operations use the new key version. The rotation does not retroactively re-encrypt existing data, but both old and new key versions can be used for decryption as needed.\n"},{"id":32,"href":"/docs/cloud-computing-and-infrastructure/aws/lambda/basic/","title":"Basic","section":"Lambda","content":" What is a handler? # In AWS Lambda, a handler is a function within your code that AWS Lambda calls to start execution of your function. When you create a Lambda function, you specify a handler, which AWS Lambda uses to know which function to invoke when the Lambda function is triggered. The handler is the entry point for your Lambda function and is responsible for processing the input event and providing an output.\nA handler is defined as file_name.function_name.\n"},{"id":33,"href":"/docs/cloud-computing-and-infrastructure/aws/lambda/custom_domain/","title":"Custom Domain","section":"Lambda","content":" How to deploy a custom domain for the auto-generated Lambda invoke URL? # 06 APR 2022 — AWS announced an exciting news of Built-in HTTPS Endpoints for Lambda functions. You can read the original post .\nBefore this announcement, to expose an HTTP endpoint with a custom domain for your lambda backend you have to use the following AWS resources:\nLambda (application backend) API Gateway (to expose HTTP endpoint) Cloudfront distribution Route 53 Fortunately, with AWS Lambda URLs you no longer need the API Gateway !\nThis is how it looks the new architecture :\nSo in this way you are able to deploy a serverless application with a custom domain using only Lambda + Cloudfront + Route 53, thanks to Lambda Function URLs.\nReference: https://medium.com/@walid.karray/configuring-a-custom-domain-for-aws-lambda-function-url-with-serverless-framework-c0d78abdc253\nCan I just simply create a CNAME Route53 record and redirect the incoming requests to my custom domain to the auto-generated Lambda invoke URL? # No.\nIf you must use an AWS Lambda function URL, fronting it with a CloudFront distribution with your desired custom domain name is the only way currently.\nThere is no support currently for a Route 53 alias record, as the Host header must be set to the Lambda function URL domain. If the Host header does not exist, HTTP 400 Bad Request is returned \u0026amp; if it does exist but is not the correct value, HTTP 403 Forbidden is returned.\nThey are meant to be the simplest \u0026amp; fastest way to invoke your Lambda functions via a public endpoint without using other AWS services like API gateway, so a native lack of support for a custom domain name makes sense.\nReference: https://stackoverflow.com/questions/71815143/how-can-i-call-my-aws-lambda-function-url-via-a-custom-domain\n"},{"id":34,"href":"/docs/cloud-computing-and-infrastructure/aws/lambda/layer/","title":"Layer","section":"Lambda","content":" To upload custom runtime environment, libraries and configurations for Lambda function # Reference: # https://docs.aws.amazon.com/lambda/latest/dg/packaging-layers.html#packaging-layers-paths\n"},{"id":35,"href":"/docs/cloud-computing-and-infrastructure/aws/load-balancer/application-load-balancer/","title":"Application Load Balancer","section":"Load Balancer","content":" How Application Load Balancers work # Clients make requests to your application. The listeners in your load balancer receive requests matching the protocol and port that you configure. The receiving listener evaluates the incoming request against the rules you specify, and if applicable, routes the request to the appropriate target group. You can use an HTTPS listener to offload the work of TLS encryption and decryption to your load balancer. Healthy targets in one or more target groups receive traffic based on the load balancing algorithm, and the routing rules you specify in the listener. Scheme # Scheme can\u0026rsquo;t be changed after the load balancer is created.\nInternet-facing\nAn internet-facing load balancer routes requests from clients over the internet to targets. Requires a public subnet.\nOnly VPCs with an internet gateway are enabled for selection.\nInternal\nAn internal load balancer routes requests from clients to targets using private IP addresses.\n"},{"id":36,"href":"/docs/cloud-computing-and-infrastructure/aws/load-balancer/port/","title":"Port","section":"Load Balancer","content":" QUESTION # There are four ports can be configured in load-balancer related settings.\nThe port of the listener The port of the target group The port of the target group health checking The port of the instance in the target group So what is the meaning of the port of the target group?\nANSWER # The port of the listener is used to receive all the incoming requests and data sent to the load-balancer through that port.\nThe port of the target group health checking is used to send health checking request to the instances in that target group.\nThe port of the instance is used to receive all the incoming requests sent to the instance from the target group(transmitted by the listener rule)\nSo, it seems like the port of the target group is meaningless. Yes and no, the target group is required during the target group initilization process and cannot be omitted. But it only serves as the default value of all the instances attached to that target group, which means you can omit the port of the instance when it is added to the target group. Once you set the port of the instance explicitly, the port of the target group for that instance is overwritten and ignored, but still useful for all other instances without explict ports.\nReference # https://stackoverflow.com/questions/42715647/whats-the-target-group-port-for-when-using-application-load-balancer-ec2-con\n"},{"id":37,"href":"/docs/cloud-computing-and-infrastructure/aws/load-balancer/sticky-session/","title":"Sticky Session","section":"Load Balancer","content":" What is sticky session? # By default, an Application Load Balancer routes each request independently to a registered target based on the chosen load-balancing algorithm. However, you can use the sticky session feature (also known as session affinity) to enable the load balancer to bind a user\u0026rsquo;s session to a specific target.\nThis ensures that all requests from the user during the session are sent to the same target. This feature is useful for servers that maintain state information in order to provide a continuous experience to clients. To use sticky sessions, the client must support cookies.\nSticky sessions are enabled at the target group level.\nduration-based cookies # application-based cookies # Reference # https://docs.aws.amazon.com/elasticloadbalancing/latest/application/sticky-sessions.html\n"},{"id":38,"href":"/docs/cloud-computing-and-infrastructure/aws/route53/record-types/","title":"Record Types","section":"Route53","content":" QUESTION # Use A records or CNAME records for my CloudFront distribution?\nANSWER # You can create both A records and CNAME records for your CloudFront distribution, but there are specific conditions and contexts in which you would choose one over the other.\nCNAME Records: A CNAME record is used to map an alias name to a true or canonical domain name. When you use a CNAME record, the DNS query is redirected to the canonical name. CNAME records are typically used for subdomains like www.example.com or store.example.com. You would create a CNAME record if you want your custom subdomain to point to your CloudFront distribution\u0026rsquo;s domain name (e.g., d1234.cloudfront.net).\nA Records: An A record maps a domain name to its corresponding IPv4 IP address. Traditionally, A records are used for root domains (e.g., example.com without the \u0026lsquo;www\u0026rsquo;) because you cannot have a CNAME record at the zone apex (the root level of the domain). However, services like AWS Route 53 have a feature called \u0026ldquo;Alias\u0026rdquo; records, which are a type of A record that can point to AWS resources such as CloudFront distributions or S3 buckets. Alias records provide the functionality of a CNAME record for the zone apex.\nThere are some key reasons to use an Alias record in AWS, especially for CloudFront:\nZone Apex: As mentioned, you can\u0026rsquo;t use a CNAME for your root domain. If you need to point your root domain (e.g., example.com) to your CloudFront distribution, you must use an A record with Alias functionality.\nDNS Query Charges: AWS does not charge for DNS queries to Alias records that point to AWS resources, while there might be charges for CNAME record queries.\nPerformance: Alias records can provide better performance because they resolve to the most geographically appropriate IP addresses for your AWS resources, like your CloudFront distribution.\nManagement: Alias records are AWS-native, meaning they can be easily managed within the AWS ecosystem and are aware of changes in your AWS resources.\nQUESTION # Why is it possible to use an \u0026lsquo;A\u0026rsquo; record, which is traditionally used to point a domain to an IPv4 address, to redirect a domain to a CloudFront URL?\nANSWER # The ability to use an \u0026lsquo;A\u0026rsquo; record to point a domain to a CloudFront URL is made possible by a special type of DNS record offered by AWS Route 53 called an Alias record. An Alias record is a variant of the traditional \u0026lsquo;A\u0026rsquo; record that allows you to point your domain to AWS resources, such as an Elastic Load Balancer, an S3 bucket, or a CloudFront distribution, which do not have a static IP address.\nHere’s why this works:\nAlias Records: When you create an \u0026lsquo;A\u0026rsquo; record as an Alias in AWS Route 53, it functions like a \u0026lsquo;CNAME\u0026rsquo; record in that it can point to a DNS name (like a CloudFront distribution’s domain name), not just an IP address. However, it still retains some properties of an \u0026lsquo;A\u0026rsquo; record, such as working at the zone apex (the root domain).\nDynamic Resolution: Alias records are resolved dynamically by Route 53. When a user queries your domain, Route 53 automatically recognizes the CloudFront distribution and routes the query to the appropriate CloudFront edge location. Thus, even though CloudFront uses a network of edge locations and does not have a single IP address, the Alias \u0026lsquo;A\u0026rsquo; record can point to the service seamlessly.\nAWS Integration: Alias records are a feature specific to AWS Route 53 and are designed to integrate smoothly with other AWS services. This integration allows for the dynamic resolution capability and is not something that traditional DNS services can provide with standard \u0026lsquo;A\u0026rsquo; records.\nThis feature is particularly useful because it allows domain owners to use their root domain (e.g., example.com) with CloudFront, which would not be possible with a standard \u0026lsquo;CNAME\u0026rsquo; record, as \u0026lsquo;CNAME\u0026rsquo; records are not valid for root domains according to DNS standards.\n"},{"id":39,"href":"/docs/cloud-computing-and-infrastructure/aws/route53/routing-policy-types/","title":"Routing Policy Types","section":"Route53","content":" QUESTION-1: What is the difference between different routing policy types in AWS route53? # Amazon Route 53 offers several types of routing policies that allow you to control how DNS queries are answered. Each routing policy serves different use cases and scenarios. Here are the different routing policies available in Route 53:\n1. Simple Routing Policy # Use Case: Basic routing where you route traffic to a single resource. Behavior: Returns one or more values without doing any special routing. 2. Weighted Routing Policy # Use Case: Distributing traffic across multiple resources in specified proportions. Behavior: Routes traffic based on weights assigned to each resource. Useful for load balancing, testing, and gradual migration. 3. Latency Routing Policy # Use Case: Routing traffic to the resource with the lowest network latency for the user. Behavior: Routes traffic based on the lowest latency between the user and the resources in your application. 4. Failover Routing Policy # Use Case: Configuring active-passive failover configurations. Behavior: Routes traffic to a primary resource when it’s healthy, and to a secondary resource when the primary is unhealthy. 5. Geolocation Routing Policy # Use Case: Routing traffic based on the geographic location of the user. Behavior: Routes traffic to resources based on the user’s location. Useful for localizing content or complying with legal requirements. 6. Geoproximity Routing Policy (Traffic Flow Only) # Use Case: Routing traffic based on geographic location, with the ability to shift traffic from one region to another. Behavior: Routes traffic based on the geographic location of your resources and optionally shifts traffic to other resources. 7. Multi-Value Answer Routing Policy # Use Case: Distributing DNS responses across multiple IP addresses to improve availability and load balancing. Behavior: Returns multiple values (such as IP addresses) and can be configured to check the health of each resource. 8. IP-Based Routing Policy (Traffic Flow Only) # Use Case: Routing traffic based on the IP address of the DNS query. Behavior: Routes traffic based on the source IP address range. Detailed Examples and Use Cases: # Simple Routing Policy:\nExample: A single EC2 instance serving all web traffic for a small website. Configuration: A single A record pointing to the instance’s IP address. Weighted Routing Policy:\nExample: A/B testing a new version of an application. Configuration: Two A records, one with a weight of 70 (current version) and another with a weight of 30 (new version). Latency Routing Policy:\nExample: A global application with instances in multiple regions. Configuration: Multiple A records pointing to instances in different regions. Route 53 selects the instance with the lowest latency for each user. Failover Routing Policy:\nExample: An application with a primary and a backup server. Configuration: Primary A record for the main server and a secondary A record for the backup server. Health checks determine which server is healthy. Geolocation Routing Policy:\nExample: A website that serves different content based on the user’s country. Configuration: Multiple A records with different IPs for different regions (e.g., US users get directed to a US server, EU users to an EU server). Geoproximity Routing Policy:\nExample: A multinational company with data centers around the world. Configuration: Adjusts traffic flow to send more or less traffic to specific regions based on proximity and optional bias settings. Multi-Value Answer Routing Policy:\nExample: A highly available website hosted on multiple servers. Configuration: Multiple A records for different servers, with health checks to remove unhealthy servers from DNS responses. IP-Based Routing Policy:\nExample: An enterprise application that routes traffic based on user IP ranges to specific resources. Configuration: Routes traffic based on specific IP address ranges using IP-based rules. Summary: # Amazon Route 53 provides a variety of routing policies to cater to different needs, ranging from simple single-resource routing to complex, geographically-based traffic management. By selecting the appropriate routing policy, you can optimize the performance, availability, and reliability of your applications.\nQUESTION-2: Can a single domain have multiple DNS records? # Yes, a single domain like www.example.com can indeed have multiple DNS records that direct traffic to different places based on various criteria, such as weights, geographical location, or latency. This is a common practice to enhance the performance, reliability, and scalability of web applications. Let’s break this down to clarify how it works:\nMultiple DNS Records for a Single Domain # When you configure DNS for a domain in Amazon Route 53 (or any DNS service), you can create multiple DNS records for the same domain name. These records can point to different IP addresses, load balancers, or other resources. The routing policy you choose determines how Route 53 responds to DNS queries.\nRouting Policies and Their Effects # Simple Routing Policy:\nOne DNS record per domain or subdomain. All queries return the same record (e.g., a single IP address). Weighted Routing Policy:\nMultiple DNS records with assigned weights. Traffic is distributed based on the specified weights. Example: www.example.com can have two A records with weights 70 (70% of traffic) and 30 (30% of traffic). Latency Routing Policy:\nMultiple DNS records pointing to resources in different regions. Queries are routed to the resource with the lowest latency for the user. Example: www.example.com can have A records for servers in the US, Europe, and Asia, and users will be directed to the closest server. Failover Routing Policy:\nPrimary and secondary DNS records. If the primary resource fails (determined by health checks), traffic is routed to the secondary resource. Example: www.example.com has a primary A record pointing to the main server and a secondary A record for a backup server. Geolocation Routing Policy:\nDNS records are configured to serve users based on their geographic location. Example: www.example.com can have A records directing US users to a US server and EU users to an EU server. Geoproximity Routing Policy (Traffic Flow Only):\nRoutes traffic based on the geographic location of your resources and optionally shifts traffic. Example: You can bias traffic towards specific regions or shift traffic between resources. Multi-Value Answer Routing Policy:\nReturns multiple values (e.g., multiple IP addresses) in response to DNS queries. Useful for load balancing and improving availability. Example: www.example.com can return several IP addresses, and the client will choose one to connect to. IP-Based Routing Policy (Traffic Flow Only):\nRoutes traffic based on the source IP address of the DNS query. Example: Specific IP ranges can be routed to specific resources. QUESTION-3: Can I register a single domain name with multiple DNS providers? # No, you cannot register a single domain name with multiple DNS providers at the same time. However, you can delegate different aspects of DNS management and utilize multiple DNS services.\n"},{"id":40,"href":"/docs/cloud-computing-and-infrastructure/aws/route53/subdomain_delegation/","title":"Subdomain Delegation","section":"Route53","content":" How to delegate a subdomain to another AWS account in AWS Route53? # Look at the new hosted zone you created for testing.example.com. This can be in the same AWS account, a different AWS account\u0026hellip; any AWS account. There\u0026rsquo;s nothing here that is \u0026ldquo;account\u0026rdquo; related. This uses standard DNS configuration. The whole of DNS is a hierarchy. The global root can tell you where to find com, and the com servers can tell you where to find example.com, and it\u0026rsquo;s nothing materially different for example.com to tell you where to find testing.example.com instead of giving you a direct answer.\nSo another related thing is about the DNS verification, you need to add the verification DNS record to the subdomain account.\nFor example, I have domain example.com registered in the root account A, and I want to delegate the subdomain api.example.com to api account B.\nI create a public api.example.com hosted zone in the account B I add the 4 DNS resolver records in the public example.com hosted zone in account A I want to issue a TLS certificate for *.api.example.com in account B, so now we cannot add the verification CNAME DNS record in example.com hosted zone in account A, buuuut, I need to add the DNS record in api.example.com hosted zone in account B. That\u0026rsquo;s because you have already delegated all the domains under api.example.com to account B, as well as the verification DNS. Reference: https://serverfault.com/questions/817651/can-different-aws-accounts-manage-different-subdomains\n"},{"id":41,"href":"/docs/cloud-computing-and-infrastructure/aws/s3/bucket/","title":"Bucket","section":"S3","content":" Block public access (bucket settings) # BlockPublicAcls - This prevents any new ACLs to be created or existing ACLs being modified which enable public access to the object. With this alone existing ACLs will not be affected. IgnorePublicAcls - Any ACLs actions that exist with public access will be ignored, this does not prevent them being created but prevents their effects. BlockPublicPolicy - This prevents a bucket policy containing public actions from being created or modified on an S3 bucket, the bucket itself will still allow the existing policy. RestrictPublicBuckets - This will prevent non AWS services or authorized users (such as an IAM user or role) from being able to publicly access objects in the bucket. "},{"id":42,"href":"/docs/cloud-computing-and-infrastructure/aws/s3_glacier/","title":"S3 Glacier","section":"Aws","content":" What is Amazon S3 Glacier? # Amazon S3 Glacier is a cloud storage service provided by Amazon Web Services (AWS) designed for long-term data archiving and backup. It is a part of Amazon Simple Storage Service (Amazon S3). S3 Glacier is known for its extremely low cost and durability, making it an attractive option for storing data that is infrequently accessed.\nKey features of Amazon S3 Glacier include:\nCost-Effectiveness: It offers a very low cost per gigabyte of storage, which is significantly lower than typical high-availability storage. This makes it suitable for data that doesn\u0026rsquo;t need to be retrieved often.\nData Durability: Amazon S3 Glacier is designed for 99.999999999% durability. It achieves this by automatically replicating data across multiple facilities and conducting regular, systematic data integrity checks.\nScalability: It allows users to store large or small amounts of data with ease. There are no limits to how much data you can store.\nSecurity: Amazon S3 Glacier provides comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. It includes data encryption at rest.\nFlexible Retrieval Times: S3 Glacier offers various options for data retrieval times ranging from a few minutes to several hours.\nIntegration with AWS Services: It is seamlessly integrated with other AWS services, allowing for various data lifecycle policies and management strategies.\nS3 Glacier is commonly used for archiving data such as medical records, media archives, financial records, and any other data that must be retained over the long term but is not needed for frequent access. The service is especially beneficial for organizations that need to comply with regulatory requirements for data retention.\nIf the money spent on storage is not a big concern, why should I use S3 Glacier instead of S3? # If the cost is not a concern for you, there are still several scenarios where Amazon S3 Glacier might be more suitable than Amazon S3. Here are some considerations:\nLong-Term Data Archiving: S3 Glacier is specifically designed for long-term data preservation. If you have data that you need to keep for years or even decades, but you don\u0026rsquo;t need frequent access to it, S3 Glacier is an optimal choice. This is because it\u0026rsquo;s tailored for data that is accessed infrequently but needs to be retained for long periods, like legal archives, historical data, or scientific research data.\nRegulatory Compliance: Certain industries have regulatory requirements for data retention that can span several years. S3 Glacier provides features that help in meeting these compliance needs, especially in terms of data durability and immutability.\nData Durability: While both S3 and S3 Glacier offer high durability, Glacier’s design is more focused on ensuring data is preserved intact over long periods, reducing the risk of data loss.\nDigital Preservation: For digital preservation purposes, where the data may not be accessed frequently but needs to be retained without alteration, S3 Glacier offers immutability options such as Glacier Vault Lock. This allows you to enforce compliance controls on the data stored in Glacier.\nBackup and Disaster Recovery: In a comprehensive backup strategy or disaster recovery plan, S3 Glacier can be used for the deepest layer of backup, where the data is not expected to be accessed unless there is a significant need, such as a critical system failure.\nLower Cost of Ownership Over Time: Even if immediate costs are not a concern, the long-term cost savings of using S3 Glacier can be substantial, especially for very large datasets that do not require frequent access.\nData Egress Management: Since retrieving data from Glacier can take longer and might have additional costs compared to S3, it naturally discourages frequent data downloads, which can be a way to manage and control data egress.\nIn summary, if your use case involves long-term data retention, regulatory compliance, or infrequent access patterns, and you\u0026rsquo;re looking for a solution that emphasizes data preservation and durability, S3 Glacier could be a more appropriate choice than standard S3, regardless of the cost differences.\nHow data is organized in S3 Glacier? # The organization of data in Amazon S3 Glacier is somewhat similar to Amazon S3, but there are key differences due to the nature and use-cases of the services. Here\u0026rsquo;s an overview of how data is organized in S3 Glacier:\nVaults: In S3 Glacier, data is stored in \u0026ldquo;vaults,\u0026rdquo; which are analogous to buckets in Amazon S3. Vaults are the primary container for storing data in Glacier. Each vault is a unique namespace within your AWS account and region.\nArchives: Within these vaults, data is stored as \u0026ldquo;archives.\u0026rdquo; An archive can be any data such as a photo, video, document, or even an entire directory or database. Each archive is essentially a discrete, immutable blob of data. Unlike S3, where an object can be easily overwritten or modified, in Glacier, once you upload an archive, it cannot be altered; it must be deleted and re-uploaded for any changes.\nArchive IDs: Each archive is identified by a unique archive ID assigned by Glacier upon creation. Unlike S3, you don\u0026rsquo;t get to assign a human-readable key to each file. This archive ID is essential for retrieving or deleting the archive later on.\nNo Direct, Real-Time Access: Unlike S3, where you can immediately list and access your objects, in Glacier, you don\u0026rsquo;t have direct, real-time access to the archives. To retrieve data, you must first initiate a retrieval request, which can take several hours to process.\nData Retrieval Policies: Glacier supports several retrieval options, each with different costs and access times. You can choose from expedited, standard, or bulk retrievals depending on how quickly you need access to the data and how much you\u0026rsquo;re willing to pay.\nMetadata: When you upload data to Glacier, you can include descriptive metadata with each archive. However, this metadata is limited compared to the extensive metadata capabilities in S3.\nIntegration with S3 Lifecycle Policies: Although Glacier is a separate service, it\u0026rsquo;s tightly integrated with S3 through lifecycle policies. You can automatically transition objects from S3 to Glacier or Glacier Deep Archive for cost-effective, long-term storage.\nSecurity and Compliance: Like S3, Glacier offers encryption for data at rest and supports various compliance needs. However, it adds features like Vault Lock for imposing stricter compliance controls on data.\nIn summary, while both S3 and Glacier store data as objects (archives in the case of Glacier), Glacier is geared more towards long-term storage with less frequent access, and its design reflects this through its unique identifiers, slower retrieval times, and different access methodologies.\n"},{"id":43,"href":"/docs/cloud-computing-and-infrastructure/aws/secret_manager/","title":"Secret Manager","section":"Aws","content":" How to retrive the plain text of Secret in terraform? # data \u0026#34;aws_secretsmanager_secret_version\u0026#34; \u0026#34;secret\u0026#34; { secret_id = aws_db_instance.lodgr_db.master_user_secret[0].secret_arn } output \u0026#34;secret\u0026#34; { value = nonsensitive(data.aws_secretsmanager_secret_version.secret.secret_string) } ouptut \u0026gt;\u0026gt;\u0026gt; secret = jsonencode( { + password = \u0026#34;XsAsabcdefg\u0026#34; + username = \u0026#34;user\u0026#34; } ) secret-value = jsondecode(data.aws_secretsmanager_secret_version.secret.secret_string)[\u0026#34;password\u0026#34;] "},{"id":44,"href":"/docs/cloud-computing-and-infrastructure/aws/ssm/ssm-agent/","title":"Ssm Agent","section":"Ssm","content":" QUESTION # What user does AWS SSM RUN COMMAND use?\nANSWER # The SSM Agent typically runs as the root user. This means that commands executed through SSM Run Command have root-level privileges, allowing them to perform a wide range of system-level operations.\n"},{"id":45,"href":"/docs/cloud-computing-and-infrastructure/aws/storage-gateway/","title":"Storage Gateway","section":"Aws","content":" How Does AWS Storage Gateway Work? # AWS Storage Gateway is a service provided by Amazon Web Services (AWS) that connects an on-premises software appliance with cloud-based storage. It provides a seamless and secure connection between a company\u0026rsquo;s on-premises environment and AWS\u0026rsquo;s storage infrastructure. The service is designed to simplify storage management and reduce costs by leveraging AWS\u0026rsquo;s scalable, reliable, and secure cloud storage.\nHere\u0026rsquo;s an overview of how AWS Storage Gateway works:\n1. Gateway Types # AWS Storage Gateway offers three types of gateways, each serving different use cases:\nFile Gateway: For storing and retrieving files as objects in Amazon S3, using the NFS or SMB protocol. Volume Gateway: Provides cloud-backed storage volumes. It can be configured in two modes: Stored Volumes: Store primary data locally, while asynchronously backing up that data to AWS. Cached Volumes: Store primary data in AWS, retaining frequently accessed data locally. Tape Gateway: Simulates physical tape infrastructure for archiving and backup, integrating with existing tape-based backup software. 2. Deployment and Configuration # Install the Appliance: You install the Storage Gateway appliance on your local infrastructure. This can be a virtual machine or a physical gateway appliance. Connect to AWS: The appliance connects to AWS over the Internet or AWS Direct Connect, providing a secure link between your local environment and AWS storage. 3. Data Transfer and Management # Upload Data: Data written to the Storage Gateway is transferred securely to AWS storage services, like S3, EBS, or Glacier. Management: You can manage the gateway and its storage via the AWS Management Console. This includes tasks like creating volumes, snapshots, and configuring backups. 4. Security and Compliance # Encryption: Data is encrypted in transit and at rest, ensuring that your data is secure. Compliance: AWS Storage Gateway adheres to various compliance standards, making it suitable for regulated industries. 5. Hybrid Cloud Use Cases # AWS Storage Gateway is typically used in hybrid cloud environments. Common use cases include:\nDisaster Recovery: Providing a way to back up on-premises data to the cloud. Data Archiving: Leveraging AWS for long-term, cost-effective storage. Storage Expansion: Extending local storage capacity with the cloud, without needing to invest in local infrastructure. 6. Cost-Effective and Scalable # Pay-As-You-Go: You only pay for the storage and data transfer that you use. Scalable: Easily scale your storage needs up or down based on demand. In summary, AWS Storage Gateway bridges on-premises environments with AWS\u0026rsquo;s cloud storage, offering a variety of storage solutions while ensuring data is securely transferred and managed. It\u0026rsquo;s particularly beneficial for hybrid cloud strategies, data archiving, backup, and disaster recovery scenarios.\nWhat is File Shares in AWS Storage Gateway? # If you are using the File Gateway type of AWS Storage Gateway, creating file shares is a key step. File shares allow your on-premises applications to store and retrieve files in Amazon S3 through standard file storage protocols. Here\u0026rsquo;s how you would generally proceed after setting up a File Gateway:\n1. Setting Up File Gateway # First, ensure that you have successfully deployed a File Gateway on your on-premises environment. This involves downloading and installing the gateway software on a supported host, such as a virtual machine. Configure the gateway to connect to your AWS account and the appropriate AWS region. 2. Creating File Shares # Access AWS Management Console: Go to the AWS Management Console and navigate to the Storage Gateway section. Create a New File Share: Select your File Gateway and then create a new file share. Configure the File Share: Select Storage Type: Choose the Amazon S3 bucket where the files will be stored. Set Access Protocols: Choose the protocol for the file share, either NFS (Network File System) or SMB (Server Message Block). Configure Security and Access: Set up permissions and access control for the file share. You may also need to configure your network settings, such as the gateway\u0026rsquo;s IP address. 3. Using the File Shares # After the file share is created, it will be accessible from your on-premises network. You can mount the file share on your on-premises servers or user devices, just as you would with any other network share. When files are added to, updated, or deleted from this file share, these changes are synchronized with the corresponding S3 bucket in AWS. 4. Data Management and Optimization # Lifecycle Management: You might want to set up lifecycle policies on your S3 bucket to manage data efficiently. Performance Optimization: Consider enabling features like caching to optimize the performance of frequently accessed files. 5. Monitoring and Maintenance # Monitor Usage: Regularly check the performance and usage metrics of your file shares. Update and Maintain: Ensure your File Gateway software is kept up-to-date and maintained for security and performance. Important Considerations # Network Configuration: Ensure your network is configured to allow smooth communication between your on-premises environment and the AWS File Gateway. Security: Implement necessary security measures such as encryption, IAM roles, and network firewalls to secure your data. Backup and Recovery: Implement strategies for backing up your data and recovering it in case of a disaster. Creating and managing file shares through the File Gateway allows you to seamlessly integrate your on-premises file storage with the scalable, durable, and cost-effective storage available in AWS S3.\nCore of AWS Storage Gateway on EC2 # When you deploy AWS Storage Gateway on an Amazon EC2 instance, what you\u0026rsquo;re essentially running is a virtualized version of the Storage Gateway appliance. This virtual appliance is indeed managed primarily through a command-line interface (CLI), especially for initial setup and configuration. Here are some key aspects to understand:\nThe Core of AWS Storage Gateway on EC2 # Virtual Appliance: The AWS Storage Gateway deployed on an EC2 instance is a virtual appliance, which means it\u0026rsquo;s a pre-configured virtual machine image specifically designed for the Storage Gateway service.\nCommand-Line Interface (CLI):\nInitial interactions with the Storage Gateway appliance are often via a CLI. This interface is used for basic setup procedures like configuring network settings, connecting the appliance to your AWS account, and setting up initial parameters. The CLI is not where you\u0026rsquo;ll manage all aspects of the Storage Gateway; it\u0026rsquo;s primarily for setup and some basic configuration tasks. AWS Management Console:\nMost of the management and operational tasks for the Storage Gateway (like creating file shares, monitoring performance, and managing backups) are done through the AWS Management Console. Once you\u0026rsquo;ve completed the initial setup through the CLI, you\u0026rsquo;ll typically switch to the console for day-to-day operations and management. Limited Interaction with the Underlying OS:\nWhen you deploy Storage Gateway on an EC2 instance, you\u0026rsquo;re not meant to interact extensively with the underlying operating system (OS) as you would with a typical EC2 instance. The Storage Gateway software appliance abstracts most of the OS-level details. Direct modifications to the underlying Linux OS or attempts to use it for purposes other than what AWS Storage Gateway is designed for are not recommended and can lead to issues with the service. Updates and Maintenance:\nAWS manages updates and maintenance of the Storage Gateway software. These updates are usually applied directly through the AWS Management Console or automatically, depending on the configuration. Role of the Storage Gateway:\nThe primary role of this appliance is to facilitate secure data transfer between your on-premises environment (or EC2 in this case) and AWS storage services like Amazon S3, Glacier, or EBS. Its functionalities are focused on storage integration, data encryption, caching, and optimizing data transfer. Conclusion # So, yes, the CLI aspect you see after logging into the EC2 instance running the Storage Gateway is part of its core, but it\u0026rsquo;s primarily for setup and initial configuration. The main functionalities and day-to-day management of the Storage Gateway are accessed through the AWS Management Console. This setup ensures a secure, efficient, and user-friendly way to integrate your on-premises or cloud-based resources with AWS’s storage solutions.\nRole of Storage Gateway on EC2 with File Shares # Yes, that\u0026rsquo;s a good summary. When you create a file share for an Amazon S3 bucket using AWS Storage Gateway (specifically, the File Gateway type) and mount it on your local machine, the Storage Gateway serves as an intermediary that facilitates data transfer between your local environment and AWS S3. Here\u0026rsquo;s a breakdown of its role:\n1. Data Transfer and Translation # The Storage Gateway on EC2 acts as a bridge or translator between your local file system protocols (like NFS or SMB) and the object storage protocol used by Amazon S3. When you read or write files to the mounted file share on your local machine, these operations are translated by the File Gateway into corresponding S3 API calls. 2. Caching and Performance Optimization # The Gateway caches frequently accessed data. This improves performance by reducing the need to fetch data from S3 for every read operation. It can also buffer write operations locally before sending them to S3, optimizing data transfer and minimizing latency. 3. Security and Encryption # The Storage Gateway handles the encryption of data in transit between your local environment and AWS S3. It can also integrate with AWS IAM for access control and adhere to security policies set for your S3 resources. 4. Data Consistency and Integrity # Ensures data consistency between your on-premises environment and the AWS cloud. Manages data integrity checks and handles any communication retries or errors during the data transfer process. 5. Scalability and Manageability # The Storage Gateway on EC2 can scale to handle varying loads, accommodating changes in demand. It simplifies the management of cloud storage resources, offering a familiar file-based interface for your cloud storage. 6. Hybrid Cloud Integration # The Gateway provides a seamless integration point for hybrid cloud setups, allowing on-premises systems to interact efficiently with AWS cloud storage. Conclusion # In essence, the AWS Storage Gateway on an EC2 instance acts as a mediator and facilitator in the data exchange between your local systems and AWS S3. It translates file operations into S3 object actions, manages performance and security aspects, and provides a reliable, scalable, and efficient way to extend your on-premises storage capabilities into the cloud. This setup allows businesses to leverage cloud storage benefits while maintaining a familiar local storage experience.\n"},{"id":46,"href":"/docs/cloud-computing-and-infrastructure/aws/vpc/","title":"Vpc","section":"Aws","content":" VPC Question : can you have overlapping VPC CIDRs in AWS ? # You could, and it’s only an issue if you want to route traffic between them.\nThis is answer, \u0026ldquo;you can, but you shouldn\u0026rsquo;t\u0026rdquo;\nYou\u0026rsquo;ll probably one day want to connect two of these VPCs and it will be your biggest nightmare. Just assign different ranges to each VPC\u0026hellip;\nThe Internet Assigned Numbers Authority (IANA) has reserved the following three blocks of the IP address space for private internets:\n10.0.0.0 - 10.255.255.255 (10/8 prefix) 172.16.0.0 - 172.31.255.255 (172.16/12 prefix) 192.168.0.0 - 192.168.255.255 (192.168/16 prefix) "},{"id":47,"href":"/docs/cloud-computing-and-infrastructure/aws/vpc/eni/","title":"Eni","section":"Vpc","content":" Elastic Network Interface # To understand how AWS achieves the AWSVPC network mode for containers, which allows them to have their own network settings regardless of the instance\u0026rsquo;s network configuration, we need to look into the underlying technology and AWS\u0026rsquo;s infrastructure capabilities:\nElastic Network Interface (ENI): In AWSVPC mode, each ECS task is assigned its own Elastic Network Interface (ENI). An ENI is a virtual network interface that you can attach to an instance in a VPC. It provides the network connectivity for the ECS task. Each ENI is associated with a private IP address from the subnet in which it\u0026rsquo;s created, and it can have its own security group. This setup allows the task to have its own network identity within the VPC. Integration with Amazon VPC: Amazon VPC (Virtual Private Cloud) allows you to launch AWS resources in a virtual network that you\u0026rsquo;ve defined. This virtual network closely resembles a traditional network that you\u0026rsquo;d operate in your own data center, with the benefits of using scalable infrastructure. When you use AWSVPC mode, ECS tasks are first-class citizens within the VPC. They can directly interact with other AWS services and resources within the same VPC, such as Amazon RDS databases or Amazon S3 buckets, using private IP addresses. Task-Level Networking: In this mode, the network configuration for a task is defined at the task level, not at the instance level. This means that each task can have its own unique network configuration, separate from the underlying host (EC2 instance). This configuration includes the task\u0026rsquo;s own private IP address, security groups, and other network settings specific to the task. Isolation and Security: The AWSVPC mode provides a high level of isolation and security. Since each task has its own ENI, it\u0026rsquo;s isolated at the network level from other tasks, even those running on the same host instance. This isolation also means that network-related issues in one task are less likely to affect other tasks. AWS Infrastructure and Hypervisor: AWS\u0026rsquo;s infrastructure, including its hypervisor and network architecture, plays a crucial role. The hypervisor on the EC2 instances allows the virtualization and management of these ENIs, ensuring that they can be dynamically assigned and managed for ECS tasks. The network architecture of AWS is designed to support such flexibility and scalability, allowing for the dynamic provisioning of network resources like ENIs. In summary, AWS leverages its sophisticated virtual networking infrastructure, primarily through ENIs and VPC, to provide each ECS task with its own network settings. This is done at the task level, independent of the network configuration of the underlying EC2 instances, thus offering enhanced isolation, security, and flexibility for containerized applications running in ECS.\n"},{"id":48,"href":"/docs/cloud-computing-and-infrastructure/aws/vpc/gateway/","title":"Gateway","section":"Vpc","content":" NAT Gateway # Internet Gateway # An Internet Gateway is used to allow resources in your network to access the internet, and to allow the internet to connect to those resources. It serves as a route for internet traffic from and to a network. In a cloud environment like AWS (Amazon Web Services), an Internet Gateway is attached to a VPC (Virtual Private Cloud) to enable communication between instances in the VPC and the internet. It does not perform any network address translation. Subnets # Network Interface # "},{"id":49,"href":"/docs/cloud-computing-and-infrastructure/aws/vpc/route-table/","title":"Route Table","section":"Vpc","content":" Subnet -\u0026gt; Route Table # in AWS, each subnet can only be associated with one route table at a time. Here\u0026rsquo;s how it works:\nSingle Route Table per Subnet: If you need different routing rules for different parts of your network, you need to create multiple subnets and associate each with a different route table. Multiple Subnets per Route Table: Conversely, a single route table can be associated with multiple subnets. This is useful when you want multiple subnets to share the same routing configuration. "},{"id":50,"href":"/docs/cloud-computing-and-infrastructure/aws/vpc/vpc-endpoint/","title":"Vpc Endpoint","section":"Vpc","content":" VPC Endpoints # There are three types of VPC endpoints – Interface endpoints, Gateway Load Balancer endpoints, and Gateway endpoints. Interface endpoints and Gateway Load Balancer endpoints are powered by AWS PrivateLink, and use an Elastic Network Interface (ENI) as an entry point for traffic destined to the service. Interface endpoints are typically accessed using the public or private DNS name associated with the service, while Gateway endpoints and Gateway Load Balancer endpoints serve as a target for a route in your route table for traffic destined for the service.\n"},{"id":51,"href":"/docs/cloud-computing-and-infrastructure/aws/vpc/vpc-peering/","title":"Vpc Peering","section":"Vpc","content":" Configuration # Have to add routes to both two subnets\u0026rsquo; route table. Route table Destination Target VPC A VPC A CIDR Local VPC B CIDR pcx-11112222 VPC B VPC B CIDR Local VPC A CIDR pcx-11112222 No unidirection access if only one route is added to one subnet\u0026rsquo;s route table. Route table Destination Target VPC A VPC A CIDR Local VPC B CIDR pcx-11112222 VPC B VPC B CIDR Local OR\nRoute table Destination Target VPC A VPC A CIDR Local VPC B VPC B CIDR Local VPC A CIDR pcx-11112222 No traffic allowed from A -\u0026gt; B and B -\u0026gt; A.\nSteps to create VPC peering # Create VPC peering requests(in account A) Accept VPC peering requests(in account B) Update/Create route tables to transmit all requests to the peered CIDR to peered VPC(in account A) Update/Create route tables to transmit all requests to the peered CIDR to peered VPC(in account B) (Optional) Allow DNS resolving(in account A and B) (Optional) Associate Private Hosted Zone following the steps below(in account A and B) Update security groups (Can use OTHER_AWS_ACCOUNT/SG_ID in the inbound/outbound rule to allow traffic from security group in other aws accounts) VPC Peering with Private Hosted Zone # This process has to be done using command line.\n# The AWS account that created the private hosted zone must first submit a CreateVPCAssociationAuthorization request aws route53 create-vpc-association-authorization --hosted-zone-id HOSTED_ZONE_ID --vpc VPCRegion=us-west-2,VPCId=VPC_ID_IN_OTHER_ACCOUNT # Then the account that created the VPC must submit an AssociateVPCWithHostedZone request. aws route53 associate-vpc-with-hosted-zone --hosted-zone-id SAME_HOSTED_ZONE_ID --vpc VPCRegion=us-west-2,VPCId=SAME_VPC_ID "},{"id":52,"href":"/docs/cloud-computing-and-infrastructure/aws/waf/sql-injection/","title":"SQL Injection","section":"Waf","content":" What is the difference between different options in field_to_match in the statement block? # When setting up a rule in AWS WAF to protect against SQL injections, choosing the right field to match is crucial. Among the options provided (all_query_arguments, body, cookies, header_order, headers, ja3_fingerprint, json_body, method, query_string, single_header, single_query_argument, uri_path), the most effective choice can depend on the specifics of your application and where you expect SQL injection attempts to occur. However, there are common fields that are typically targeted by SQL injection attacks:\nall_query_arguments: This is a comprehensive choice as it inspects all parameters passed in the query string of the URL. SQL injections often occur through URL parameters, making this a very effective option for a general SQL injection rule.\nbody: If your application receives SQL commands through POST requests, where data is sent in the body of the request (especially in forms or API payloads), inspecting the body can be crucial.\nsingle_query_argument or single_header: These can be used if you know that SQL injection attacks are only likely through specific query parameters or headers.\nFor the most common and broad protection against SQL injections, focusing on all_query_arguments and body will cover many of the bases:\nall_query_arguments covers all the data sent in the query strings, which is a common attack vector for SQL injections. body covers data sent in the body of POST requests, which can include form submissions and JSON payloads that might also be susceptible to injection. Example Terraform Configuration # Here’s how you can set up SQL injection rules in AWS WAF using Terraform to monitor both the query arguments and the body of requests:\nresource \u0026#34;aws_wafv2_web_acl\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;example-waf\u0026#34; scope = \u0026#34;CLOUDFRONT\u0026#34; # Use \u0026#34;REGIONAL\u0026#34; for resources in AWS regions or \u0026#34;CLOUDFRONT\u0026#34; for CloudFront distributions description = \u0026#34;WAF for protecting from SQL injection\u0026#34; default_action { allow {} } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;exampleWaf\u0026#34; sampled_requests_enabled = true } rule { name = \u0026#34;SQLInjectionRuleQueryArgs\u0026#34; priority = 1 action { block {} } statement { sqli_match_statement { field_to_match { all_query_arguments {} } text_transformation { priority = 0 type = \u0026#34;URL_DECODE\u0026#34; } text_transformation { priority = 1 type = \u0026#34;HTML_ENTITY_DECODE\u0026#34; } } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;SQLInjectionRuleQueryArgs\u0026#34; sampled_requests_enabled = true } } rule { name = \u0026#34;SQLInjectionRuleBody\u0026#34; priority = 2 action { block {} } statement { sqli_match_statement { field_to_match { body {} } text_transformation { priority = 0 type = \u0026#34;URL_DECODE\u0026#34; } text_transformation { priority = 1 type = \u0026#34;HTML_ENTITY_DECODE\u0026#34; } } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;SQLInjectionRuleBody\u0026#34; sampled_requests_enabled = true } } } what is text_transformation? # In the context of AWS WAF (Web Application Firewall), text_transformation is a rule setting used to normalize or sanitize incoming web requests before inspecting them for malicious content, such as SQL injection or cross-site scripting (XSS) attacks. The main purpose of text transformations is to eliminate any obfuscations that attackers might use to hide their payloads, making it easier for AWS WAF to detect and mitigate potential threats.\nPurpose of Text Transformations # Text transformations are crucial because attackers often use various encoding methods to disguise their attacks. For instance, an attacker might encode characters or use HTML entities to bypass simple pattern-matching rules that are designed to detect straightforward malicious inputs. By applying text transformations, AWS WAF can decode these inputs into a format that is easier to inspect.\nTypes of Text Transformations # AWS WAF supports several types of text transformations, each designed to address specific obfuscation techniques:\nURL Decode: Converts encoded characters back to their original form. For example, \u0026ldquo;%20\u0026rdquo; would be transformed to a space character. This is particularly useful for inspecting parts of requests that might be URL-encoded.\nHTML Entity Decode: Converts HTML entities back to their respective characters. For example, \u0026amp;lt; becomes \u0026lt;. This is essential for inspecting web requests that include HTML content.\nLowercase: Converts all characters to lowercase. This helps in creating case-insensitive matches for rule conditions.\nCMD Line: Simplifies command line inputs by trimming spaces, converting sequences of more than one space into a single space, and removing certain quoting styles commonly used in system command lines. This can help in detecting command injection attempts.\nBase64 Decode: Decodes Base64 encoded strings. This is useful if attackers are encoding malicious scripts or payloads in Base64 to evade detection.\nHex Decode: Converts hexadecimal encoded data into its binary representation, which can be necessary for further inspections in binary or script-based attack vectors.\nNone: Applies no transformation. This can be used when the raw input needs to be inspected without any modifications.\nExample in Terraform Configuration # Here’s how a text transformation might be specified in a Terraform configuration for AWS WAF:\nresource \u0026#34;aws_wafv2_web_acl\u0026#34; \u0026#34;example\u0026#34; { // Other configurations rule { name = \u0026#34;SQLInjectionRule\u0026#34; priority = 1 action { block {} } statement { sqli_match_statement { field_to_match { body {} } text_transformation { priority = 0 type = \u0026#34;URL_DECODE\u0026#34; } text_transformation { priority = 1 type = \u0026#34;HTML_ENTITY_DECODE\u0026#34; } } } // Other configurations } } Reference: https://docs.aws.amazon.com/waf/latest/APIReference/API_TextTransformation.html\n"},{"id":53,"href":"/docs/cloud-computing-and-infrastructure/cicd/","title":"Cicd","section":"Cloud Computing and Infrastructure","content":"CICD\n"},{"id":54,"href":"/docs/cloud-computing-and-infrastructure/cicd/circleci/","title":"Circleci","section":"Cicd","content":"CircleCI\n"},{"id":55,"href":"/docs/cloud-computing-and-infrastructure/cicd/circleci/avoid-infinite-loop/","title":"Avoid Infinite Loop","section":"Circleci","content":" Skip CI for a specific push # By default, CircleCI automatically triggers a pipeline whenever you push changes to your project. You can override this behavior by adding one of the following tags within the first 250 characters of the body or title of the latest commit you pushed:\n[ci skip]\n[skip ci]\nAdding one of these tags skips pipeline execution for all the commits included in the push.\n"},{"id":56,"href":"/docs/cloud-computing-and-infrastructure/cicd/circleci/circleci/","title":"Circle Ci","section":"Circleci","content":" # In CircleCI, the save_cache step allows you to cache dependencies or other directories between jobs. The scope of a cache in CircleCI is at the repository level, but its applicability can be fine-tuned using keys that may include branch names, environment variables, or other parameters. Here\u0026rsquo;s how it works:\n"},{"id":57,"href":"/docs/cloud-computing-and-infrastructure/cicd/circleci/workspace/","title":"Workspace","section":"Circleci","content":" CircleCI Workspace # What Exactly Are CircleCI Workspaces? # CircleCI Workspaces allow you to share files and directories between jobs within a workflow. It\u0026rsquo;s crucial to note that Workspaces are not the same as caching or artifacts—they serve a unique role. Workspaces are mutable; you can add data to them as jobs progress through the workflow. Think of it as a temporary storage unit that multiple jobs within a single workflow can access and modify.\nPractical Use Cases for Workspaces # Let’s look at a couple of scenarios where Workspaces can really help:\n1. Sharing Build Artifacts Across Jobs # Suppose you have a microservice architecture, and you need to build multiple Docker images. Let\u0026rsquo;s say you have a build job that compiles your code and packages it. You can store your compiled binaries in a Workspace, allowing subsequent jobs to use those binaries to build Docker images in parallel. This means you avoid re-building artifacts in multiple jobs, which is a huge time saver.\n2. Running Extensive Test Suites # Another common scenario is splitting tests across multiple jobs to reduce total build time. Say you have unit tests, integration tests, and some UI tests. Each of these requires the same codebase but involves different environments or configurations. You could have a prepare-test job that installs dependencies and builds the environment, storing everything in a Workspace. Then, the unit tests, integration tests, and UI tests can all start from the same state, cutting down on redundant setup time.\nSome Gotchas You Should Watch Out For # 1. Size Limitations # One of the biggest misconceptions is that you can just dump everything into a Workspace without considering size. Large files can dramatically slow down the workflow because CircleCI has to store and retrieve them multiple times. Always try to keep the data shared via Workspace as lightweight as possible. For instance, avoid placing full Docker images in a Workspace—use caching for that instead.\n2. Mutable State Challenges # A Workspace is mutable, which means that every job that writes to it changes its state. If multiple jobs depend on the same workspace, unexpected changes can lead to flaky workflows. Imagine job A writes some test results to the Workspace, and then job B modifies them in some way—that’s a recipe for non-deterministic behavior. To counter this, you should always be explicit about what data is being put into or taken out of the Workspace.\nBest Practices # Isolate Data by Workflow Stage: Instead of using one giant Workspace, split your data into separate Workspaces by the nature of the job—for example, build artifacts, dependencies, and test results. This can make your workflow easier to manage and debug. Leverage Filters: Make sure you’re not using Workspaces unnecessarily—often, job dependencies or caching mechanisms are more suitable, especially for read-only data. Use Workspaces for data that is actively modified or needs to be shared across jobs. Minimize Workspace Data: Only persist what\u0026rsquo;s absolutely necessary. If you need specific build artifacts, don’t store the entire build directory—filter the data before persisting it to the Workspace. Why I cannot access files in a workspace when there is no internet in my job? # Workspaces do rely on an internet connection. CircleCI is a cloud-based service, and Workspaces are uploaded to and downloaded from CircleCI\u0026rsquo;s storage during the workflow. If your job has no internet connection, it will not be able to upload data to or retrieve data from the Workspace, which will likely cause your workflow to fail. To ensure smooth usage of Workspaces, all jobs that interact with the Workspace must have a stable internet connection.\nWorkspaces vs. Caching in CircleCI # While both Workspaces and caching help in optimizing your CI/CD pipeline, they serve different purposes and have distinct behaviors:\nWorkspaces are used to share files and directories between jobs within the same workflow. They are mutable, meaning jobs can modify the data as they progress. Workspaces are ideal when different jobs within a workflow need to access and modify the same set of files. Caching, on the other hand, is used to save dependencies or data between different workflow runs. The cache is immutable, meaning once data is cached, it cannot be changed. Caching is particularly useful for saving build dependencies, like packages from npm install or pip install, to speed up subsequent builds. Unlike Workspaces, the cache is meant for storing data that remains constant across different runs to avoid repeated downloads or builds. In practical terms, use Workspaces when you need to pass data between jobs in a single workflow and allow for modification. Use Caching when you want to speed up jobs by reusing previously downloaded or generated data across different workflows or branches.\nHow Long Will Data Be Stored in the Workspace and When Will It Be Removed? # The data stored in the Workspace is temporary and will be removed once the workflow is finished. Workspaces are designed to facilitate data sharing between jobs within a single workflow, and they do not persist beyond the lifecycle of that workflow. However, if you need to re-run a workflow, CircleCI retains the Workspace data for up to 15 days by default. This allows you to reuse the data when re-running a workflow without having to regenerate it. After 15 days, if no re-run occurs, the Workspace data will be deleted automatically. If you need to store data beyond the workflow or the 15-day retention, consider using caching or artifacts instead.\n"},{"id":58,"href":"/docs/cloud-computing-and-infrastructure/cicd/github_actions/","title":"Github Actions","section":"Cicd","content":"Github Action\n"},{"id":59,"href":"/docs/cloud-computing-and-infrastructure/cicd/github_actions/caching/","title":"Caching","section":"Github Actions","content":" How to speed up npm ci command? # npm ci should be preferred in CI because it respects the package-lock.json file. Unlike npm install, which rewrites the file and always installs new versions.\nBy design this command always purges all local packages, by removing the node_modules directory in the beginning. This is the main reason for long builds. And there is no option to avoid this irritating behaviour.\nSo the idea is not to cache the node_modules folder, but to cache the libs of npm.\nhttps://github.com/actions/setup-node#caching-global-packages-data\nhttps://stackoverflow.com/questions/55230628/is-there-a-way-to-speedup-npm-ci-using-cache\n"},{"id":60,"href":"/docs/cloud-computing-and-infrastructure/cicd/github_actions/deployment-status/","title":"Deployment Status","section":"Github Actions","content":" There is only one active deployment for each environment # This can be annoying when I have multiple branches deployed to the same environment and have several open PRs to be merged at the same time. Only the PR that has the active deployment mark can be merged when I set deployment before merge branch protection rule.\nSolution 1: Run each deployment CICD again when you are reviewing the PR, so that branch\u0026rsquo;s deployment will be the latest one and becomd active. Then it is good to be merged.\nSolution 2: Use dynamic environments. When you use an environment name in your GitHub Actions workflow, GitHub will automatically create that environment if it doesn\u0026rsquo;t already exist. However, we have populated the secrets and variables manually. (TODO: Have to find a total automatic method.)\n"},{"id":61,"href":"/docs/cloud-computing-and-infrastructure/config-management/","title":"Config Management","section":"Cloud Computing and Infrastructure","content":"Config Management\n"},{"id":62,"href":"/docs/cloud-computing-and-infrastructure/config-management/ansible/","title":"Ansible","section":"Config Management","content":"TBD\n"},{"id":63,"href":"/docs/cloud-computing-and-infrastructure/config-management/cloud-init/","title":"Cloud Init","section":"Config Management","content":"TBD\n"},{"id":64,"href":"/docs/cloud-computing-and-infrastructure/containerization-and-orchestration/","title":"Containerization and Orchestration","section":"Cloud Computing and Infrastructure","content":"Containerization\n"},{"id":65,"href":"/docs/cloud-computing-and-infrastructure/containerization-and-orchestration/docker/configuration/","title":"Configuration","section":"Docker","content":" Hostname VS Container Name # ECS Link Container # Host Port = 0 # In an AWS ECS (Elastic Container Service) task definition, if you set the host port as 0 in the port mapping of a container definition, it enables dynamic port mapping. This means that ECS will automatically assign an available port from the EC2 instance (or the underlying infrastructure) to the container.\n"},{"id":66,"href":"/docs/cloud-computing-and-infrastructure/containerization-and-orchestration/docker/entrypoint_and_command/","title":"Entrypoint and Command","section":"Docker","content":" What is the difference between CMD and ENTRYPOINT? # Docker has a default entrypoint which is /bin/sh -c but does not have a default command.\nWhen you run docker like this: docker run -i -t ubuntu bash the entrypoint is the default /bin/sh -c, the image is ubuntu and the command is bash.\nThe command is run via the entrypoint. i.e., the actual thing that gets executed is /bin/sh -c bash. This allowed Docker to implement RUN quickly by relying on the shell\u0026rsquo;s parser.\nLater on, people asked to be able to customize this, so ENTRYPOINT and \u0026ndash;entrypoint were introduced.\nEverything after the image name, ubuntu in the example above, is the command and is passed to the entrypoint. When using the CMD instruction, it is exactly as if you were executing docker run -i -t ubuntu The parameter of the entrypoint is .\nYou will also get the same result if you instead type this command docker run -i -t ubuntu: a bash shell will start in the container because in the ubuntu Dockerfile a default CMD is specified: CMD [\u0026ldquo;bash\u0026rdquo;].\nAs everything is passed to the entrypoint, you can have a very nice behavior from your images. @Jiri example is good, it shows how to use an image as a \u0026ldquo;binary\u0026rdquo;. When using [\u0026quot;/bin/cat\u0026quot;] as entrypoint and then doing docker run img /etc/passwd, you get it, /etc/passwd is the command and is passed to the entrypoint so the end result execution is simply /bin/cat /etc/passwd.\nAnother example would be to have any cli as entrypoint. For instance, if you have a redis image, instead of running docker run redisimg redis -H something -u toto get key, you can simply have ENTRYPOINT [\u0026ldquo;redis\u0026rdquo;, \u0026ldquo;-H\u0026rdquo;, \u0026ldquo;something\u0026rdquo;, \u0026ldquo;-u\u0026rdquo;, \u0026ldquo;toto\u0026rdquo;] and then run like this for the same result: docker run redisimg get key.\nreference: https://stackoverflow.com/questions/21553353/what-is-the-difference-between-cmd-and-entrypoint-in-a-dockerfile\nWill docker exec use Entrypoint as the executable file? # No, when you use docker exec to run a command in a running container, it does not use the ENTRYPOINT as the executable to run your command. Instead, docker exec runs the command you specify directly within the container.\nHere\u0026rsquo;s how it works:\ndocker exec Usage: The docker exec command is typically used to run a new command in a running container. For example, if you run docker exec [container_id] mycommand, it executes mycommand inside the running container.\nIndependence from ENTRYPOINT and CMD: This command execution is independent of the ENTRYPOINT and CMD instructions defined in the Dockerfile. These instructions (ENTRYPOINT and CMD) determine what the container runs at startup, not what happens when you use docker exec.\nPractical Example: Suppose you have a container running a web server, started with its own ENTRYPOINT and/or CMD instructions. If you run docker exec [container_id] ls /var/www/html, this will list the contents of /var/www/html in the container. This ls command is executed independently and does not involve the container\u0026rsquo;s ENTRYPOINT or CMD.\nIn summary, docker exec is used for executing new commands in a running container, and it does this independently of the container\u0026rsquo;s ENTRYPOINT or CMD configuration.\nThe ENTRYPOINT and CMD instructions in a Dockerfile define behavior that is specific to the docker run command, which is used to create and start a new container.\nExample Entrypoint file # #!/bin/sh set -e # first arg is `-f` or `--some-option` if [ \u0026#34;${1#-}\u0026#34; != \u0026#34;$1\u0026#34; ]; then set -- php-fpm \u0026#34;$@\u0026#34; fi exec \u0026#34;$@\u0026#34; This block checks if the first argument ($1) to the script starts with a dash (-). This is a common convention for passing options or flags to a command. ${1#-} is a shell parameter expansion that removes a leading hyphen (-) from the first argument $1. If $1 doesn\u0026rsquo;t start with a hyphen, ${1#-} equals $1. If $1 starts with a -, indicating that it\u0026rsquo;s an option/flag rather than a command, the script prepends php-fpm to the arguments list. set -- php-fpm \u0026quot;$@\u0026quot; resets the positional parameters ($@) of the script, effectively transforming the script\u0026rsquo;s arguments from, for example, -f myconfig.conf to php-fpm -f myconfig.conf. This is useful for allowing users to pass options to php-fpm when running the Docker container without needing to explicitly specify the php-fpm command. exec \u0026quot;$@\u0026quot;: # Finally, exec \u0026quot;$@\u0026quot; replaces the current shell with the command specified in the arguments. $@ is a special variable that holds all of the script\u0026rsquo;s arguments. Using exec here ensures that the final command (e.g., php-fpm with any passed options) becomes the main process of the container (PID 1). This is important in Docker containers because the container lives as long as this main process. In summary, this script is an entrypoint script for a Docker container running PHP-FPM. It allows the container to be started with or without additional command-line options for php-fpm. If the first argument is an option (starts with -), it prepends php-fpm to the arguments, and then it executes this command. This pattern is particularly useful for creating flexible Docker images that can be easily configured with different settings at runtime.\n"},{"id":67,"href":"/docs/cloud-computing-and-infrastructure/infra-as-code/","title":"Infra as Code","section":"Cloud Computing and Infrastructure","content":"Infra as Code\n"},{"id":68,"href":"/docs/data-store-and-management/caching/caching-algorithm/","title":"Caching Algorithm","section":"Caching","content":"Caching Algorithm\n"},{"id":69,"href":"/docs/data-store-and-management/caching/caching-algorithm/arc/","title":"Arc","section":"Caching Algorithm","content":" ARC # Adaptive Replacement Cache (ARC) is a fascinating approach to improving cache performance. Unlike more straightforward caching algorithms like LRU (Least Recently Used) or LFU (Least Frequently Used), ARC dynamically adapts to changing access patterns.\nHow ARC Works # ARC aims to balance between recency (like LRU) and frequency (like LFU). To do this, ARC maintains four lists:\nT1: Recently seen but not yet frequently used items. B1: A ghost list of items recently evicted from T1. T2: Frequently accessed items. B2: A ghost list of items recently evicted from T2. The key innovation of ARC is how it uses these lists to adaptively manage the cache. When a new page request comes in, ARC decides whether to place the item in the recent or frequent pool based on its prior access history. If it notices a lot of churn in recently used items, it will increase the space for T1, essentially behaving more like LRU. On the other hand, if certain items are frequently requested, it will allocate more space to T2, behaving more like LFU.\nThis adaptability makes ARC particularly effective in scenarios with changing access patterns, where traditional LRU or LFU might perform poorly.\nPractical Scenario: Database Caching # A typical use case for ARC is in database systems, where query patterns can change depending on the workload. Imagine a database that powers an online shopping site. During a flash sale, customer activity becomes highly erratic. Some products get a surge in popularity, while others experience lower-than-usual traffic.\nTraditional LRU might fail here, evicting items that might soon become hot again. LFU might keep less popular items in the cache for too long. ARC\u0026rsquo;s adaptability helps retain recently hot items while ensuring frequently accessed ones are not prematurely evicted, making it ideal for handling such dynamic workloads.\nVisual Representation # Here\u0026rsquo;s a simple flowchart illustrating how ARC manages cache hits and evictions:\nNew Request: Is the item in T1 or T2? If yes, it\u0026rsquo;s a cache hit. Cache Miss: Is the item in B1 or B2 (ghost lists)? If yes, adjust the cache partitioning to give more priority to either recent or frequent. If no, evict from the appropriate list based on current partition size. By using ghost lists (B1 and B2), ARC keeps track of what was recently evicted, allowing it to \u0026ldquo;learn\u0026rdquo; from recent cache misses and adjust its strategy accordingly.\nChallenges in Implementation # Implementing ARC can be complex compared to simpler algorithms. Maintaining four separate lists means additional overhead, and managing the size of each list efficiently can be challenging. Moreover, the algorithm\u0026rsquo;s performance depends on careful tuning. For instance, in a scenario where memory is tight, the added overhead might negate some of ARC\u0026rsquo;s benefits.\nAnother potential issue is the cost of adaptation. While ARC adapts well to changing workloads, the adaptation itself takes time and incurs computational overhead. If the access pattern changes too frequently, ARC might spend too much time adapting, leading to suboptimal performance.\nPractical Considerations # Memory Overhead: Since ARC keeps two ghost lists in addition to the primary cache, it needs extra memory to manage these lists. This could be problematic in memory-constrained environments. Latency Sensitivity: In systems where latency is critical, like financial trading platforms, ARC\u0026rsquo;s added overhead might make LRU or LFU a better choice despite their simplicity. However, for general-purpose databases or file systems, ARC\u0026rsquo;s adaptability can lead to much better hit rates over time. Code Snippet # Here’s a Python-style pseudocode to give you an idea of how ARC works:\nfrom collections import OrderedDict class ARC: def __init__(self, cache_size): self.cache_size = cache_size self.T1 = OrderedDict() # Recent cache (new) - Size = p self.B1 = OrderedDict() # Ghost cache for T1 (recently evicted from T1) self.T2 = OrderedDict() # Frequent cache - Size = Cach self.B2 = OrderedDict() # Ghost cache for T2 (recently evicted from T2) self.p = 0 # p represents the target size of T1 def request(self, key): if key in self.T1 or key in self.T2: # Cache hit self.hit(key) # T1 -\u0026gt; T2 or node in T2 -\u0026gt; T2.end elif key in self.B1: # Cache miss but in B1 (ghost of T1) self.adapt_cache(True) self.replace(key) # B in T out value = self.B1[key] del self.B1[key] self.T2[key] = value # Promote to T2 elif key in self.B2: # Cache miss but in B2 (ghost of T2) self.adapt_cache(False) self.replace(key) # B in T out value = self.B2[key] del self.B2[key] self.T2[key] = value # Promote to T2 else: # Cache miss and not in ghost caches if len(self.T1) + len(self.B1) \u0026gt;= self.cache_size: if len(self.T1) \u0026lt; self.cache_size: # Remove from B1 self.B1.popitem(last=False) replace(key) # B in T out else: # Remove from T1 self.T1.popitem(last=False) elif len(self.T1) + len(self.T2) + len(self.B1) + len(self.B2) \u0026gt;= 2 * self.cache_size: # Ensure total size does not exceed 2 * cache_size if len(self.B2) \u0026gt; 0: # Remove from B2 self.B2.popitem(last=False) else: # Remove from B1 self.B1.popitem(last=False) replace(key) # B in T out self.T1[key] = value_of(key) # Insert into T1 def hit(self, key): # Move the page to T2 if it is in T1, otherwise update position in T2 if key in self.T1: del self.T1[key] self.T2[key] = key # Move to T2 elif key in self.T2: # Update recency in T2 self.T2.move_to_end(key) def replace(self, key): # The replace function in ARC decides which cache, T1 (recent) or T2 (frequent), should evict an item when space is needed, based on the adaptive parameter p. if len(self.T1) \u0026gt;= 1 and ((key in self.B2 and len(self.T1) == self.p) or len(self.T1) \u0026gt; self.p): # Evict from T1 and add to B1 evicted, value = self.T1.popitem(last=False) self.B1[evicted] = value else: # Evict from T2 and add to B2 evicted, value = self.T2.popitem(last=False) self.B2[evicted] = value def adapt_cache(self, hit_in_B1): # Adjust the adaptive parameter p if hit_in_B1: delta = len(self.B2) / 2 if len(self.B1) == 0 else max(1, len(self.B2) / (2 * len(self.B1))) self.p = min(self.cache_size, self.p + delta) else: delta = len(self.B1) / 2 if len(self.B2) == 0 else max(1, len(self.B1) / (2 * len(self.B2))) self.p = max(0, self.p - delta) "},{"id":70,"href":"/docs/data-store-and-management/caching/caching-algorithm/fifo/","title":"Fifo","section":"Caching Algorithm","content":" FIFO # Practical Use Cases # FIFO can be particularly useful in scenarios where recency does not necessarily imply usefulness. For example:\nData Streams: Imagine you are buffering data from a sensor, like temperature readings. You may need a limited number of recent readings, but they are all of equal importance, and the order of their removal doesn\u0026rsquo;t need to depend on frequency or recency of use. Network Packet Management: When buffering network packets in a router, FIFO can be useful since the packets are processed in the order they arrive. Implementation Considerations # FIFO is dead simple to implement. You often use a queue for this, and operations are performed in O(1) time complexity. Adding an element (enqueue) and removing the oldest element (dequeue) are both constant-time operations, making FIFO very efficient.\nIn Python, you can implement a FIFO cache using collections.deque:\nfrom collections import deque import threading class FIFOCache: def __init__(self, capacity: int): self.cache = {} # Dictionary to store key-value pairs self.order = deque() # Deque to maintain the order of keys self.capacity = capacity self.lock = threading.Lock() # Lock for thread safety def put(self, key: int, value: int) -\u0026gt; None: with self.lock: # If the key is already in the cache, update its value if key in self.cache: self.cache[key] = value else: # If the cache is full, evict the oldest element if len(self.cache) \u0026gt;= self.capacity: self.evict() # Add the new key to the cache and its order to the deque self.cache[key] = value self.order.append(key) def get(self, key: int) -\u0026gt; int: with self.lock: # Retrieve the value associated with the key if key in self.cache: return self.cache[key] return -1 # Return -1 if the key is not found def evict(self): # Evict the oldest element (FIFO), which is at the front of the deque oldest_key = self.order.popleft() del self.cache[oldest_key] # Remove it from the dictionary def get_cache_state(self): # Returns the current state of the cache as a dictionary with self.lock: return dict(self.cache) Issues You Might Face # Although FIFO is easy to implement, it\u0026rsquo;s not without its flaws. Let\u0026rsquo;s discuss a few scenarios where FIFO might fall short:\nCache Miss Problem: FIFO does not account for frequency or recency of access, which means that highly used elements can be evicted simply because they were added earlier. This can lead to frequent cache misses and degrade performance, especially in systems with non-uniform access patterns. Example: Imagine you\u0026rsquo;re caching user sessions. If you use FIFO, an active user\u0026rsquo;s session could get evicted just because it was added before others, even if they are still actively using the application. Thrashing: This is when you keep evicting useful items repeatedly. If your access pattern changes and the FIFO eviction does not align well with it, your cache will continuously evict and replace the wrong entries, leading to poor hit rates. Example: In an e-commerce application, if users suddenly start revisiting previously viewed items, FIFO may have already evicted those items and you\u0026rsquo;ll end up with a cache miss, leading to extra processing or slower user experiences. Comparing FIFO with Other Algorithms # It\u0026rsquo;s important to evaluate FIFO against more sophisticated cache replacement strategies like Least Recently Used (LRU) or Least Frequently Used (LFU).\nLRU vs. FIFO: LRU keeps track of usage recency, which helps keep recently accessed items in the cache. This often performs better than FIFO in scenarios where recent items are more likely to be accessed again. LFU vs. FIFO: LFU keeps track of how frequently items are accessed. If certain items are requested often, LFU will ensure they stay in the cache, while FIFO would evict them just based on insertion time. FIFO, LRU, and LFU all have trade-offs in terms of implementation complexity and memory usage. FIFO is light on bookkeeping, making it attractive for limited-memory environments where tracking usage patterns is costly.\nWhere FIFO Fits in the Real World # FIFO tends to be popular in embedded systems or real-time systems where predictability is more important than optimized hit rates. Since FIFO is not burdened with the overhead of maintaining access history, it\u0026rsquo;s well-suited to environments where simplicity and constant eviction patterns are preferred.\nFor instance, a simple caching layer in a streaming application that doesn’t need to worry about access frequency might use FIFO just for ease of implementation and predictability.\nOptimizing FIFO in Practice # If you are using FIFO but facing frequent thrashing, one possible optimization is to create a hybrid approach where you apply FIFO to certain parts of your system (e.g., less critical, non-dynamic content) while using more sophisticated strategies elsewhere. You could also introduce heuristics to add flexibility: such as checking an item’s usage count before evicting it to prevent active elements from being removed.\nAnother potential tweak could involve partitioned caching, where you maintain several small FIFO queues to provide a broader set of items and reduce the chance of evicting frequently used content.\nConclusion # While FIFO is one of the simpler caching algorithms, its application and usefulness come down to the problem you\u0026rsquo;re tackling. If you are in a low-memory, high-throughput scenario with relatively uniform access to elements, FIFO can be an ideal choice due to its simplicity and minimal overhead.\nThat said, keep in mind the possible pitfalls, like thrashing or cache miss rates, and weigh them against the needs of your specific application. Often, FIFO works best when coupled with other strategies or applied selectively in parts of your system where simplicity is paramount.\n"},{"id":71,"href":"/docs/data-store-and-management/caching/caching-algorithm/lfu/","title":"Lfu","section":"Caching Algorithm","content":" LFU - Least Frequently Used (LFU) Caching # In LFU, every time a key is accessed, we increment a counter associated with it. When the cache gets full, we look for the key with the smallest count to evict. The underlying idea is simple: if a data point is hardly ever accessed, it’s less likely to be accessed again, so it\u0026rsquo;s not worth keeping in our limited space.\nHow LFU Works # Incoming Request for Data → Check Cache. Cache Hit: Increment frequency counter. Cache Miss: Fetch data, add it to cache. If cache is full, evict the least frequently used item. Real-World Applications # LFU can be particularly useful when dealing with media content platforms. Think of an app that serves both trending and evergreen content. Trending content is typically accessed a lot in a short period, while evergreen content gets slow but steady views. LFU helps to keep both popular trending pieces and consistent evergreen items in the cache without giving up either in favor of simply \u0026ldquo;the most recent\u0026rdquo; accesses.\nChallenges with LFU # Cold Start Problem: One practical issue with LFU is that it doesn’t handle new items well, particularly in cases where some old data has been in the cache for a long time. This problem, often called the cold start problem, can lead to newer items being evicted before they have a chance to build up their frequency. Possible Solution: One approach is to periodically reset the frequency counts or implement a decay mechanism to reduce the frequency of less recently accessed items. By doing this, items that haven’t been accessed in a long time have their frequency values naturally decreased, making space for fresh data. Memory Usage: Another challenge is memory overhead. Each cache entry in LFU requires an additional count field, and maintaining an efficient data structure to find the least frequently used element can be expensive in terms of time complexity. Practical Tip: A common way to manage this is to use a min-heap or a frequency list structure to minimize time spent looking up frequencies or evicting elements. Code Example: LFU in Python # Let’s look at a basic LFU cache implementation in Python. We\u0026rsquo;ll use a combination of a dictionary to store values and frequencies and a heap to efficiently manage evictions:\nLFU using heap - O(lgn) # import heapq class LFUCache: def __init__(self, capacity: int): self.capacity = capacity self.data = {} self.freq = {} self.min_heap = [] # (frequency, key) def get(self, key) -\u0026gt; any: if key not in self.data: return -1 # Update frequency self.freq[key] += 1 heapq.heappush(self.min_heap, (self.freq[key], key)) # Push new frequency, but note that stale entries remain (lazy deletion) return self.data[key] def put(self, key, value) -\u0026gt; None: if self.capacity == 0: return if key in self.data: self.data[key] = value self.get(key) # Update frequency and heap else: if len(self.data) \u0026gt;= self.capacity: # Evict least frequently used while self.min_heap: freq, evict_key = heapq.heappop(self.min_heap) # Perform lazy deletion: remove stale entries until we find a valid one if freq == self.freq[evict_key] and evict_key in self.data: del self.data[evict_key] del self.freq[evict_key] break # Insert new element self.data[key] = value self.freq[key] = 1 heapq.heappush(self.min_heap, (1, key)) LFU using hash map and doubly linked list - O(1) # class Node: def __init__(self, key, value): self.key = key self.value = value self.freq = 1 self.prev = None self.next = None class DoublyLinkedList: def __init__(self): self.head = Node(None, None) # Dummy head self.tail = Node(None, None) # Dummy tail self.head.next = self.tail self.tail.prev = self.head self.size = 0 def append(self, node): \u0026#34;\u0026#34;\u0026#34;Add a node to the end of the list.\u0026#34;\u0026#34;\u0026#34; node.prev = self.tail.prev node.next = self.tail self.tail.prev.next = node self.tail.prev = node self.size += 1 def pop(self, node=None): \u0026#34;\u0026#34;\u0026#34;Remove a node from the list (LRU policy if node is None).\u0026#34;\u0026#34;\u0026#34; if self.size == 0: return None if not node: node = self.head.next # LRU node node.prev.next = node.next node.next.prev = node.prev self.size -= 1 return node class LFUCache: def __init__(self, capacity: int): self.capacity = capacity self.size = 0 self.min_freq = 0 self.key_to_node = {} self.freq_to_list = {} def _update(self, node): \u0026#34;\u0026#34;\u0026#34;Update the frequency of a node.\u0026#34;\u0026#34;\u0026#34; freq = node.freq self.freq_to_list[freq].pop(node) if self.freq_to_list[freq].size == 0: del self.freq_to_list[freq] if self.min_freq == freq: self.min_freq += 1 # Only increase min_freq when the list is empty # Move the node to the next frequency list node.freq += 1 self.freq_to_list.setdefault(node.freq, DoublyLinkedList()).append(node) def get(self, key: int) -\u0026gt; int: if key not in self.key_to_node: return -1 node = self.key_to_node[key] self._update(node) return node.value def put(self, key: int, value: int) -\u0026gt; None: if self.capacity == 0: return if key in self.key_to_node: node = self.key_to_node[key] node.value = value self._update(node) else: if self.size \u0026gt;= self.capacity: # Evict the least frequently used item lfu_list = self.freq_to_list[self.min_freq] evicted_node = lfu_list.pop() del self.key_to_node[evicted_node.key] self.size -= 1 # Insert new node new_node = Node(key, value) self.key_to_node[key] = new_node self.freq_to_list.setdefault(1, DoublyLinkedList()).append(new_node) self.min_freq = 1 self.size += 1 "},{"id":72,"href":"/docs/data-store-and-management/caching/caching-algorithm/lru-k/","title":"Lru K","section":"Caching Algorithm","content":" LRU-K # The LRU-K (Least Recently Used-K) is an extension of the LRU (Least Recently Used) cache replacement algorithm. It keeps track of the K-th most recent access of an item rather than just the most recent one. By doing so, LRU-K can identify more frequently accessed items over time, which improves decision-making about which items to evict from the cache.\nKey Concepts of LRU-K: # Access History: Each item in the cache keeps track of its last K access times. Eviction Decision: The item with the oldest K-th access is chosen for eviction. This means items that have been accessed more recently will be retained, while items that haven’t been accessed frequently over their last K access times are evicted. Parameter K: The parameter K determines how many recent accesses to keep track of. When K = 1, LRU-K behaves just like a standard LRU cache. Steps to Implement LRU-K: # Data Structures: A hash map or dictionary to store cache items and their access history. A priority queue (min-heap) to efficiently find the item with the oldest K-th access. Tracking Accesses: Each time an item is accessed, its access history (a queue or list of timestamps) is updated. If the number of accesses exceeds K, the oldest access time is removed to keep the list of size K. Eviction Policy: When the cache reaches its capacity, the item with the oldest K-th access time is evicted. This is where the min-heap comes in, which helps to quickly determine the item with the oldest K-th access. # Time Complexity - O(lgn) import heapq import time class LRU_K_Cache: def __init__(self, capacity, K): self.capacity = capacity # Max size of the cache self.K = K # Number of recent accesses to track self.cache = {} # Stores cache entries and their access history self.heap = [] # Min-heap to track the K-th access time for each item with K accesses self.candidate_pool = {} # Items with fewer than K accesses def current_time(self): return time.time() # Simple time representation for access times def access(self, key, value=None): current_time = self.current_time() if key in self.cache: # Update access history for the key self.cache[key].append(current_time) if len(self.cache[key]) \u0026gt; self.K: self.cache[key].pop(0) # Keep the last K access times # Update the heap if this key has been accessed K times if len(self.cache[key]) == self.K: # Insert the new K-th access time into the heap heapq.heappush(self.heap, (self.cache[key][0], key)) else: # Add new item to candidate pool if it hasn\u0026#39;t been accessed K times if key in self.candidate_pool: self.candidate_pool[key].append(current_time) else: self.candidate_pool[key] = [current_time] # Promote to cache once it has been accessed K times if len(self.candidate_pool[key]) == self.K: self.cache[key] = self.candidate_pool.pop(key) heapq.heappush(self.heap, (self.cache[key][0], key)) # Handle eviction if cache size exceeds capacity if len(self.cache) + len(self.candidate_pool) \u0026gt; self.capacity: self.evict() def evict(self): \u0026#34;\u0026#34;\u0026#34;Evict an item based on LRU-K logic: prefer evicting items with fewer than K accesses.\u0026#34;\u0026#34;\u0026#34; if self.candidate_pool: # Evict an item from the candidate pool first (items with fewer than K accesses) key_to_evict = next(iter(self.candidate_pool)) # Get any item from the candidate pool del self.candidate_pool[key_to_evict] else: # If all items have been accessed K times, evict based on the K-th access time while self.heap: oldest_kth_access, key_to_evict = heapq.heappop(self.heap) # Check if the cache has this key and if the K-th access time matches if key_to_evict in self.cache and self.cache[key_to_evict][0] == oldest_kth_access: del self.cache[key_to_evict] break # Evict the valid item "},{"id":73,"href":"/docs/data-store-and-management/caching/caching-algorithm/lru/","title":"Lru","section":"Caching Algorithm","content":" LRU # LRU Caching # The core idea revolves around using a mechanism to evict the data that\u0026rsquo;s been least recently accessed when we run out of capacity. We generally use a combination of a hash map and a doubly linked list to achieve an optimal O(1) time complexity for both put and get operations.\nData Structures for LRU: Hash Map and Doubly Linked List # Let’s break down why the combination of a hash map and doubly linked list is so effective. The hash map provides constant-time access for cache hits, while the doubly linked list maintains the order of usage. Every time we access an element, we update its position in the linked list to represent its recent use. The tail of the list holds the least recently used item—perfect for easy eviction.\nA practical issue you might run into, though, is balancing this cache when dealing with concurrent access. For example, let’s say you have multiple threads hitting this cache. How do you make sure the cache remains thread-safe without compromising on the efficiency that LRU is supposed to bring? A typical approach is using read-write locks or even implementing the LRU in a lock-free manner if you\u0026rsquo;re after extreme performance.\nImplementation using map and doubly linked list # class Node: def __init__(self, key: int, value: int): self.key = key self.value = value self.prev = None self.next = None class LRUCache: def __init__(self, capacity: int): self.capacity = capacity self.cache = {} # This will act as the hash map # Initialize the head and tail of the doubly linked list self.head = Node(0, 0) # Dummy head self.tail = Node(0, 0) # Dummy tail self.head.next = self.tail self.tail.prev = self.head def _remove(self, node: Node): \u0026#34;\u0026#34;\u0026#34;Remove a node from the doubly linked list.\u0026#34;\u0026#34;\u0026#34; prev_node = node.prev next_node = node.next prev_node.next = next_node next_node.prev = prev_node def _add(self, node: Node): \u0026#34;\u0026#34;\u0026#34;Add a node right after the head (most recently used position).\u0026#34;\u0026#34;\u0026#34; next_node = self.head.next node.prev = self.head node.next = next_node self.head.next = node next_node.prev = node def get(self, key: int) -\u0026gt; int: if key in self.cache: node = self.cache[key] self._remove(node) # Remove it from its current position self._add(node) # Add it back to the front (most recently used) return node.value return -1 # Key not found def put(self, key: int, value: int) -\u0026gt; None: if key in self.cache: # Update the value and move it to the front node = self.cache[key] node.value = value self._remove(node) self._add(node) else: if len(self.cache) \u0026gt;= self.capacity: # Remove the least recently used item from the cache and linked list lru_node = self.tail.prev self._remove(lru_node) del self.cache[lru_node.key] # Add the new key-value pair new_node = Node(key, value) self.cache[key] = new_node self._add(new_node) Implementation using OrderedDict # from collections import OrderedDict class LRUCache: def __init__(self, capacity: int): self.capacity = capacity self.cache = OrderedDict() def get(self, key: int) -\u0026gt; int: if key not in self.cache: return -1 # Move the key to the end to show it was recently used self.cache.move_to_end(key) return self.cache[key] def put(self, key: int, value: int) -\u0026gt; None: if key in self.cache: # Update the value self.cache.move_to_end(key) self.cache[key] = value if len(self.cache) \u0026gt; self.capacity: # Remove the first (least recently used) item self.cache.popitem(last=False) Common Scenarios: Issues in Production Environments # Cache Thrashing: One scenario where LRU can become problematic is cache thrashing. Suppose your cache size is significantly smaller than the working set of your data. You might end up evicting items that are re-requested almost immediately, resulting in frequent cache misses and a performance penalty. The LRU cache might start working against you, increasing latency rather than reducing it. A potential fix here is to use a hybrid approach like LRU-K, where you look at the K most recent uses rather than a single instance of usage to help the cache make better decisions. Hot Data: Another issue is hot data. In an LRU cache, if some data is accessed constantly while other entries only see sporadic hits, this can cause the cache to retain less useful information. One way to handle this is to introduce a segmented LRU (SLRU), where frequently accessed items are stored in a separate, higher-priority segment. This way, you minimize the risk of valuable data being evicted because of a burst in accesses elsewhere. Performance Considerations: Memory Overhead and Latency # Memory overhead is something to bear in mind. If you have a massive cache with millions of entries, keeping a doubly linked list for maintaining order can introduce significant memory overhead. The linked list itself, with its pointers, can be quite hefty. In such cases, some developers opt for a segmented least recently used (SLRU) approach, where they divide their cache into multiple parts, thus limiting the linked list size per segment, or they use TinyLFU to augment the decision-making process with frequency counts.\nAdvanced Variants of LRU # While LRU is a popular and effective caching strategy, there are advanced variants that improve upon its basic approach to address specific use cases:\nLRU-K Segmented LRU (SLRU) TinyLFU Reference: # https://redis.io/glossary/lru-cache/ "},{"id":74,"href":"/docs/data-store-and-management/caching/caching-algorithm/mru/","title":"Mru","section":"Caching Algorithm","content":" MRU # What Makes MRU Different? # Most Recently Used (MRU) is a caching strategy where, when the cache is full and you need to make room for new data, the algorithm will remove the most recently used item to make space. Unlike Least Recently Used (LRU), which removes the item that hasn\u0026rsquo;t been used for the longest time, MRU assumes that the item just used is less likely to be needed again soon.\nThis approach may seem counterintuitive, but it\u0026rsquo;s effective in specific use cases. The main idea behind MRU is to evict the item that has been accessed recently, under the assumption that recently used items are less likely to be used again compared to older ones.\nPractical Scenarios for MRU # MRU works well in scenarios where items are typically accessed just once or in a sequential manner. A practical example is accessing records in a transaction processing system where recent data might only need to be read once and then discarded. Another scenario could be in media streaming applications, where users often consume content linearly, moving from one segment to the next without revisiting previous segments.\nIn these types of workloads, MRU helps maintain optimal cache performance by keeping data that is more likely to be needed in the near future while discarding the items just accessed.\nHowever, MRU is not ideal for workloads where data is frequently revisited soon after it was accessed. For instance, in web browsing or repeated searches, MRU can lead to frequent cache misses, as the items evicted are often required again shortly after being used.\nChallenges with MRU # One of the key challenges of MRU is its susceptibility to cache thrashing when data access patterns change frequently. Imagine a scenario where you have a small cache and are frequently accessing new items while occasionally revisiting old ones. In such a case, MRU\u0026rsquo;s tendency to remove the most recently used item may lead to evicting data that is still useful, causing excessive cache misses and reducing performance.\nCode Example: MRU Implementation # Here’s a Python snippet demonstrating how MRU could be implemented with additional considerations for thread safety and cache eviction policies:\nimport threading from collections import OrderedDict class MRUCache: def __init__(self, capacity: int): self.cache = OrderedDict() self.capacity = capacity self.lock = threading.Lock() def get(self, key: int) -\u0026gt; int: with self.lock: if key not in self.cache: return -1 else: # Move the accessed item to the end to mark it as most recently used value = self.cache.pop(key) self.cache[key] = value return value def put(self, key: int, value: int) -\u0026gt; None: with self.lock: if key in self.cache: # Remove the old value self.cache.pop(key) elif len(self.cache) \u0026gt;= self.capacity: # Pop the most recently used item self.evict() # Insert the new key-value pair as the most recent one self.cache[key] = value def evict(self): # Evict the most recently used item (the last item in the ordered dict) self.cache.popitem(last=True) def get_cache_state(self): with self.lock: return list(self.cache.items()) MRU vs. Other Caching Algorithms # Compared to LRU and LFU, MRU is most effective in environments where recently accessed items are unlikely to be used again. For example, LRU would be a better choice in scenarios with frequent re-access to older data, as it keeps the most recently accessed items, assuming that data accessed earlier is less likely to be needed.\nIn contrast, LFU keeps track of how often items are accessed and evicts the least frequently used items. MRU, on the other hand, ignores frequency and focuses on the recency of access, which can be both its strength and its downfall depending on the use case.\nPractical Issues and Considerations # Cache Size: MRU\u0026rsquo;s effectiveness heavily depends on the cache size relative to the working set. A cache that is too small will lead to frequent evictions and cache misses. Access Pattern Changes: If your application\u0026rsquo;s access patterns change dynamically, MRU may struggle to adapt, resulting in inefficient cache usage. Hybrid Approaches: In some situations, a hybrid caching approach may be beneficial. Combining MRU with other algorithms (e.g., an LRU-MRU hybrid) can help balance the strengths of each, depending on data access patterns. "},{"id":75,"href":"/docs/data-store-and-management/caching/caching-strategies/","title":"Caching Strategies","section":"Caching","content":"Cache Aside Read/Write Through Write-Behind (Write-Back) Refresh-Ahead\n"},{"id":76,"href":"/docs/data-store-and-management/caching/caching-strategies/caching-strategies/","title":"Caching Strategies","section":"Caching Strategies","content":"4.1 Write-through Cache 4.2 Write-back Cache 4.3 Write-around Cache 4.4 Cache Eviction Policies\n"},{"id":77,"href":"/docs/data-store-and-management/caching/redis/","title":"Redis","section":"Caching","content":"Redis\n"},{"id":78,"href":"/docs/data-store-and-management/caching/redis/data-type/","title":"Data Type","section":"Redis","content":"TBD\n"},{"id":79,"href":"/docs/data-store-and-management/caching/redis/performance/","title":"Performance","section":"Redis","content":"TBD\n"},{"id":80,"href":"/docs/data-store-and-management/caching/redis/persistence/","title":"Persistence","section":"Redis","content":"TBD\n"},{"id":81,"href":"/docs/data-store-and-management/caching/redis/security/","title":"Security","section":"Redis","content":"TBD\n"},{"id":82,"href":"/docs/data-store-and-management/caching/redis/source-code/","title":"Source Code","section":"Redis","content":"TBD\n"},{"id":83,"href":"/docs/data-store-and-management/caching/types-of-caching/","title":"Types of Caching","section":"Caching","content":"Type of Caching\n"},{"id":84,"href":"/docs/data-store-and-management/caching/types-of-caching/broswer-caching/","title":"Broswer Caching","section":"Types of Caching","content":"TBD\n"},{"id":85,"href":"/docs/data-store-and-management/caching/types-of-caching/cdn/","title":"Cdn","section":"Types of Caching","content":"TBD\n"},{"id":86,"href":"/docs/data-store-and-management/caching/types-of-caching/cpu-caching/","title":"CPU Caching","section":"Types of Caching","content":"TBD\n"},{"id":87,"href":"/docs/data-store-and-management/caching/types-of-caching/disk-caching/","title":"Disk Caching","section":"Types of Caching","content":"TBD\n"},{"id":88,"href":"/docs/data-store-and-management/caching/types-of-caching/ram-caching/","title":"RAM Caching","section":"Types of Caching","content":"TBD\n"},{"id":89,"href":"/docs/data-store-and-management/database/","title":"Database","section":"Docs","content":"Database\n"},{"id":90,"href":"/docs/data-store-and-management/database/graph/","title":"Graph","section":"Database","content":"Graph Database\n"},{"id":91,"href":"/docs/data-store-and-management/database/nosql/","title":"Nosql","section":"Database","content":"NoSQL Database\nTypes of NoSQL Databases:\nKey-Value Stores Document Databases Wide-Column Stores Graph Databases "},{"id":92,"href":"/docs/data-store-and-management/database/nosql/data-modeling/","title":"Data Modeling","section":"Nosql","content":"Data Modeling\n"},{"id":93,"href":"/docs/data-store-and-management/database/nosql/data-modeling/indexing/","title":"Indexing","section":"Data Modeling","content":"TBD\n"},{"id":94,"href":"/docs/data-store-and-management/database/nosql/data-modeling/partitioning/","title":"Partitioning","section":"Data Modeling","content":"TBD\n"},{"id":95,"href":"/docs/data-store-and-management/database/nosql/data-modeling/sharding/","title":"Sharding","section":"Data Modeling","content":"TBD\n"},{"id":96,"href":"/docs/data-store-and-management/database/object/","title":"Object","section":"Database","content":"Object Database\n"},{"id":97,"href":"/docs/data-store-and-management/database/orm/","title":"Orm","section":"Database","content":"TBD\n"},{"id":98,"href":"/docs/data-store-and-management/database/relational-databases/","title":"Relational Databases","section":"Database","content":"Relational Database\n"},{"id":99,"href":"/docs/data-store-and-management/database/relational-databases/data-modeling/","title":"Data Modeling","section":"Relational Databases","content":"Data Modeling\n"},{"id":100,"href":"/docs/data-store-and-management/database/relational-databases/data-modeling/indexing/","title":"Indexing","section":"Data Modeling","content":"TBD\n"},{"id":101,"href":"/docs/data-store-and-management/database/relational-databases/data-modeling/partitioning/","title":"Partitioning","section":"Data Modeling","content":"TBD\n"},{"id":102,"href":"/docs/data-store-and-management/database/relational-databases/data-modeling/sharding/","title":"Sharding","section":"Data Modeling","content":"TBD\n"},{"id":103,"href":"/docs/data-store-and-management/database/relational-databases/postgres/","title":"Postgres","section":"Relational Databases","content":" What is search_path? # The default search path usually includes the public schema, which is why \\dt typically shows tables from the public schema if no schema is specified. However, if your search path is set to include other schemas, or if you\u0026rsquo;ve modified the default search path, \\dt will show tables from those schemas as well.\nTo view the current search path, you can use the following command in psql:\nSHOW search_path;\nWhat is schema? # "},{"id":104,"href":"/docs/data-store-and-management/database/time-series/","title":"Time Series","section":"Database","content":"Time Series Database\n"},{"id":105,"href":"/docs/data-store-and-management/database/transactions-and-concurrency/","title":"Transactions and Concurrency","section":"Database","content":"Transaction\n"},{"id":106,"href":"/docs/data-store-and-management/database/transactions-and-concurrency/acid/","title":"Acid","section":"Transactions and Concurrency","content":"TBD\n"},{"id":107,"href":"/docs/data-store-and-management/database/transactions-and-concurrency/concurrency/","title":"Concurrency","section":"Transactions and Concurrency","content":"TBD\n"},{"id":108,"href":"/docs/data-store-and-management/database/transactions-and-concurrency/isolation/","title":"Isolation","section":"Transactions and Concurrency","content":"TBD\n"},{"id":109,"href":"/docs/data-store-and-management/files-systems/","title":"Files Systems","section":"Docs","content":"File Systems\n"},{"id":110,"href":"/docs/data-store-and-management/files-systems/basic-concepts/","title":"Basic Concepts","section":"Files Systems","content":"File Systems\n"},{"id":111,"href":"/docs/data-store-and-management/files-systems/distributed/","title":"Distributed","section":"Files Systems","content":"Distributed File Systems\n"},{"id":112,"href":"/docs/data-store-and-management/files-systems/network/","title":"Network","section":"Files Systems","content":"Network File System\n"},{"id":113,"href":"/docs/data-store-and-management/files-systems/traditional/","title":"Traditional","section":"Files Systems","content":"Traditional File System\n"},{"id":114,"href":"/docs/data-store-and-management/message-middleware/rabbitmq/priority-queue/","title":"Priority Queue","section":"Rabbitmq","content":" Does a priority queue with 5 levels in RabbitMQ maintain 5 separate queues internally? # In RabbitMQ, a priority queue allows you to assign different levels of priority to messages, which affects the order in which they are consumed. Messages with higher priority are dequeued and processed before lower-priority messages, regardless of when they were published.\nHere’s how it works and some examples to illustrate:\nHow Priority Queues Work in RabbitMQ: # Priority Levels: When you declare a queue with x-max-priority, you specify the number of priority levels. For instance, if x-max-priority is set to 5, the priority levels will range from 0 (lowest priority) to 4 (highest priority). Message Priority: When publishing a message, you specify the priority as an argument. If a message doesn\u0026rsquo;t have a priority set, it will be treated as having the default priority of 0. Processing Order: RabbitMQ will process messages with higher priority before lower-priority messages, even if lower-priority messages arrived earlier. Priority Queue Best Practices: # Limited Levels: RabbitMQ limits the number of priority levels (e.g., 255 maximum), but it\u0026rsquo;s usually more practical to work with a small number of levels (e.g., 3-10). Too many priority levels can complicate the system. Real Use Case: Priority queues are ideal for systems where not all messages are equally important, such as task scheduling, alert systems, and message handling in applications where some actions need to take precedence over others. By using priority queues in RabbitMQ, you can fine-tune how your system handles and processes tasks or messages, ensuring that more important actions are completed first.\nDoes a priority queue with 5 levels maintain 5 separate queues internally? # No, a priority queue with 5 levels in RabbitMQ does not maintain 5 separate queues internally. Instead, it maintains a single queue where the messages are stored, but the queue internally tracks the priority levels of the messages and adjusts the order of message delivery based on their priority.\nHere\u0026rsquo;s how it works: # Single Queue Structure: There is only one queue, but messages are tagged with different priority levels (e.g., priority 0 through 4 for a queue with 5 levels). When consumers retrieve messages, RabbitMQ selects the highest-priority messages from this single queue, rather than managing multiple queues.\nInternally Ordered: RabbitMQ keeps track of the message priorities and reorders the messages internally so that higher-priority messages are delivered before lower-priority messages. The prioritization happens within the queue itself rather than having separate queues for each priority level.\nExample: # You declare a priority queue with x-max-priority=4, meaning 5 priority levels (0, 1, 2, 3, and 4). Messages are published with different priorities (e.g., some with priority 4, some with priority 2, and some with priority 0). All these messages are stored in the same queue. However, when the messages are dequeued, RabbitMQ ensures that higher-priority messages (e.g., priority 4) are delivered before lower-priority messages (e.g., priority 0). Alternative Approach (Multiple Queues for Each Priority): # If RabbitMQ were to maintain separate internal queues for each priority level (e.g., 5 queues for a priority queue with 5 levels), the process could be more efficient in certain cases:\nAdding Messages: Each message would simply be appended to its corresponding priority queue (e.g., messages with priority 4 go into queue 4). Retrieving Messages: Messages could be dequeued from the highest-priority queue that has messages available, which would be quicker as there’s no need to reorder messages within a single queue. Trade-offs: # Single Queue with Internal Prioritization (RabbitMQ\u0026rsquo;s Approach):\nPros: Simpler queue management (single queue), easier for the broker to maintain. Cons: Computationally more expensive when reordering messages, especially with large numbers of messages and frequent priority changes. Multiple Queues for Each Priority Level (Hypothetical Alternative):\nPros: More efficient for insertion and removal (constant time operations for each priority queue). Cons: Increased complexity in managing multiple queues, potential overhead in checking which queue has messages when processing, and more memory overhead as RabbitMQ would need to maintain metadata for each priority queue. Why RabbitMQ Uses a Single Queue: # RabbitMQ likely chooses to implement priority queues as a single queue with internal prioritization to avoid the complexity and overhead of managing multiple queues. It simplifies the queue management logic, and for many use cases, the performance trade-offs are acceptable. The priority queue feature is primarily designed for workloads where the number of messages is not excessively large, and where the reordering costs do not become a significant bottleneck.\nHowever, in extremely high-performance systems, maintaining multiple queues for each priority level (as you described) could indeed be more efficient, but it would also require more complex management logic on the broker side.\nHow RabbitMQ Implements Priority Queues: # RabbitMQ likely uses a combination of:\nLinked lists or arrays to hold messages. Priority comparisons to reorder the queue as messages are inserted or dequeued. This approach allows RabbitMQ to handle messages with different priorities without adhering to a strict heap-based structure. The message ordering is maintained based on the message\u0026rsquo;s priority, and the broker ensures that higher-priority messages are delivered before lower-priority messages.\nWhy Not Use a Heap? # While heaps are often used to implement priority queues due to their efficient time complexity for insertion and removal, RabbitMQ is designed for messaging systems with unique needs:\nMessage Acknowledgements: RabbitMQ supports message acknowledgements, re-delivery, and various delivery guarantees, which can complicate the use of a strict heap structure. Persistence and Durability: RabbitMQ can persist messages to disk, which may further complicate the use of a heap due to the need to maintain message order across memory and disk storage. Flexible Prioritization: RabbitMQ allows custom priority levels (e.g., you can specify any number of priority levels), and handling these priorities flexibly may be easier with a more generalized data structure rather than a strict binary heap. "},{"id":115,"href":"/docs/data-store-and-management/search-engine/","title":"Search Engine","section":"Docs","content":"Search Engine\n"},{"id":116,"href":"/docs/data-store-and-management/search-engine/elasticsearch/","title":"Elasticsearch","section":"Search Engine","content":"Elastic Search\n"},{"id":117,"href":"/docs/data-store-and-management/search-engine/elasticsearch/boolean-query/","title":"Boolean Query","section":"Elasticsearch","content":" The difference between must clause and filter clause # The must and filter clauses within a boolean query in Elasticsearch have distinct roles and behaviors:\nmust Clause:\nPurpose: The must clause is used to specify search criteria that must be met for a document to be included in the search results. Behavior: Queries in the must clause contribute to the scoring of the document. This means that matching documents are not only required to match the criteria specified in the must clause but their relevance score is also influenced by how well they match these criteria. Use Case: Use the must clause for conditions that are essential for the search and where the degree of matching affects the relevance of the document. filter Clause:\nPurpose: The filter clause is used to apply a filter to the search results without affecting the scoring. Behavior: Queries in the filter clause do not contribute to the scoring of documents. They are used purely to include or exclude documents based on the filter criteria. Documents either match the filter or they don\u0026rsquo;t. Use Case: Use the filter clause for conditions that are binary (a document either meets the condition or it doesn\u0026rsquo;t) and when you do not need the condition to influence the relevance score. In summary, the must clause affects both the inclusion of documents in the search results and their relevance scoring, while the filter clause affects only the inclusion of documents, without impacting their scoring. The choice between must and filter can also have performance implications, as filters can be cached for faster execution in subsequent queries.\n"},{"id":118,"href":"/docs/data-store-and-management/search-engine/elasticsearch/customized-preference-search/","title":"Customized Preference Search","section":"Elasticsearch","content":" In which step, the user preference is used to customize the searching results? # The user preference is primarily used to customize the search results during the step where the search query is tailored before being sent to Elasticsearch. This is a crucial step for integrating user-specific context into the search process. Here\u0026rsquo;s how it works:\nStep of Integrating User Preferences: # Retrieve User Context and Preferences:\nAfter the user initiates a search query, the system first retrieves relevant user data from the relational database. This data includes the user\u0026rsquo;s preferences, such as topics they frequently engage with, users they follow, and their past search history. Tailoring the Search Query:\nBased on this user context, the actual search query that will be sent to Elasticsearch is adjusted or tailored. For instance, if a user often reads about \u0026ldquo;sustainable technology,\u0026rdquo; this interest can be factored into the query. The search can be tailored to boost posts or content related to \u0026ldquo;sustainable technology\u0026rdquo; in the search results. This tailoring can involve modifying the query to prioritize certain keywords, applying filters, or boosting certain types of content within the search results. Executing the Tailored Query in Elasticsearch:\nThe modified query is then executed in Elasticsearch, which searches the indexed post content. Elasticsearch processes this query, taking into account the tailored aspects that reflect the user\u0026rsquo;s preferences. Example Scenario: # User\u0026rsquo;s Interest: Suppose a user frequently interacts with posts about \u0026ldquo;space exploration\u0026rdquo;. Search Query: The user searches for \u0026ldquo;recent discoveries\u0026rdquo;. Query Tailoring: The search query is tailored to boost relevance of content related to \u0026ldquo;space exploration\u0026rdquo;. Elasticsearch Query: The tailored query might look like \u0026quot;recent discoveries\u0026quot; AND \u0026quot;space exploration\u0026quot; or might include a boosted relevance for \u0026ldquo;space exploration\u0026rdquo; related terms. Customized Results: The search results are then more likely to include and prioritize posts about recent discoveries in space exploration, aligning with the user\u0026rsquo;s known interests. Conclusion: # The integration of user preferences to customize search results primarily occurs during the query tailoring step. This approach ensures that the search results are personalized, reflecting the individual interests and behaviors of the user, leading to a more engaging and relevant user experience.\nDoes it mean that the user preference is not a fine-tune factor inside the elastic search relevance scoring algorithm? # That\u0026rsquo;s correct. User preferences typically don\u0026rsquo;t directly fine-tune the internal relevance scoring algorithm of Elasticsearch itself. Instead, user preferences are used to modify the search query before it is sent to Elasticsearch. This pre-processing step tailors the query to align with the user\u0026rsquo;s interests, but the core scoring algorithm within Elasticsearch remains unchanged.\nHere\u0026rsquo;s how it works:\nOutside of Elasticsearch:\nUser preferences and contextual data are managed outside of Elasticsearch, often in a relational database or another data store. These preferences include a user\u0026rsquo;s search history, interactions, follows, likes, etc. Query Modification:\nWhen a search request is made, the query is modified or tailored based on these user preferences before being sent to Elasticsearch. This could involve boosting certain terms, adding specific filters, or prioritizing certain content types based on the user\u0026rsquo;s profile. Elasticsearch Processing:\nOnce the modified query reaches Elasticsearch, it processes the query using its standard relevance scoring algorithms, like BM25. The query modifications influence which documents are considered more relevant, but the underlying scoring mechanism remains standard. Customization via Query, Not Scoring Algorithm:\nThe customization for user preferences is thus realized through query construction, not by altering Elasticsearch\u0026rsquo;s internal scoring algorithms. This allows for flexibility and user-specific customization without the need to modify the complex workings of the search engine\u0026rsquo;s scoring system. Example: # If a user frequently interacts with content about \u0026ldquo;artificial intelligence\u0026rdquo;, a search query for \u0026ldquo;latest research\u0026rdquo; might be adjusted to \u0026ldquo;latest research in artificial intelligence\u0026rdquo; or given a higher relevance score for documents related to \u0026ldquo;artificial intelligence\u0026rdquo;. Conclusion: # In essence, user preference customization in search platforms like Twitter is achieved through intelligent query construction and manipulation, leveraging the powerful, but standard, scoring capabilities of Elasticsearch. This approach provides a balance between personalized search results and the robust, tried-and-tested scoring algorithms of Elasticsearch.\n"},{"id":119,"href":"/docs/data-store-and-management/search-engine/elasticsearch/full-text-search/","title":"Full Text Search","section":"Elasticsearch","content":" What is full-text search? # Full-text search refers specifically to searching for text within content in a way that understands the nuances of human language, like handling synonyms, stemming, and relevance scoring.\nInverted Index # When documents are indexed, the search engine processes the text and creates a list of all the unique words (terms) present in the documents. For each term, the engine maintains a list of documents (often with specific positions within those documents) where that term appears. This list is known as a \u0026ldquo;posting list.\u0026rdquo; How the inverted index Works in Full-Text Search? # Query Processing When a user submits a search query, the search engine processes the query terms in the same way as it processes document text during indexing. Each term in the query is looked up in the inverted index. Retrieving Relevant Documents The search engine retrieves the posting lists for each term in the query. If the query has multiple terms, the search engine combines these lists to find documents that contain all (or some) of the terms, depending on the type of search (e.g., boolean, phrase search). Ranking The search engine then ranks these documents based on various factors, including how often and where the terms appear in each document, the length of the documents, and the rarity of the terms across all documents. The ranked results are then presented to the user. How the Term Frequency(TF) is used in full-text search? # Step 1: Finding Related Documents\nInverted Index Lookup: When you perform a full-text search, Elasticsearch first uses the inverted index to quickly identify all the documents that contain the query terms. At this stage, the primary goal is to find all potentially relevant documents, without immediately considering their relative relevance. Term Presence: The focus here is on whether the terms are present in the documents, not how frequently they appear. This step is crucial for efficiency, as it narrows down the search to a subset of the entire dataset. Step 2: Ranking the Documents\nTerm Frequency Involvement: Once Elasticsearch has a list of all documents that match the query terms, it then uses term frequency (along with other factors) to rank these documents in order of relevance to the query. Relevance Scoring: This is where term frequency becomes crucial. Documents where the query terms appear more frequently might be considered more relevant. However, Elasticsearch\u0026rsquo;s relevance scoring algorithms (like TF-IDF or BM25) also take into account other factors, such as the rarity of the terms (document frequency) and the overall length of the document. Contextual Factors: Other contextual factors may also influence ranking, such as the use of synonyms, the proximity of query terms within the document, and any custom ranking criteria specified in the query. Inverse Document Frequency (IDF) # The IDF is a measure of how much information a word provides, i.e., how common or rare it is across all documents in the dataset. IDF is calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the term. Here\u0026rsquo;s the formula for IDF:\n$$ \\text{IDF}(t) = \\log \\left( \\frac{N}{n_t} \\right) $$\nWhere:\n$ N $ is the total number of documents in the dataset. $ n_t $ is the number of documents that contain the term $ t $. So, if a term is very common and appears in many documents, its IDF value decreases. Conversely, if a term is rare, its IDF value increases, indicating it provides more distinguishing power.\nMissing terms in the query # An inverted index in Elasticsearch (or any full-text search system) maps terms to their locations within documents. When you perform a search query, the system looks up the terms in this index to find matching documents.\nIf a term from your query does not appear in any document, it essentially contributes nothing to the search results for that query. Elasticsearch will not find any matches for that term in the inverted index.\nThe overall search results will depend on how Elasticsearch handles the query. If your query has multiple terms, the search engine might still return documents that match the other terms in your query.\nThe impact of a missing term also depends on how the query is structured. For instance, in a boolean query, if the missing term is a \u0026ldquo;must\u0026rdquo; condition, then no results will be returned. However, if it\u0026rsquo;s a \u0026ldquo;should\u0026rdquo; condition, other terms can still influence the search results.\nIn summary, the absence of a term in all documents means that this particular term does not contribute to the matching process, but the overall query can still return results based on other terms and how the query logic is structured.\nMatch - the standard query for full-text search # When a match query is executed, Elasticsearch first analyzes the query string using the same analyzer that was used for the field being searched. This process breaks the text into tokens (terms). The query then uses the inverted index to quickly find the documents where those tokens appear. Remember, the inverted index is essentially a mapping of terms to the documents that contain them. For each token derived from the query text, Elasticsearch looks up the inverted index to find and retrieve the documents containing that token. After gathering all the relevant documents for each token, Elasticsearch then scores these documents based on relevance. The scoring might include factors like term frequency (how often a term appears in a document) and document frequency (how often the term appears across all documents), among other criteria. Example Scenario: Searching Tweets # Imagine you have an Elasticsearch index containing tweets, and you perform a match query to find tweets about \u0026ldquo;sunny weather\u0026rdquo;.\nStep 1: Analyze the Query # Query: \u0026ldquo;sunny weather\u0026rdquo; Analysis: The query is analyzed (tokenized and normalized) into terms: [\u0026ldquo;sunny\u0026rdquo;, \u0026ldquo;weather\u0026rdquo;]. Step 2: Use the Inverted Index # Elasticsearch uses the inverted index to find documents containing these terms.\nInverted Index Example: \u0026ldquo;sunny\u0026rdquo;: [Doc1, Doc3, Doc5] \u0026ldquo;weather\u0026rdquo;: [Doc2, Doc3, Doc6] Step 3: Gathering Documents # For \u0026ldquo;sunny\u0026rdquo;: Retrieves Doc1, Doc3, Doc5 For \u0026ldquo;weather\u0026rdquo;: Retrieves Doc2, Doc3, Doc6 Step 4: Scoring and Combining Results # Scoring: Each document is scored based on factors like term frequency, document frequency, etc. Combining Results: Documents containing both terms (like Doc3) are considered more relevant. Final Result: Documents are ranked by relevance. In this case, Doc3 might be ranked highest because it contains both \u0026ldquo;sunny\u0026rdquo; and \u0026ldquo;weather\u0026rdquo;. Example Explanation # Doc1: Contains \u0026ldquo;sunny\u0026rdquo; but not \u0026ldquo;weather\u0026rdquo;. Scores some points for \u0026ldquo;sunny\u0026rdquo;. Doc2: Contains \u0026ldquo;weather\u0026rdquo; but not \u0026ldquo;sunny\u0026rdquo;. Scores some points for \u0026ldquo;weather\u0026rdquo;. Doc3: Contains both \u0026ldquo;sunny\u0026rdquo; and \u0026ldquo;weather\u0026rdquo;. Scores higher points as it matches the entire query. Conclusion # The match query doesn\u0026rsquo;t simply intersect lists of documents for each term. Instead, it gathers all documents that match any of the terms and then scores them based on their match relevance. Documents that contain more query terms (or terms that are rarer or more significant in the context) are scored higher.\nNote # This example is simplified. In reality, Elasticsearch\u0026rsquo;s scoring algorithm is more complex, incorporating additional factors like the proximity of terms, the overall length of the document, etc. Elasticsearch can handle more complex queries and ranking logic, depending on the specific needs and configurations of the search application. How the match accelerate Relevance Scoring? Will that be time-consuming if elastic search score all the documents that contains one or more terms from a match query? # Scoring Algorithms Optimization:\nExample: Modern scoring algorithms, like BM25 (used by Elasticsearch), are designed for performance. They balance accuracy with computational efficiency.\nBM25 and TF-IDF in Elasticsearch # BM25: BM25 is a scoring algorithm used in Elasticsearch for relevance scoring. It is an evolution of the TF-IDF (Term Frequency-Inverse Document Frequency) algorithm. BM25 improves upon TF-IDF by considering the length of documents and mitigating the impact of term frequency saturation. It provides a more sophisticated way of scoring documents based on query terms. TF-IDF: While TF-IDF was traditionally used in Elasticsearch and other search engines, modern versions of Elasticsearch use BM25 by default. However, TF-IDF is still conceptually important and underlies many of the principles of BM25. Early Termination:\nExample: For a query that only needs the top 10 results, Elasticsearch can stop considering documents once it has identified the 10 best matches, rather than scoring all possible documents.\nBlock-Max WAND (Weak AND): This is a technique used to efficiently skip non-competitive documents. The idea is to keep a threshold score and skip scoring documents that are unlikely to exceed this threshold. By processing documents in blocks and using precomputed maximum scores for terms in each block, Elasticsearch can quickly disregard blocks of documents that don\u0026rsquo;t have the potential to meet the threshold. Tiered Processing: Sometimes, Elasticsearch may first process a subset of the data or use a simplified scoring model to estimate which documents are likely to be most relevant, and then perform detailed scoring on this narrowed set. Document Frequency Thresholds:\nExample: Elasticsearch may skip scoring documents for very common terms if they exceed a certain frequency threshold, as these terms might not be useful for distinguishing relevant documents.\nDocument Frequency Thresholds: In some configurations, Elasticsearch can use document frequency thresholds as a way to optimize query performance. This means that terms which appear in an extremely high number of documents (i.e., very common terms) might be skipped or given less weight in the scoring process. Relation to IDF: The concept is related to IDF in the sense that both are concerned with the rarity of a term across documents. However, a strict document frequency threshold is a simpler concept — it\u0026rsquo;s a cut-off point, beyond which a term may be considered too common to be useful for distinguishing relevant documents. In contrast, IDF is a continuously varying score that decreases as the document frequency of a term increases. How the Relevance Scoring is calcualted? # Documents in the Index # Document 1: \u0026ldquo;The best coffee shop in town.\u0026rdquo; Document 2: \u0026ldquo;A new coffee shop has opened.\u0026rdquo; Document 3: \u0026ldquo;I love visiting the local bakery and coffee house.\u0026rdquo; Search Query # Query: \u0026ldquo;coffee shop\u0026rdquo; Steps in Relevance Scoring # Query Analysis:\nThe query \u0026ldquo;coffee shop\u0026rdquo; is analyzed and broken down into terms: [\u0026ldquo;coffee\u0026rdquo;, \u0026ldquo;shop\u0026rdquo;]. Term Frequency (TF):\nDocument 1: Both \u0026ldquo;coffee\u0026rdquo; and \u0026ldquo;shop\u0026rdquo; appear once. Document 2: Both \u0026ldquo;coffee\u0026rdquo; and \u0026ldquo;shop\u0026rdquo; appear once. Document 3: \u0026ldquo;coffee\u0026rdquo; appears once, but \u0026ldquo;shop\u0026rdquo; does not appear. Inverse Document Frequency (IDF):\nIf \u0026ldquo;coffee\u0026rdquo; and \u0026ldquo;shop\u0026rdquo; are common in the entire document set, their IDF is lower. If they are rare, their IDF is higher. Assume both terms have a moderate IDF value. Field-Length Norm:\nShorter fields are given more weight. Let\u0026rsquo;s assume all documents have similar lengths for simplicity. Calculating Relevance Score:\nDocument 1 and 2: High relevance because they contain both \u0026ldquo;coffee\u0026rdquo; and \u0026ldquo;shop\u0026rdquo;. Document 3: Lower relevance as it only contains \u0026ldquo;coffee\u0026rdquo;. Example Relevance Score Calculation # Score Calculation (simplified formula):\nScore = TF(term1) * IDF(term1) + TF(term2) * IDF(term2) + \u0026hellip; For Document 1 and 2:\nHigher score because both terms match and their combined TF-IDF score is higher. For Document 3:\nLower score because only one of the terms (\u0026ldquo;coffee\u0026rdquo;) matches. Conclusion # Documents containing both terms of the query (\u0026ldquo;coffee\u0026rdquo; and \u0026ldquo;shop\u0026rdquo;) receive higher scores. The exact scoring involves more complex mathematical formulas and additional factors like term proximity, but this example provides a basic understanding. Note # This is a simplified explanation. The actual scoring algorithm in Elasticsearch is more complex and includes various other factors and optimizations. Elasticsearch\u0026rsquo;s relevance scoring is highly configurable and can be adjusted to suit specific use case requirements. "},{"id":120,"href":"/docs/data-store-and-management/search-engine/elasticsearch/query-types/","title":"Query Types","section":"Elasticsearch","content":" All query types in elastic search # Full-Text Search Queries # Match Query: Searches text fields for matches on a given query string. It\u0026rsquo;s the most common type and handles full-text search, including analyzing the query string.\nPhrase Query: Looks for a specific sequence of terms in a specified order. Useful for searching exact phrases.\nQuery String Query: Supports a compact, expressive syntax for specifying complex search criteria, including boolean logic, wildcards, fuzzy matches, and more.\nMulti-Match Query: Similar to the match query but allows for searching across multiple fields.\nSimple Query String Query: A simpler, more robust version of the Query String Query.\nMore Like This Query: Finds documents that are \u0026ldquo;like\u0026rdquo; a given document or a specified text.\nNon-Full-Text Search Queries # Term Query: Searches for the exact term in the field specified. It does not analyze the query string.\nRange Query: Finds documents where the specified field falls within a specified range. Useful for numerical, date, and other range-based queries.\nBoolean Query: Combines multiple queries using boolean logic. Includes must (AND), must_not (NOT), should (OR), and filter clauses. A logical query that combines other queries (which can be full-text or non-full-text).\nPrefix Query: Finds documents containing terms that start with a specified prefix.\nWildcard Query: Supports searching with wildcard characters (* for zero or more characters, ? for a single character).\nFuzzy Query: Returns documents that contain terms similar to the search term, allowing for typos and misspellings.\nRegexp Query: Enables searching with regular expressions.\nGeo Queries: Used for geospatial search, such as finding documents within a certain distance from a geographic point.\nScript Query: Allows custom scripts to specify custom query logic.\nNested Query: Used for searching nested objects (documents within documents).\nSome query use examples # In a platform like Twitter, which involves searching through vast amounts of text-based data (tweets, user names, hashtags, etc.), various types of Elasticsearch queries would be employed to cater to different search functionalities. Here are some hypothetical examples of how Twitter might structure its queries:\nBasic Text Search:\nObjective: A user wants to find tweets containing a specific word or phrase. Elasticsearch Query: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;tweet_text\u0026#34;: \u0026#34;Elasticsearch\u0026#34; } } } Explanation: A match query is used to search the tweet_text field for the term \u0026ldquo;Elasticsearch.\u0026rdquo; Hashtag Search:\nObjective: Searching for tweets containing a specific hashtag. Elasticsearch Query: { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;hashtags\u0026#34;: \u0026#34;#Elasticsearch\u0026#34; } } } Explanation: A term query is ideal for exact matches, like searching for a specific hashtag. Combining Text and User Search:\nObjective: Finding tweets by a specific user containing certain words. Elasticsearch Query: { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;tweet_text\u0026#34;: \u0026#34;database\u0026#34; } }, { \u0026#34;term\u0026#34;: { \u0026#34;user_name\u0026#34;: \u0026#34;tech_guru\u0026#34; } } ] } } } Explanation: A bool query combines a match query for text and a term query for the user\u0026rsquo;s handle. Searching with Date Ranges:\nObjective: Finding tweets within a specific date range. Elasticsearch Query: { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;post_date\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2024-01-01\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;2024-01-31\u0026#34; } } } } Explanation: A range query is used to find tweets posted in January 2024. Complex Query for Trend Analysis:\nObjective: Finding popular tweets about a topic within a date range. Elasticsearch Query: { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;tweet_text\u0026#34;: \u0026#34;new product\u0026#34; } }, { \u0026#34;range\u0026#34;: { \u0026#34;post_date\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2024-03-01\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;2024-03-18\u0026#34; } } } ], \u0026#34;filter\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;is_retweet\u0026#34;: false } }, { \u0026#34;range\u0026#34;: { \u0026#34;likes\u0026#34;: { \u0026#34;gte\u0026#34;: 100 } } } ] } } } Explanation: This uses a bool query to find original (non-retweeted) tweets about a \u0026ldquo;new product\u0026rdquo; from a specific date range that have received a certain number of likes, indicating popularity. These examples demonstrate how Elasticsearch\u0026rsquo;s diverse query capabilities can be utilized in a social media context like Twitter to perform efficient and accurate text searches, ranging from simple keyword searches to more complex queries involving multiple fields and criteria.\n"},{"id":121,"href":"/docs/distributed-systems/","title":"Distributed Systems","section":"Docs","content":"Distributed System\n"},{"id":122,"href":"/docs/distributed-systems/algorithms/","title":"Algorithms","section":"Distributed Systems","content":"Distributed System Algorithms\n"},{"id":123,"href":"/docs/distributed-systems/algorithms/mapreduce/","title":"Mapreduce","section":"Algorithms","content":" Question: Where is the data stored for Mapreduce? # In a typical MapReduce framework, the large datasets are stored in a distributed file system, such as the Hadoop Distributed File System (HDFS). Here\u0026rsquo;s how the storage and data distribution process works:\nStorage and Data Distribution in MapReduce: # Distributed File System:\nLarge datasets are stored in a distributed file system (e.g., HDFS). The data is divided into blocks (e.g., 128 MB each) and distributed across multiple nodes in the cluster. These nodes are often called DataNodes in HDFS. Replication:\nTo ensure fault tolerance and reliability, each block of data is typically replicated across multiple nodes (e.g., each block might be stored on three different nodes). Map Phase:\nWhen a MapReduce job is initiated, the framework splits the job into tasks. Each task is assigned to a map node, and the input data for each task is a block of data stored in the distributed file system. The framework tries to schedule tasks on nodes where the data blocks are already stored, minimizing data transfer across the network. This concept is known as data locality. Workflow: # Data Storage:\nData is initially ingested into the distributed file system. Example: A dataset is divided into blocks and stored on nodes A, B, and C in the cluster, with each block replicated across multiple nodes. Job Initialization:\nA MapReduce job is started. The job is divided into multiple map tasks, each responsible for processing a block of data. Data Locality:\nThe framework schedules map tasks on nodes where the data is already located to minimize network traffic. Example: If a data block is stored on nodes A, B, and C, a map task for that block will be scheduled on one of these nodes if possible. Data Transfer:\nIf it is not possible to schedule a task on a node with the local data (e.g., due to resource constraints), the data will be transferred over the network to the node where the map task is running. Example Scenario: # Data Storage:\nLarge dataset (e.g., a 1 TB file) is stored in HDFS. The dataset is split into 128 MB blocks, resulting in approximately 8,000 blocks. Each block is replicated three times across different nodes. MapReduce Job:\nA MapReduce job is submitted to process the dataset. The job is divided into 8,000 map tasks, each responsible for one block of data. Task Scheduling:\nMap task 1 is scheduled on node A, which already has a copy of block 1. Map task 2 is scheduled on node B, which has a copy of block 2, and so on. Data Processing:\nEach map task processes its local block of data, generating intermediate key-value pairs. If a map task is scheduled on a node without the local data, the necessary block is transferred from the distributed file system to that node. Key Points: # Data is stored in a distributed file system (e.g., HDFS), not directly on map nodes. The data is divided into blocks and distributed across multiple nodes for fault tolerance and efficiency. Map tasks are scheduled to run on nodes where the data blocks are located to take advantage of data locality and minimize network transfer. This approach ensures efficient processing of large datasets by leveraging the distributed storage and computation capabilities of the cluster.\nQuestion: What protocol is used in Mapreduce data transmission? # TCP is used in several parts of the MapReduce framework, particularly during the following phases:\nData Input and Output: When reading input data from and writing output data to distributed storage systems like HDFS (Hadoop Distributed File System), TCP is used to reliably transfer data between the storage nodes and the MapReduce nodes.\nShuffle and Sort Phase: During the shuffle and sort phase, intermediate data produced by the map tasks needs to be transferred to the appropriate reduce tasks. This phase involves significant data transfer between nodes, and TCP ensures that this data is reliably delivered.\nInter-Node Communication: Throughout the execution of a MapReduce job, various nodes (e.g., DataNodes, TaskTrackers/NodeManagers) need to communicate with each other. This inter-node communication, which includes heartbeats, status updates, and task assignments, relies on TCP for reliable transmission.\nTo summarize, TCP is used extensively for reliable data transmission during the shuffle and sort phase, data input/output operations, and inter-node communications within the MapReduce framework.\nQuestion: What is the comprehensive data flow in MapReduce job? # Data Storage in HDFS: Data is stored in multiple blocks across a distributed file system like HDFS.\nStarting a MapReduce Job:\nThe JobTracker/ResourceManager assigns map tasks to various DataNodes where the data blocks are located. This minimizes data transfer by moving computation to where the data resides (data locality). Map tasks read the input data blocks from HDFS using TCP. Map Phase:\nEach map task processes the data and produces intermediate key-value pairs. These intermediate results are often stored temporarily on the local disk of the map node. Shuffle and Sort Phase:\nAfter all map tasks complete, the intermediate data is transferred to the nodes responsible for the reduce tasks. This involves moving data across the network using TCP. The data is sorted by key during this phase to group all values associated with the same key together. Reduce Phase:\nReduce tasks receive the sorted intermediate data, process it, and produce the final output. The final output of the reduce tasks is written to HDFS, again using TCP for reliable data transfer. Final Output Storage:\nThe final results from the reduce tasks are stored back into HDFS or another distributed file system. To summarize, here\u0026rsquo;s the refined process:\nData blocks stored in HDFS. Map tasks read data blocks from HDFS (using TCP). Map tasks produce intermediate data and store it on local disk. Intermediate data is transferred to reduce nodes during the shuffle and sort phase (using TCP). Reduce tasks process the intermediate data and write the final results to HDFS (using TCP). It\u0026rsquo;s important to note that the shuffle and sort phase is not a separate \u0026ldquo;system\u0026rdquo; but rather part of the overall MapReduce process, specifically handled by the MapReduce framework to ensure data is properly sorted and grouped before reaching the reduce phase.\nQuestion: Is there any centralized component in a MapReduce system? # The MapReduce framework itself is not centralized; it operates in a distributed manner, coordinating various tasks across a cluster of nodes. Here\u0026rsquo;s how it works:\nJobTracker/ResourceManager:\nIn the original Hadoop MapReduce (often referred to as MRv1), a central component called the JobTracker is responsible for managing the job and tracking the progress of each task. In the newer Hadoop YARN (Yet Another Resource Negotiator) architecture, which is part of Hadoop 2.x and later (often referred to as MRv2), the ResourceManager is the central authority that manages resources and schedules jobs. TaskTrackers/NodeManagers:\nIn MRv1, TaskTrackers are distributed across nodes in the cluster. They execute the map and reduce tasks as assigned by the JobTracker. In YARN, NodeManagers run on each node and are responsible for launching and monitoring containers that execute the map and reduce tasks. MapReduce Job Execution:\nWhen a MapReduce job is submitted, the JobTracker/ResourceManager breaks it down into smaller tasks (map tasks and reduce tasks). These tasks are distributed across the cluster to the nodes where the data is stored (data locality principle). The map tasks process the input data and produce intermediate key-value pairs. The shuffle and sort phase involves transferring intermediate data across the network to the nodes running the reduce tasks. Reduce tasks process the intermediate data and produce the final output. Fault Tolerance:\nThe framework handles node failures by reassigning tasks to other nodes and ensures that the job completes successfully. To summarize, while there are centralized components (JobTracker in MRv1 or ResourceManager in YARN) responsible for coordinating the overall job execution and resource management, the actual processing is distributed across the cluster. The MapReduce framework orchestrates this distributed processing to ensure efficient and reliable job completion.\nDoes the ResourceManager in Hadoop support the master-slave mechanism? # The ResourceManager in Hadoop YARN (Yet Another Resource Negotiator) is indeed a centralized component, but it operates within a master-slave architecture to provide high availability and fault tolerance. Here’s a detailed breakdown:\nResourceManager (RM):\nThe ResourceManager is the central authority responsible for resource allocation and job scheduling in a YARN cluster. It consists of several components, including the Scheduler and the ApplicationManager. High Availability:\nTo avoid a single point of failure, YARN can be configured for high availability (HA). In an HA setup, there are typically two ResourceManagers: an active ResourceManager and a standby ResourceManager. The active RM handles all the resource management and job scheduling tasks, while the standby RM remains synchronized and can take over in case the active RM fails. NodeManagers (NMs):\nNodeManagers are distributed across the cluster, running on each node. They are responsible for launching and managing containers on the nodes, monitoring resource usage, and reporting back to the ResourceManager. ApplicationMaster (AM):\nFor each application submitted to the YARN cluster, an ApplicationMaster is launched. The ApplicationMaster negotiates resources with the ResourceManager and works with the NodeManagers to execute and monitor tasks. Each application has its own ApplicationMaster, which provides a level of decentralization for task management. Here\u0026rsquo;s how it works in practice:\nJob Submission:\nWhen a job is submitted, the ResourceManager allocates a container for the ApplicationMaster of that job. The ApplicationMaster then negotiates further resources with the ResourceManager to execute the map and reduce tasks. Resource Allocation:\nThe ResourceManager’s Scheduler allocates resources based on various policies (e.g., capacity, fairness). NodeManagers communicate with the ResourceManager to report resource availability and task statuses. Task Execution:\nThe ApplicationMaster requests containers from the ResourceManager to run the tasks. NodeManagers launch the containers and manage their execution. Fault Tolerance:\nThe ResourceManager ensures high availability with an active-standby configuration. The ApplicationMaster can handle task failures by requesting new containers and re-executing failed tasks. In summary, while the ResourceManager is a centralized component in YARN, it operates within a master-slave architecture with mechanisms for high availability and decentralization of task management through ApplicationMasters and NodeManagers.\nQuestion: Will the network throughput be the significant bottleneck in the MapReduce job? # Network throughput can indeed be a significant bottleneck in a MapReduce job, especially during the shuffle and sort phase where large volumes of intermediate data are transferred between nodes. To mitigate this, several strategies are employed:\nData Locality:\nMapReduce frameworks try to schedule map tasks on the nodes where the data is stored or as close to those nodes as possible. This minimizes the need to transfer large amounts of input data over the network. Intermediate Data Storage:\nIntermediate data generated by map tasks is often stored locally on the map node\u0026rsquo;s disk. This local storage reduces the amount of data that needs to be transferred over the network initially. Cluster Topology Awareness:\nThe framework is aware of the cluster topology, such as the network bandwidth between racks and nodes. This awareness helps in optimizing data transfer during the shuffle and sort phase by grouping and scheduling tasks to minimize cross-rack network traffic. Combiner Function:\nA combiner function can be used to perform local aggregation of intermediate data on the map node before sending it over the network. This reduces the volume of data that needs to be shuffled. Regarding your question about Google setting GFS (Google File System) and MapReduce workers on the same physical machines:\nYes, Google’s original MapReduce implementation indeed places both GFS (or later Colossus, the successor to GFS) and MapReduce workers on the same physical machines. This co-location is done to:\nImprove Data Locality:\nBy placing data storage (GFS) and data processing (MapReduce workers) on the same machines, the framework can leverage data locality to reduce network traffic. Map tasks can read input data directly from the local disk without needing to transfer it over the network. Optimize Resource Utilization:\nThis setup ensures better utilization of the cluster\u0026rsquo;s resources. Machines are used for both storage and computation, balancing the workload and avoiding the need for a separate storage cluster and a separate compute cluster. Reduce Latency:\nCo-locating storage and compute reduces the latency associated with data access, as the data is readily available on the same machine or at least within the same rack, minimizing the distance it needs to travel. Is the data locality a major consideration nowadays? # Google\u0026rsquo;s design for its distributed systems, including GFS (Google File System) and MapReduce, has evolved over time. However, the principle of co-locating storage and compute resources has remained a key strategy for optimizing data locality and minimizing network bottlenecks.\nIn the original MapReduce and GFS setup, co-locating the distributed file system and MapReduce workers on the same physical machines was a fundamental design choice. This approach improved performance by leveraging data locality, reducing the need to transfer large amounts of data over the network.\nWith advancements and the development of new systems like Colossus (the successor to GFS) and other distributed computing frameworks, Google has continued to optimize these principles. Some of the improvements and advancements include:\nColossus (GFS successor):\nColossus provides improvements in scalability, performance, and reliability over the original GFS. It is designed to handle even larger datasets and more complex workloads. Colossus still emphasizes data locality to reduce network traffic, so co-locating storage and compute remains a beneficial practice. Borg and Kubernetes:\nBorg, Google’s cluster management system, and its successor Kubernetes, also emphasize efficient resource management and scheduling. These systems manage both storage and compute resources across large clusters. Kubernetes, widely adopted outside of Google, provides mechanisms for co-locating pods (containers) with data, emphasizing locality and reducing network overhead. Cloud Infrastructure:\nIn cloud environments like Google Cloud Platform (GCP), services such as Google Cloud Storage (GCS) and Google BigQuery provide highly optimized data storage and processing capabilities. While these services may separate storage and compute physically, they are interconnected with high-speed, low-latency networks to minimize the performance impact. Data locality and efficient data transfer remain critical considerations. In summary, while the exact implementations and technologies have evolved, the principle of optimizing data locality by co-locating storage and compute resources continues to be a key strategy for enhancing performance and reducing network bottlenecks. Google’s infrastructure improvements have built on these principles, offering more advanced and scalable solutions for handling distributed data processing tasks.\nWhy does MapReduce use batch processing instead of stream processing in the reduce phase, allowing reduce nodes to avoid waiting for all map nodes to finish processing? # MapReduce, as a programming model, is inherently designed around batch processing. This design choice affects the reduce phase and how it handles data. Here are the main reasons why MapReduce typically doesn\u0026rsquo;t use stream processing in the reduce phase:\nSynchronization and Completeness:\nThe reduce phase needs to ensure that all intermediate data generated by the map phase is available before it begins. This requirement ensures that each reducer receives a complete set of data for each key it is responsible for. If reducers started processing data as it arrived (i.e., stream processing), there is a risk of incomplete data, leading to incorrect results. Data Shuffling:\nIn MapReduce, there is a critical step called the shuffle and sort phase that occurs between the map and reduce phases. During this step, the framework sorts and groups the intermediate data by key. Stream processing would complicate this step because it would require continuous sorting and grouping as data arrives, which is much more complex and less efficient compared to batch processing. Fault Tolerance:\nMapReduce is designed to handle faults gracefully. If a map task fails, it can be re-executed without affecting the reduce tasks since reduce tasks only start after all map tasks are completed. This batch processing approach simplifies fault tolerance because the system can ensure that all necessary data is available and correct before reducers start processing. Resource Optimization:\nBy waiting until all map tasks are complete before starting the reduce phase, the system can optimize resource allocation. Resources can be dynamically allocated to map tasks first, and once they complete, those resources can be reassigned to reduce tasks. This staged approach can lead to better utilization of cluster resources. Implementation Simplicity:\nThe original MapReduce model was designed with simplicity in mind. Batch processing in the reduce phase simplifies the implementation and logic of the MapReduce framework. Streaming data processing would require a more complex implementation to handle continuous data flow, synchronization issues, and potential partial results. While MapReduce uses batch processing in the reduce phase for the reasons mentioned above, there are other frameworks and models designed specifically for stream processing, such as Apache Storm, Apache Flink, and Kafka Streams, which can handle continuous data streams and provide real-time processing capabilities. These frameworks address the challenges of stream processing and are better suited for scenarios where real-time data processing is required.\n"},{"id":124,"href":"/docs/distributed-systems/algorithms/paxos/","title":"Paxos","section":"Algorithms","content":"TBD\n"},{"id":125,"href":"/docs/distributed-systems/algorithms/raft/","title":"Raft","section":"Algorithms","content":"TBD\n"},{"id":126,"href":"/docs/distributed-systems/algorithms/zab/","title":"Zab","section":"Algorithms","content":"TBD\n"},{"id":127,"href":"/docs/distributed-systems/commit-protocols/three-phase-commit/","title":"Three Phase Commit","section":"Distributed Systems","content":"TBD\n"},{"id":128,"href":"/docs/distributed-systems/commit-protocols/two-phase-commit/","title":"Two Phase Commit","section":"Distributed Systems","content":"TBD\n"},{"id":129,"href":"/docs/distributed-systems/systems/","title":"Systems","section":"Distributed Systems","content":"Common Systems\n"},{"id":130,"href":"/docs/distributed-systems/systems/hadoop/","title":"Hadoop","section":"Systems","content":"TBD\n"},{"id":131,"href":"/docs/distributed-systems/systems/kafka/","title":"Kafka","section":"Systems","content":"TBD\n"},{"id":132,"href":"/docs/distributed-systems/systems/rabbitmq/","title":"Rabbitmq","section":"Systems","content":" RabbitMQ plugin # [E*]: This indicates that the plugin is enabled. The uppercase E signifies that the plugin is currently active and functioning in your RabbitMQ instance.\n[e*]: The lowercase e typically indicates that the plugin is enabled, but it\u0026rsquo;s either a dependency of another plugin or it was enabled as a result of enabling another plugin. In other words, it might not have been explicitly enabled by the user, but it\u0026rsquo;s required for the functioning of another plugin that has been enabled.\n[ ]: No letter inside the brackets usually means that the plugin is not enabled.\nDeploying RabbitMQ on AWS # If you want to deploy a RabbitMQ server on AWS, please create a route53 record pointing to the RabbitMQ instance directly. If you use an Application Load Balancer and tried to redirect requests throught it, it will raise incompatible protocol issue, because ALB only supports HTTP/HTTPS.\n"},{"id":133,"href":"/docs/distributed-systems/systems/spark/","title":"Spark","section":"Systems","content":"TBD\n"},{"id":134,"href":"/docs/programming-basic/","title":"Programming Basic","section":"Docs","content":"Distributed System\n"},{"id":135,"href":"/docs/programming-basic/programming-concepts/algorithms/","title":"Algorithms","section":"Programming Basic","content":"Programming Algorithm\n"},{"id":136,"href":"/docs/programming-basic/programming-concepts/algorithms/advanced-algorithms/","title":"Advanced Algorithms","section":"Algorithms","content":"Programming Algorithm\n"},{"id":137,"href":"/docs/programming-basic/programming-concepts/algorithms/advanced-algorithms/bloomfilter/","title":"Bloomfilter","section":"Advanced Algorithms","content":" Bridging the Efficiency Gap with Bloom Filters: A Deep Dive # Introduction # In today\u0026rsquo;s data-centric world, every millisecond spent searching and verifying can equate to tangible losses. For industries managing vast data sets, ensuring the efficiency of lookup operations is paramount. Enter the Bloom filter - an ingenious space-efficient probabilistic data structure designed to tackle this very challenge. While many in the computer science realm are aware of its existence, a detailed understanding of its practical applications and opportunities for optimization can make a world of difference.\nThe Essence of Bloom Filters # A Bloom filter is essentially a data structure that allows us to test if an element is a member of a set. The primary advantages? Speed and space. Traditional hash tables may guarantee accurate membership testing but consume significantly more memory. Bloom filters, in contrast, allow for a small possibility of false positives but offer a guarantee against false negatives. In essence, it\u0026rsquo;s a trade-off.\nHow does it work? # Imagine an m-bit array initialized with zeros and a set of k hash functions. When an item is added, it is processed by these hash functions, which then determine which bits to set to 1. To check if an item is in the set, it\u0026rsquo;s again processed by the hash functions. If all corresponding bits are 1, the item might be in the set. If any bit is 0, it\u0026rsquo;s definitely not.\nReal-World Industry Applications # Web Browsers (e.g., Chrome, Firefox): They employ Bloom filters to check URLs against a list of malicious sites. This first check with a Bloom filter helps in reducing the expensive operation of a full database lookup.\nBig Data (e.g., Apache HBase, Cassandra): They use Bloom filters to avoid unnecessary disk lookups. When searching for data that doesn’t exist, a Bloom filter can swiftly indicate its absence, hence saving time.\nNetwork Routers: Bloom filters assist in ensuring packet routes are free of loops. They can quickly check if a packet has visited a router before, reducing redundant operations.\nPotential Improvements \u0026amp; Optimization # Tuning for Desired Error Rate: The false positive rate can be reduced by increasing the size of the bit array or using more hash functions. This tuning can be optimized based on the specific requirements of an application.\nCounting Bloom Filters: A variant that allows deletion by maintaining a count of additions and deletions. This is invaluable for dynamic datasets.\nScalable Bloom Filters: For unpredictable datasets, scalable Bloom filters can grow in size, ensuring the false positive rate remains bounded.\nCombining with Traditional Databases: Using Bloom filters as a front-end check can significantly reduce expensive disk or network operations, especially in distributed databases.\nConclusion # Bloom filters, with their unique blend of efficiency and probabilistic membership checking, have cemented their utility in modern computing infrastructures. By understanding their nuances, professionals can leverage them to build faster, more efficient systems. In the ever-evolving world of computer science, it\u0026rsquo;s not just about knowing the tools at our disposal, but mastering their intricacies and potential avenues for innovation.\n"},{"id":138,"href":"/docs/programming-basic/programming-concepts/algorithms/advanced-algorithms/cuckoo-filter/","title":"Cuckoo Filter","section":"Advanced Algorithms","content":"TODO\n"},{"id":139,"href":"/docs/programming-basic/programming-concepts/algorithms/dynamic-programming/","title":"Dynamic Programming","section":"Algorithms","content":"Dynamic Programming\n"},{"id":140,"href":"/docs/programming-basic/programming-concepts/algorithms/searching-algorithms/","title":"Searching Algorithms","section":"Algorithms","content":"Searching Algorithm\n"},{"id":141,"href":"/docs/programming-basic/programming-concepts/algorithms/sorting-algorithms/","title":"Sorting Algorithms","section":"Algorithms","content":"Sorting Algorithm\n"},{"id":142,"href":"/docs/programming-basic/programming-concepts/design-patterns/","title":"Design Patterns","section":"Programming Basic","content":"Design Patterns\n"},{"id":143,"href":"/docs/programming-basic/programming-concepts/design-patterns/behavioral-patterns/","title":"Behavioral Patterns","section":"Design Patterns","content":"Behavioral Patterns\n"},{"id":144,"href":"/docs/programming-basic/programming-concepts/design-patterns/creational-patterns/","title":"Creational Patterns","section":"Design Patterns","content":"Creational Patterns\n"},{"id":145,"href":"/docs/programming-basic/programming-concepts/design-patterns/structural-patterns/","title":"Structural Patterns","section":"Design Patterns","content":"Structural Patterns\n"},{"id":146,"href":"/docs/programming-basic/programming-concepts/programming-paradigms/","title":"Programming Paradigms","section":"Programming Basic","content":"Programming Paradigms\n"},{"id":147,"href":"/docs/programming-basic/programming-languages/","title":"Programming Languages","section":"Programming Basic","content":"Programming Languages\n"},{"id":148,"href":"/docs/programming-basic/programming-languages/go/","title":"Go","section":"Programming Languages","content":"Golang\n"},{"id":149,"href":"/docs/programming-basic/programming-languages/go/channel/","title":"Channel","section":"Go","content":"TODO\n"},{"id":150,"href":"/docs/programming-basic/programming-languages/go/common-issues/","title":"Common Issues","section":"Go","content":"Common issues in Golang\n"},{"id":151,"href":"/docs/programming-basic/programming-languages/go/common-issues/loop-variable-in-goroutine/","title":"Loop Variable in Goroutine","section":"Common Issues","content":" Summary of the Loop Variable Capturing Issue in Goroutines Closure # Problem Description # In Go, when using a for loop to iterate over elements and launching goroutines within the loop, a common issue arises due to the loop variable being reused across all iterations. Since the loop variable (req in our example) is allocated once and its memory address remains constant, all goroutines that capture this variable end up referencing the same memory location. As a result, they might all see the value of the loop variable from the last iteration, leading to unintended behavior.\nWhy It Happens # This problem occurs because the loop variable is effectively a reference that is updated with each iteration, but its address does not change. When a goroutine is launched, it captures the current state of the loop variable. However, if the goroutine does not execute immediately, it may capture the variable\u0026rsquo;s value after it has been updated by subsequent iterations, resulting in all goroutines accessing the same final value.\nSolution # To ensure that each goroutine gets the correct value corresponding to its iteration, the loop variable should be passed as an argument to the anonymous function (closure) inside the goroutine. This creates a new variable with the same value as the loop variable, ensuring that each goroutine operates on its own copy.\nExample # Incorrect Approach:\nfunc Serve(queue chan *Request) { for req := range queue { go func() { process(req) // All goroutines may see the same `req` }() } } Correct Approach:\nfunc Serve(queue chan *Request) { for req := range queue { go func(req *Request) { process(req) // Each goroutine gets its own `req` }(req) } } In the correct approach, req is passed as a parameter to the anonymous function, creating a distinct copy of the variable for each goroutine. This prevents all goroutines from referencing the same memory location and ensures that each one processes the correct request.\nBroader Context # This issue is not unique to Go; it can occur in other languages like JavaScript, Python, and Java when closures capture loop variables. The underlying principle is the reuse of a single memory location for the loop variable, which can lead to unexpected behavior in concurrent or asynchronous contexts. The solution typically involves creating a new binding for each iteration, either by passing the variable as an argument or using language-specific constructs that ensure unique instances.\n"},{"id":152,"href":"/docs/programming-basic/programming-languages/go/go-routine/","title":"Go Routine","section":"Go","content":"TODO\n"},{"id":153,"href":"/docs/programming-basic/programming-languages/go/go/","title":"Go","section":"Go","content":" Go Basic # xmapf, err := p.Lookup(\u0026quot;Map\u0026quot;) can I use the xmapf as a function directly? # In Go, when you use the plugin.Lookup method, it returns a plugin.Symbol, which is a type alias for interface{}. This means that the returned value is of type interface{} and doesn\u0026rsquo;t have a concrete type until you use a type assertion.\nYou cannot use xmapf and xreducef directly as functions because they are of type interface{}. To use them as functions, you need to perform a type assertion to convert them to the specific function type you expect.\nHere\u0026rsquo;s a detailed breakdown of why you need the type assertion:\nplugin.Lookup Returns interface{}:\nThe Lookup method of the plugin.Plugin type returns a plugin.Symbol, which is defined as interface{}. This means the returned value is of an empty interface type, which can hold any value but does not have a specific type. Type Assertion:\nTo use the returned value as a specific function type, you need to assert its type. The type assertion xmapf.(func(string, string) []mr.KeyValue) converts the interface{} value to a function with the signature func(string, string) []mr.KeyValue. Without this assertion, you cannot call xmapf as a function because it is still of type interface{}. []rune vs string in GO? # Strings in Go:\nDefinition: A string in Go is a sequence of bytes (uint8), and it is immutable. UTF-8 Encoding: Go strings are UTF-8 encoded by default, allowing them to represent any Unicode character, including Chinese characters and other non-ASCII characters. Length: The length of a string, as returned by len(s), is the number of bytes in the string, not the number of characters. Usage: Sequence of bytes, useful for storage and transmission. UTF-8 encoded strings can include multi-byte characters. Runes in Go:\nDefinition: A rune is an alias for int32 and is used to represent a Unicode code point. It emphasizes that the value represents a character. Usage: Runes are used when you need to work with individual characters of a string, especially when dealing with non-ASCII characters. Useful for text manipulation and processing. Key Points:\nString Length:\nThe length of a string (len(s)) gives the number of bytes. For UTF-8 encoded strings, non-ASCII characters (e.g., Chinese characters) use more than one byte. Rune Length:\nConverting a string to a slice of runes ([]rune(s)) allows you to work with Unicode code points. The length of a rune slice (len([]rune(s))) or the number of runes (utf8.RuneCountInString(s)) gives the number of characters. Type Assertion VS Type Conversion # Type assertion is necessary and specifically designed for extracting and working with concrete values stored in interfaces. Type conversion, on the other hand, is for converting known, compatible types to one another. The inability to use direct conversion with interface{} arises because, with interface{}, the underlying type isn\u0026rsquo;t known at compile time and needs to be determined at runtime, which is precisely what type assertions facilitate.\n####################### ### Type Conversion ### ####################### package main import \u0026#34;fmt\u0026#34; type Stringer interface { String() string // Print() string } type Printer interface { Print() string } type MyType struct { value string } func (m MyType) String() string { return m.value } func (m MyType) Print() string { return m.value } func main() { var s Stringer = MyType{\u0026#34;Hello, world!\u0026#34;} p := Printer(s) // Type conversion from Stringer to Printer fmt.Println(p.Print()) } ERROR: cannot convert s (variable of type Stringer) to type Printer: Stringer does not implement Printer (missing method Print) NOTE: It works when Stringer include all the methods defined in Printer(uncommen Print method and it works) ###################### ### Type Assertion ### ###################### package main import \u0026#34;fmt\u0026#34; type Stringer interface { String() string } type Printer interface { Print() string } type MyType struct { value string } func (m MyType) String() string { return m.value } func (m MyType) Print() string { return m.value } func main() { var s Stringer = MyType{\u0026#34;Hello, world!\u0026#34;} p := s.(Printer) // Type conversion from Stringer to Printer fmt.Println(p.Print()) } NOTE: It works well because the underlying struct implement Print method. Will raise error when remove Print method on MyType. Interface Satisfiability: In Go, a type satisfies an interface if it implements all the methods of the interface. This also means that one interface can be satisfied by another if the former includes all the methods of the latter. Type Conversion: When converting between interface types, the source interface must include all the methods required by the destination interface. If an interface A includes all methods of another interface B, any value that satisfies A can also satisfy B. Therefore, type conversion between these interfaces is allowed. package main import \u0026#34;fmt\u0026#34; type Stringer interface { String() string Print() string } type Printer interface { Print() string } type MyType struct { value string } func (m MyType) String() string { return m.value } func (m MyType) Print() string { return m.value } func main() { var s Stringer p := s.(Printer) // Type conversion from Stringer to Printer fmt.Printf(\u0026#34;%#v\u0026#34;, p) } ERROR: panic: interface conversion: interface is nil, not main.Printer s is a nil interface of type Stringer (it has not been assigned any concrete value that satisfies Stringer). The type assertion s.(Printer) attempts to assert that the dynamic type of s implements the Printer interface. However, since s is nil, it does not contain any concrete value, let alone one that implements Printer. This leads to a runtime panic with the error: interface conversion: interface is nil, not main.Printer. The reason is that Go\u0026rsquo;s type assertion mechanism checks if the actual concrete value inside the interface can be asserted to the target type. When nil, there\u0026rsquo;s no value to assert, causing a panic. package main import \u0026#34;fmt\u0026#34; type Stringer interface { String() string Print() string } type Printer interface { Print() string } type MyType struct { value string } func (m MyType) String() string { return m.value } func (m MyType) Print() string { return m.value } func main() { var s Stringer p := Printer(s) // Type conversion from Stringer to Printer fmt.Printf(\u0026#34;%#v\u0026#34;, p) } NOTE: Just output \u0026lt;nil\u0026gt; s is still a nil interface of type Stringer. The type conversion Printer(s) attempts to convert s to the Printer interface. Unlike type assertion, type conversion does not involve a runtime check for the dynamic type of the value inside the interface. Since both Stringer and Printer are interfaces and Go\u0026rsquo;s type system allows this conversion (because Stringer includes all the methods in Printer), the conversion is syntactically valid. The result is that p will also be a nil interface of type Printer. The output will simply show \u0026lt;nil\u0026gt;, indicating that p is a nil interface. Key Differences # Type Assertion:\nRuntime Check: Ensures the dynamic value inside the interface matches the target type. Panic on Nil: Panics if the value is nil or doesn\u0026rsquo;t implement the asserted type. Use Case: To access concrete type information or a specific interface implementation. Type Conversion:\nNo Runtime Check: Does not verify the dynamic type of the value; only checks the compatibility of the types. No Panic on Nil: Allows conversion as long as the source interface type includes all methods of the target interface type, even if the value is nil. Use Case: To change the static type of an interface or value without concern for the underlying concrete type, as long as the methods align. Method with Function Receiver # package main import \u0026#34;fmt\u0026#34; // Define a function type type MyFunc func(int, int) int // Define a method with a function receiver func (f MyFunc) Describe() { fmt.Println(\u0026#34;This is a method attached to a function type.\u0026#34;) } func (f MyFunc) Print(a int) { fmt.Println(a) } func (f MyFunc) Apply(a int, b int) { fmt.Println(f(a, b)) } func main() { // Define a function that matches the MyFunc type var add MyFunc = func(a, b int) int { return a + b } add.Describe() add.Print(1) add.Apply(2, 3) mul := MyFunc(func(a, b int) int { return a * b }) mul.Describe() mul.Print(1) mul.Apply(2, 3) } RESULT: This is a method attached to a function type. 1 5 This is a method attached to a function type. 1 6 Goroutine # Goroutines are functions that can be run concurrently with other functions. They are managed by the Go runtime, which multiplexes thousands of goroutines onto a smaller number of operating system threads. This management allows goroutines to be highly efficient, with much less overhead compared to traditional threads.\nThe Go runtime handles the scheduling and execution of goroutines, making them easy to use and manage. Unlike traditional threads, goroutines are not associated with a fixed system thread. The Go scheduler can move a goroutine between OS threads as needed, which is similar to how coroutines can be suspended and resumed. However, unlike coroutines that typically require explicit yielding, goroutines can be preempted by the Go scheduler, giving them behavior more akin to threads.\nIn summary, while goroutines share some similarities with coroutines in terms of lightweight and cooperative multitasking, they also have characteristics of threads, such as preemptive scheduling and potential concurrency on multiple CPU cores. This unique combination makes them a hybrid concept within Go\u0026rsquo;s concurrency model.\nType VS Kind # TODO:\nnil slice VS empty slice # In Go, explicitly initializing slices like pushStack and popStack to empty slices ([]int{}) is not strictly necessary because Go automatically initializes slices to their zero value, which is nil. You can omit the initialization if you prefer, as a nil slice behaves like an empty slice in most cases (for example, it can be appended to, and its length will be 0).\nIn Go, the behavior of nil slices is designed to avoid the common pitfalls associated with null pointers in languages like C. Here\u0026rsquo;s why a nil slice can be used without causing null pointer issues:\n1. Go\u0026rsquo;s zero value philosophy # Go emphasizes simplicity and safety by ensuring that all types have well-defined zero values, which are safe to use. For slices, the zero value is nil, and it behaves like an empty slice. This is part of Go\u0026rsquo;s design to minimize runtime errors and make code more predictable.\nIn C, NULL is not a valid pointer to an array, and dereferencing NULL causes undefined behavior or a crash. In Go, a nil slice has well-defined behavior:\nYou can call len() on a nil slice, and it will return 0. You can call cap() on a nil slice, and it will return 0. You can append to a nil slice, and it will automatically create a new underlying array. This makes it safe to use a nil slice without the need for explicit checks or initialization, unlike in C.\n2. Internal representation of slices # A Go slice is a structure with three components:\nPointer to the underlying array Length (number of elements) Capacity (size of the underlying array) A nil slice is simply a slice where the pointer is nil, and both length and capacity are 0. Importantly, this structure means you never directly work with raw memory or deal with unsafe operations like dereferencing a null pointer.\nHere\u0026rsquo;s what happens with a nil slice:\nvar s []int // s is a nil slice fmt.Println(len(s)) // prints 0 fmt.Println(cap(s)) // prints 0 s = append(s, 1) // works fine, appends 1 to the slice fmt.Println(s) // prints [1] 3. nil VS empty slice # If we think of a slice like this:\n[pointer] [length] [capacity] then:\nnil slice: [nil][0][0] empty slice: [addr][0][0] // it points to an address\nfunc main() { var nil_slice []int var empty_slice = []int{} fmt.Println(nil_slice == nil, len(nil_slice), cap(nil_slice)) fmt.Println(empty_slice == nil, len(empty_slice), cap(empty_slice)) } OUTPUT: true 0 0 false 0 0 nil pointers The behavior with nil pointers in Go is different from nil slices. While you can safely work with a nil slice as an empty slice, attempting to dereference or operate on a nil pointer without proper initialization will result in a runtime error, similar to what happens in C.\nvar p *int // p is a nil pointer fmt.Println(p) // Prints: \u0026lt;nil\u0026gt; fmt.Println(*p) // Causes panic: runtime error: invalid memory address or nil pointer dereference type MyQueue struct { pushStack *[]int popStack *[]int } func Constructor() MyQueue { myQueue := MyQueue{} myQueue.pushStack = \u0026amp;[]int{} return myQueue } func main() { test := Constructor() fmt.Println(test) fmt.Println(tt.popStack == nil) fmt.Println(tt.pushStack == nil) } OUTPUT: {0xc0000ba060 \u0026lt;nil\u0026gt;} true false Nil Slice Nil Slice Pointer The slice itself is nil. The pointer is nil. Can be operated on safely (e.g., append), as it behaves like an empty slice. Cannot be dereferenced or operated on without initialization (runtime panic if dereferenced while nil). Safe to use with len(), cap(), and append() even when nil. Requires initialization before dereferencing or operations. Common use case: when you want an empty or uninitialized slice. Common use case: when you want to pass a reference to a slice or allow nil-pointers explicitly. No assignment or operations allowed outside the functions in Golang # In Go, you can declare variables outside of functions, but there are restrictions on what operations you can perform at the package level (i.e., outside of functions or methods).\nWhy can you declare variables outside functions in Go? # You can declare variables at the package level because Go allows package-level variables to hold global or shared state across functions and methods. These variables are typically initialized with constant values or default zero values and can be accessed throughout the package. For example:\nvar globalVar int // This is allowed Why can\u0026rsquo;t you assign values or do operations outside functions? # In Go, package-level code is restricted to declarations and constant expressions. This means you can declare variables and initialize them with simple constant expressions (like literals) but not perform complex operations or assignments. The reason for this restriction is to keep package-level code simple, efficient, and predictable. Operations or computations are expected to be done within functions, ensuring a clear separation of initialization and logic.\nFor example, this will produce an error:\nvar x = 5 x = x + 10 // Error: cannot assign to x However, this is allowed:\nvar x = 5 // Declaration and initialization If you need to perform complex operations or assignments, you should do them inside functions, such as init() or main(). For example:\nvar globalVar int func init() { globalVar = 5 + 10 // Operations done inside the function } Conclusion # Go enforces these restrictions to prevent package-level code from becoming too complex or slow at compile-time, and to maintain clear, function-based execution of logic. This design helps Go maintain simplicity and performance, especially for large codebases.\nReference # https://stackoverflow.com/questions/50661263/why-cant-i-assign-values-to-global-variables-outside-a-function-in-c\ninit function in Golang # In Go, the init function is a special function that is automatically invoked by the Go runtime when a package is initialized. It is primarily used to set up initial states or perform setup tasks before the main program begins executing.\nHere are the key characteristics of the init function:\n1. No Arguments and No Return Values # The init function cannot take arguments and does not return any values. Its signature is simply:\nfunc init() { // initialization code } 2. Automatic Invocation # You do not call init() explicitly in your code. The Go runtime will automatically execute init() functions when the package is imported or when the program starts.\n3. Purpose of init # The init function is typically used to:\nInitialize global variables. Set up necessary configurations (e.g., reading configuration files). Initialize state or resources (e.g., establishing a database connection). Register components with package-level structures (e.g., registering handlers). 4. Multiple init Functions # You can define more than one init function within a single package, even across multiple files in the same package. The Go runtime will call each init function in the order they appear in the source files, which are executed after global variable declarations but before main().\n5. Order of Execution # Package-level variable declarations are evaluated first. Then, the init() function is executed. Finally, the main() function is executed after all init functions have been completed. Example Usage # package main import \u0026#34;fmt\u0026#34; var globalVar int func init() { globalVar = 10 fmt.Println(\u0026#34;Init function executed: globalVar =\u0026#34;, globalVar) } func main() { fmt.Println(\u0026#34;Main function executed\u0026#34;) } Output:\nInit function executed: globalVar = 10 Main function executed 6. Use Case: Package Initialization # When you import a package, the init function in that package is called automatically before your code begins. This is useful for initializing resources or performing setup without requiring the importing code to do anything explicitly.\nFor example, in a logging package, init() might configure default log settings, so when the package is imported, logging is ready to be used:\npackage log func init() { // Set up logging defaults fmt.Println(\u0026#34;Logging initialized\u0026#34;) } Important Notes: # init functions cannot be called explicitly from other parts of the code. They should be used carefully to avoid complex initialization logic, which can lead to difficult-to-trace errors or performance issues. The init function is most useful when you need to set up package-level state before any other part of the program runs.\n"},{"id":154,"href":"/docs/programming-basic/programming-languages/go/reflect/","title":"Reflect","section":"Go","content":"TODO\n"},{"id":155,"href":"/docs/programming-basic/programming-languages/late-static-binding/","title":"Late Static Binding","section":"Programming Languages","content":"Late static binding (LSB) is a feature primarily associated with dynamically typed languages like PHP. It allows static methods and properties to be resolved at runtime based on the calling class context, rather than the class in which they were defined.\nLanguages with Late Static Binding # PHP: PHP is the most well-known language that supports late static binding using the static keyword. Languages with Early Static Binding # Early static binding is more common in statically typed languages, where static methods and properties are resolved at compile-time based on the class in which they were defined. This is the behavior seen in Java.\nJava:\nIn Java, static methods and properties are resolved at compile-time based on the class where they are defined. C#:\nC# has similar behavior to Java regarding static methods and properties. They are resolved at compile-time based on the class where they are defined. C++:\nC++ also uses early static binding for static members. The resolution is based on the class in which they are defined. Swift:\nSwift resolves static members at compile-time based on the defining class. Kotlin:\nKotlin, like Java, resolves static members at compile-time. It uses companion objects to simulate static members, but the resolution is similar to early static binding. TypeScript:\nTypeScript, when compiled to JavaScript, follows JavaScript\u0026rsquo;s behavior for static members, which is early static binding. Summary # Languages with Late Static Binding:\nPHP: Uses static keyword to achieve late static binding. Languages with Early Static Binding:\nJava: Static members are resolved at compile-time based on the defining class. C#: Follows the same pattern as Java. C++: Uses early static binding for static members. Swift: Resolves static members at compile-time. Kotlin: Uses early static binding for static members. TypeScript: Follows JavaScript\u0026rsquo;s early static binding behavior. Late Static Binding in Dynamically Typed Languages # In general, late static binding is more naturally supported in dynamically typed languages where runtime behavior can be more flexible. Besides PHP, other dynamically typed languages like Python and Ruby can achieve similar dynamic behavior, but they do not have a direct equivalent of PHP\u0026rsquo;s static keyword.\nPython:\nPython doesn\u0026rsquo;t have late static binding as PHP, but it allows dynamic method resolution through its class and instance methods, and the use of self and cls. Ruby:\nRuby uses class methods and instance methods to achieve flexible method resolution, but it doesn\u0026rsquo;t have a direct equivalent to PHP\u0026rsquo;s late static binding. Conclusion # Late static binding is a feature primarily seen in PHP and is rare in other languages, especially statically typed ones. Most statically typed languages, like Java, C#, and C++, use early static binding where static members are resolved at compile-time based on the class where they are defined. Dynamically typed languages like Python and Ruby can achieve similar dynamic behavior but do not provide a direct equivalent to PHP\u0026rsquo;s late static binding.\n"},{"id":156,"href":"/docs/programming-basic/programming-thoughts/","title":"Programming Thoughts","section":"Programming Basic","content":"Programming Thoughts\n"},{"id":157,"href":"/docs/programming-basic/programming-thoughts/api-database-synchronization/","title":"API Database Synchronization","section":"Programming Thoughts","content":" Service Code and Database Schema Synchronization in Production Environment # To ensure that your microservices on AWS ECS and your database schema updates are synchronized effectively without causing service disruptions or inconsistencies, you can consider the following approaches:\n1. Implement a Blue-Green Deployment Strategy # Useful when the databases and code are not backward/forward compatible.\nPrepare a Green environment with the new code and the updated schema while the Blue environment continues to run the old code and schema.\nOnce the Green environment is fully tested and confirmed to be stable, switch traffic from the Blue to the Green environment.\nThe Blue environment can then be updated in a similar manner, ensuring that there is always a fallback available in case of issues.\nHow to Handle Incoming Requests During Transition # Drain Connections Gracefully: Before switching all traffic to the Green environment, start by gracefully draining existing connections from the Blue environment. This involves configuring the load balancer to stop directing new connections to the Blue environment while allowing existing connections to complete their processes.\nSynchronize State: If your application involves session data or other stateful information, ensure that such states are either:\nSynchronized between the Blue and Green environments before the switch, or Stored in a shared service that is not affected by the switch (e.g., external databases, in-memory data grids). Session Stickiness: Maintain session stickiness (persistent sessions) on the load balancer during the transition period. This ensures that users who are already connected to the Blue environment can continue their sessions uninterrupted until they naturally complete.\nData Handling and Synchronization # Data Replication: Set up data replication from the Blue environment to the Green environment. This can be done using database replication features, ensuring that any updates made in the Blue environment during the transition are mirrored in the Green environment\u0026rsquo;s database.\nDelayed Switch: In scenarios where complete immediate replication isn\u0026rsquo;t feasible, consider delaying the traffic switch until you have a low-traffic window or until you\u0026rsquo;re confident that most critical data has been replicated.\nWrite-Behind Caching: Implement write-behind caching strategies where writes are queued and synchronized with the database asynchronously. This can help manage data consistency across environments without impacting user experience.\nTransactional Integrity: Ensure that transactions are managed properly during the switch. This might mean temporarily halting certain operations during the critical switch phase or using distributed transaction protocols if applicable.\nAfter the Switch # Monitor the Old Environment: Keep the Blue environment running for a short period after the switch. Monitor for any stray requests or data anomalies and redirect these requests to the Green environment if necessary.\nFinal Data Sync: Perform a final synchronization of any residual data from the Blue to the Green environment after the initial traffic switch but before completely shutting down the Blue environment.\nArchiving and Backup: Before fully decommissioning the Blue environment, ensure that all data is backed up and that any useful logs or diagnostic information is archived for future analysis if needed.\nImplementation Tips # Automate as Much as Possible: Use scripts or automation tools to handle the steps involved in draining connections, synchronizing data, and switching traffic. Automation reduces the risk of human error and can speed up the process. Test Thoroughly: Before implementing this approach in a production environment, thoroughly test the process in a staging environment. Simulate various scenarios, including failure modes, to understand how the system behaves. By taking these steps, you can ensure that the transition from the Blue environment to the Green environment minimizes disruption, maintains data integrity, and provides a seamless experience to end users. This method requires careful planning and testing but is highly effective for zero-downtime deployments.\n2. Expand and Contract Method # Phase 1: Expand\nAdd new schema elements without removing or altering existing structures. This can include adding new tables, columns, or constraints that the new version of the code will use.\nDeploy the new version of the application that can operate with both the old and new schema. This version should be capable of determining which schema to interact with based on its availability and compatibility.\nThis approach ensures that if the new code encounters the old schema, it will still function correctly, albeit without utilizing the new features.\nPhase 2: Contract\nOnce the new application code is live and stable, remove deprecated schema elements that are no longer needed by any version of the application.\nThe cleanup should be done as a separate deployment to ensure that there is no reliance on the outdated schema elements by the new application.\nThis phase effectively contracts the schema to its new desired state, removing any temporary or transitional elements.\nReference: https://www.prisma.io/dataguide/types/relational/expand-and-contract-pattern # 3. Database versioning tool like Liquibase or Flyway # "},{"id":158,"href":"/docs/programming-basic/programming-thoughts/microservice/","title":"Microservice","section":"Programming Thoughts","content":" Monolithic VS Microservice # In a monolithic architecture, the components interact directly within a single application instance, often sharing data in-memory or through internal function calls.\nIn contrast, in a microservices architecture, different services are typically deployed on separate servers or environments and communicate through network calls, often using HTTP requests.\nScalability # Monolithic`: Scaling requires scaling the entire application, even if only one part of the application needs more resources. Microservice: Individual components can be scaled as needed, without affecting the entire application. Data Transfer: # Monolithic: In a monolithic application, components typically communicate with each other through function calls, method invocations, or shared memory. This internal communication is often straightforward since all components are part of the same codebase. Performance: Since communication is internal and direct, it tends to be faster with lower latency, which is beneficial for operations requiring quick data access and manipulation. Microservice: In a microservices architecture, services communicate with each other using well-defined APIs, often over a network. This communication is typically based on lightweight protocols such as HTTP/REST or message queues. Performance: Network communication introduces latency. Also, ensuring data consistency across services can be challenging and might require implementing patterns like Saga for distributed transactions. Data Transfer # API Calls (Synchronous Communication): Microservices communicate with each other using APIs, typically over HTTP/HTTPS. REST (Representational State Transfer) is a common choice for API design, but some systems might use other methods like GraphQL or gRPC. RESTful APIs: a request-response model where one service sends a request, and the receiver sends back a response. gRPC: especially in performance-critical systems, is using gRPC, which allows for efficient binary communication based on Protocol Buffers Communication is often stateless, especially in RESTful services. Each request from one service to another contains all the information needed to understand and process the request. Data Format The data transferred between services is typically in a lightweight, easy-to-parse format like JSON or XML. Asynchronous Communication To decouple services and enhance performance, microservices often communicate asynchronously. This means one service sends a message without waiting for an immediate response from the other service. Asynchronous communication can be implemented using message queues, event streams, or other messaging systems like Kafka, RabbitMQ, or AWS SQS. Message Queues: Event Streaming: Service Discovery In microservices, services need to discover each other dynamically. Service discovery mechanisms allow services to find and communicate with each other in a distributed environment. This can be done through service registries and service discovery tools. Load Balancing and Fault Tolerance Microservices architectures often implement load balancers to distribute requests efficiently across multiple instances of a service. They also use circuit breakers and other fault tolerance patterns to prevent failures in one service from cascading to others. API Gateway An API Gateway is often used as a single entry point for client requests, which are then routed to the appropriate microservice. It can handle cross-cutting concerns like authentication, SSL termination, and rate limiting. Circuit Breakers and Retry Mechanisms To maintain stability, microservices architectures often implement patterns like circuit breakers (using tools like Hystrix). This prevents a failing service from causing a cascading failure across other services. Distributed Tracing and Monitoring With multiple services communicating, it\u0026rsquo;s important to have distributed tracing (like Zipkin or Jaeger) to track requests across services. Monitoring and logging (using tools like Prometheus or ELK stack) are also crucial to observe the health and performance of services. "},{"id":159,"href":"/docs/system-design/","title":"System Design","section":"Docs","content":" Section 1: Understand the scenario and scope # 1. essential features (functional requirements) # 2. quality attributes (non-functional requirements) # 3. limitations (constraints) # "},{"id":160,"href":"/docs/system-design/common-systems/","title":"Common Systems","section":"System Design","content":"Common Real-World Systems\n"},{"id":161,"href":"/docs/system-design/common-systems/amazon/","title":"Amazon","section":"Common Systems","content":"Online Commercial System\n"},{"id":162,"href":"/docs/system-design/common-systems/paypal/","title":"Paypal","section":"Common Systems","content":"Online Payment System\n"},{"id":163,"href":"/docs/system-design/common-systems/telegram/","title":"Telegram","section":"Common Systems","content":"Online Chating System\n"},{"id":164,"href":"/docs/system-design/common-systems/twitch/","title":"Twitch","section":"Common Systems","content":"Online Live Broadcast System\n"},{"id":165,"href":"/docs/system-design/common-systems/uber/1.requirement-clarification/","title":"1. Requirement Clarification","section":"Uber","content":" 1. Requirements Clarification # Functional Requirements: Matching riders with drivers Calculating the fare Handling payments Tracking the ride in real-time Rating system for drivers and riders Non-functional Requirements: High availability Scalability Low latency Real-time processing Data consistency Constraints: Handling millions of users and rides Supporting multiple platforms (iOS, Android, Web) A. essential features (functional requirements) # 1. User Registration and Profiles # Include features like photo upload, personal information, payment information, vehicle details for drivers, etc. Ensure secure handling of personal data, including password hashing and secure storage. 2. Maps Integration # Select a reliable and accurate mapping service (e.g., Google Maps, OpenStreetMap). 3. Ride Booking # What makes an Order: Pickup location, drop-off location, departure time, vehicle type, carpool or not, etc. Allow users to modify or cancel bookings. 4. Fare Calculation # A fare calculation system based on distance, time, demand, and type of service. Include dynamic pricing or surge pricing during high-demand periods. 5. Matching Algorithm # Develop an algorithm to efficiently match passengers with nearby drivers. Consider factors like location, vehicle type, driver rating, and user preferences. 6. Location Tracking # Use GPS for real-time location tracking of both drivers and passengers. Provide estimated time of arrival (ETA). 7. Notifications and Communications # Send notifications for ride status, driver details, changes in ETA, etc. Integrate in-app chat or call feature for driver-passenger communication without sharing personal numbers. 8. Payment Integration # Integrate a secure payment system for in-app payments. Support multiple payment methods like credit/debit cards, mobile wallets, and promo codes. 9. Rating and Feedback System # Allow passengers and drivers to rate each other and leave feedback. Use this data to realize incentive mechanisms. 10. Driver’s Interface # Include features for drivers to accept/decline ride requests, navigate to the destination, and manage earnings. Show requests and optimize routes for pick-up. 11. Emergency Calling System # Implement features like SOS buttons, ride tracking for trusted contacts, and driver background checks. B. quality attributes (non-functional requirements) # 1. Scalability # Horizontal Scaling Microservices Architecture\nUser Management Service Ride Matching Service Location Tracking Service Pricing and Fare Calculation Service Payment Processing Service Ride Booking Service Notification and Communication Service Ratings and Feedback Service Load Balancing\nDatabase Sharding\nStateless Design\nVertical Scaling Caching C. limitations (constraints) # "},{"id":166,"href":"/docs/system-design/common-systems/uber/","title":"Uber","section":"Common Systems","content":"Online Real-Time Transportation System\n"},{"id":167,"href":"/docs/system-design/common-systems/uber/outline/","title":"Outline","section":"Uber","content":" 1. Requirements Clarification # Functional Requirements: Matching riders with drivers Calculating the fare Handling payments Tracking the ride in real-time Rating system for drivers and riders Non-functional Requirements: High availability Scalability Low latency Real-time processing Data consistency Constraints: Handling millions of users and rides Supporting multiple platforms (iOS, Android, Web) 2. High-Level Architecture # Client Applications: Separate apps for riders and drivers. Web Servers: Handle API requests. Application Server: Business logic. Database Servers: Store data like user profiles, ride information, etc. Location Tracking Service: To track drivers and riders in real-time. Notification Service: For sending updates and alerts. 3. Core Components # User Management: Authentication, profiles, etc. Ride Matching Algorithm: Match drivers with nearby riders. Pricing Engine: Calculate ride fares based on distance, time, and demand. Payment Processing: Secure transaction handling. Geolocation Services: Track and update locations in real-time. Ride History: Record of all past rides. Rating and Review System: For feedback after each ride. 4. Database Design # SQL vs. NoSQL: Decisions based on data structure and scalability. Schema Design: Users, drivers, rides, payments, etc. Data Sharding: For scalability and faster access. Replication: For high availability and backup. 5. Scalability and Performance # Load Balancing: Distribute load across servers. Caching: For frequently accessed data. Database Optimization: Indexes, optimized queries. Asynchronous Processing: For tasks like sending notifications. Data Partitioning: To distribute data and load. 6. APIs and Integrations # RESTful APIs: For communication between front-end and back-end. External APIs: Maps, payment gateways, etc. Microservices Architecture: For managing different functionalities independently. 7. Security # Data Encryption: For sensitive data. Authentication and Authorization: OAuth, JWT. Secure Communication: HTTPS. 8. Monitoring and Logging # System Monitoring: Track performance metrics. Logging: Record system and user activities. Alerting System: For anomalies or system failures. 9. Disaster Recovery # Backup Systems: Regular data backups. Failover Mechanisms: Switch to backup systems in case of failure. 10. Future Enhancements # Scalability Plans: To handle growing users and data. Feature Additions: New features like ride scheduling, carpooling. Machine Learning: For demand prediction, dynamic pricing. Final Tips: # Focus on explaining your thought process and reasoning behind each decision. Be prepared to discuss trade-offs and potential improvements. Think about how each component impacts the overall user experience and system performance. "},{"id":168,"href":"/docs/system-design/design-methodologies/","title":"Design Methodologies","section":"System Design","content":"Design MethoDologies\n"},{"id":169,"href":"/docs/system-design/design-patterns/","title":"Design Patterns","section":"System Design","content":"Design Patterns\n"},{"id":170,"href":"/docs/system-design/design-principles/","title":"Design Principles","section":"System Design","content":"Design Principles\n"},{"id":171,"href":"/docs/system-design/ideas/","title":"Ideas","section":"System Design","content":"Novel Ideas\n"},{"id":172,"href":"/docs/systems-and-networking/networking/","title":"Networking","section":"Docs","content":"Networking\n"},{"id":173,"href":"/docs/systems-and-networking/networking/advanced-concepts/","title":"Advanced Concepts","section":"Networking","content":"Advanced Concepts in Networking\n"},{"id":174,"href":"/docs/systems-and-networking/networking/advanced-concepts/forward-routing/","title":"Forward Routing","section":"Advanced Concepts","content":" forwarding and routing # Routing Table # Prefix/Length Next Hop 18/8 171.69.245.10 Forwarding Table # Prefix/Length Interface MAC Address 18/8 if0 8:0:2b:e4:b:1:2 "},{"id":175,"href":"/docs/systems-and-networking/networking/advanced-concepts/sliding_window/","title":"Sliding Window","section":"Advanced Concepts","content":" Frame Header # typedef uint8_t SwpSeqno; typedef struct { SwpSeqno SeqNum; /* sequence number of this frame */ SwpSeqno AckNum; /* ack of received frame */ uint8_t Flags; /* up to 8 bits worth of flags */ } SwpHdr; Sliding Window Protocol State # typedef struct { /* sender side state: */ SwpSeqno LAR; /* seqno of last ACK received */ SwpSeqno LFS; /* last frame sent */ Semaphore sendWindowNotFull; SwpHdr hdr; /* pre-initialized header */ struct sendQ_slot { Event timeout; /* event associated with send-timeout */ Msg msg; } sendQ[SWS]; /* receiver side state: */ SwpSeqno NFE; /* seqno of next frame expected */ struct recvQ_slot { int received; /* is msg valid? */ Msg msg; } recvQ[RWS]; } SwpState; A semaphore sendWindowNotFull is a synchronization primitive that supports semWait and semSignal operations. Every invocation of semSignal increments the semaphore by 1, and every invocation of semWait decrements s by 1, with the calling process blocked (suspended) should decrementing the semaphore cause its value to become less than 0.\nHandle Messages # Receiver will destroy all the resending timers and events whose Sequence Number is lower(Not exactly correct, but advanced in a round) than ACK after receiving a ACK.\nSender will mark the message as received when it receives a new message, and then check if it receives all the messages in a line. If so, then send all the messages in the buffer to the higher protocol and remove all the cache. At last, send a message to the sender, including a ACK which stands for the highest Sequence Number that it has received without any vacant position in between.\nstatic int deliverSWP(SwpState state, Msg *frame) { SwpHdr hdr; char *hbuf; hbuf = msgStripHdr(frame, HLEN); load_swp_hdr(\u0026amp;hdr, hbuf) if (hdr-\u0026gt;Flags \u0026amp; FLAG_ACK_VALID) { /* received an acknowledgment—do SENDER side */ if (swpInWindow(hdr.AckNum, state-\u0026gt;LAR + 1, state-\u0026gt;LFS)) { do { struct sendQ_slot *slot; slot = \u0026amp;state-\u0026gt;sendQ[++state-\u0026gt;LAR % SWS]; evCancel(slot-\u0026gt;timeout); msgDestroy(\u0026amp;slot-\u0026gt;msg); semSignal(\u0026amp;state-\u0026gt;sendWindowNotFull); } while (state-\u0026gt;LAR != hdr.AckNum); } } if (hdr.Flags \u0026amp; FLAG_HAS_DATA) { struct recvQ_slot *slot; /* received data packet—do RECEIVER side */ slot = \u0026amp;state-\u0026gt;recvQ[hdr.SeqNum % RWS]; if (!swpInWindow(hdr.SeqNum, state-\u0026gt;NFE, state-\u0026gt;NFE + RWS - 1)) { /* drop the message */ return SUCCESS; } msgSaveCopy(\u0026amp;slot-\u0026gt;msg, frame); slot-\u0026gt;received = TRUE; if (hdr.SeqNum == state-\u0026gt;NFE) { Msg m; while (slot-\u0026gt;received) { deliver(HLP, \u0026amp;slot-\u0026gt;msg); msgDestroy(\u0026amp;slot-\u0026gt;msg); slot-\u0026gt;received = FALSE; slot = \u0026amp;state-\u0026gt;recvQ[++state-\u0026gt;NFE % RWS]; } /* send ACK: */ prepare_ack(\u0026amp;m, state-\u0026gt;NFE - 1); send(LINK, \u0026amp;m); msgDestroy(\u0026amp;m); } } return SUCCESS; } Three roles of the algorithm # The first role is the one we have been concentrating on in this section—to reliably deliver frames across an unreliable link.\nThe second role that the sliding window algorithm can serve is to preserve the order in which frames are transmitted.\nThe third role that the sliding window algorithm sometimes plays is to support flow control—a feedback mechanism by which the receiver is able to throttle the sender.\nAmbiguity ??? How to differentiate different frames with the same sequence number? # Due to the limitation of sequence number is 2*n - 1. What will happen if they need to use the sequence number from 0 again in the second round?\nReference: https://book.systemsapproach.org/direct/reliable.html # "},{"id":176,"href":"/docs/systems-and-networking/networking/advanced-concepts/webrtc/","title":"Web Rtc","section":"Advanced Concepts","content":" How WebRTC segment and assemble datagram over UDP? # WebRTC (Web Real-Time Communication) incorporates similar principles for handling real-time audio and video streams, but it is more sophisticated and includes additional mechanisms to ensure reliable and efficient communication. Here’s how WebRTC works and how it handles segmenting and reassembling using UDP:\nWebRTC is a protocol suite and set of APIs that enable real-time communication directly between web browsers or other endpoints without requiring an intermediary server for the media exchange. It uses a combination of UDP and TCP, along with several other protocols to ensure low latency, reliability, and security.\n"},{"id":177,"href":"/docs/systems-and-networking/networking/attacks/","title":"Attacks","section":"Networking","content":"Network Attacks\n"},{"id":178,"href":"/docs/systems-and-networking/networking/basic-concepts/0.0.0.0-127.0.0.1/","title":"0.0.0.0 127.0.0.1","section":"Basic Concepts","content":" The difference between 0.0.0.0 and 127.0.0.1 # 0.0.0.0: This is a non-routable meta-address used to designate an invalid, unknown, or non-applicable target. In the context of servers, 0.0.0.0 means \u0026ldquo;all IPv4 addresses on the local machine.\u0026rdquo; If a server is set to listen on 0.0.0.0, it will be reachable at any IPv4 address that the machine happens to have. This makes it useful for services that should be accessible from any network. For example, if your computer is connected to multiple networks (e.g., Ethernet and Wi-Fi), a service listening on 0.0.0.0 can accept connections from both.\n127.0.0.1: This is the loopback address for IPv4. It is used to establish an IP connection to the same machine or host. This address is commonly used for testing network software. When you connect to 127.0.0.1, you are effectively connecting to \u0026ldquo;yourself.\u0026rdquo; In the context of a server, listening on 127.0.0.1 means the service will only be reachable from the local machine. It is not accessible from other machines on the network.\nSo typically, will people set up a web server listening on a host rather than 0.0.0.0 or 127.0.0.1? # In practice, the choice of which IP address a web server should listen on—be it 0.0.0.0, 127.0.0.1, or a specific host IP—depends on the desired accessibility and security considerations for the web service. Here\u0026rsquo;s how these options are typically used:\nListening on 0.0.0.0: This is a common setup for servers intended to be accessible from any network interface on the server. By binding to 0.0.0.0, the server listens for incoming connections on all IPv4 addresses that the server possesses. This is particularly useful when the server is part of a larger network or is meant to be accessible from the internet. It\u0026rsquo;s a standard configuration for production web servers.\nListening on 127.0.0.1: Binding to the loopback address (127.0.0.1) is used when the service should only be accessible from the local machine itself. This setup is common for development purposes, where developers run a web server on their machines that they don\u0026rsquo;t want to expose to the external network. It\u0026rsquo;s also used for security reasons when a service should not be accessible from outside the host.\nListening on a Specific Host IP: In some scenarios, a server might be configured to listen on a specific IP address. This is used when you want to restrict the service to a particular network interface. For example, if a server has multiple network interfaces (e.g., one public and one private), you might configure the web server to listen only on the private network for internal access.\nListening on an IPv6 Address: With the growing adoption of IPv6, web servers might also listen on an IPv6 address, either in addition to or instead of IPv4 addresses.\nIn summary, the choice of listening address depends on the use case:\nUse 0.0.0.0 for general accessibility across all network interfaces. Use 127.0.0.1 for local testing or security reasons. Use a specific IP address for controlled access on a specific network interface. Consider IPv6 addresses for modern network compatibility. For most production environments where services are intended to be widely accessible, listening on 0.0.0.0 (or the appropriate IPv6 equivalent) is typically the preferred configuration. However, always consider security implications and configure firewalls and other protective measures appropriately.\n"},{"id":179,"href":"/docs/systems-and-networking/networking/basic-concepts/","title":"Basic Concepts","section":"Networking","content":"Basic Networking Concepts\n"},{"id":180,"href":"/docs/systems-and-networking/networking/basic-concepts/datagram/","title":"Datagram","section":"Basic Concepts","content":" The idea behind datagram # The idea behind datagrams is incredibly simple: You just include in every packet enough information to enable any switch to decide how to get it to its destination. That is, every packet contains the complete destination address.\nContactless # A host can send a packet anywhere at any time, since any packet that turns up at a switch can be immediately forwarded (assuming a correctly populated forwarding table). For this reason, datagram networks are often called connectionless\nVirtual Circuit Switching # Not popular today, just ignore it.\n"},{"id":181,"href":"/docs/systems-and-networking/networking/basic-concepts/hostname_domainname/","title":"Hostname Domainname","section":"Basic Concepts","content":"What’s the difference between a hostname, a domain name, and an FQDN? A hostname refers to a particular device on a network. So, in the URL www.mybusiness.com, “www” is the hostname.\nA domain name identifies the website. So, stick with the example website URL www.mybusiness.com. “Mybusiness” is the domain name. Multiple hostnames can be associated with a singular domain.\nThe FQDN is the hostname, domain name, and TLD (e.g., .com). So, “www.mybusiness.com” is the FQDN.\n"},{"id":182,"href":"/docs/systems-and-networking/networking/basic-concepts/ip/","title":"IP","section":"Basic Concepts","content":" IP Header # Version (4 bits): Specifies the IP version. For IPv4, this value is 4. HLEN (Header Length, 4 bits): Indicates the length of the header in 32-bit words. The minimum value is 5 (20 bytes), and the maximum is 15 (60 bytes). TOS (Type of Service, 8 bits): Used to specify the priority of the packet and request specific types of service (e.g., low delay, high throughput). Length (16 bits): Total length of the IP packet (header + data) in bytes. The maximum length is 65,535 bytes. Ident (Identification, 16 bits): Used for uniquely identifying fragments of an original IP packet. Flags (3 bits): Control or identify fragments. The three bits are: Reserved bit: Must be zero. Don\u0026rsquo;t Fragment (DF): If set, the packet should not be fragmented. More Fragments (MF): If set, there are more fragments following this one. Offset (Fragment Offset, 13 bits): Specifies the offset of a fragment relative to the beginning of the original unfragmented packet. Measured in 8-byte units. TTL (Time to Live, 8 bits): Indicates the maximum number of hops the packet can take before being discarded. Helps prevent infinite looping in the network. Protocol (8 bits): Specifies the protocol used in the data portion of the IP datagram (e.g., TCP, UDP, ICMP). Checksum (Header Checksum, 16 bits): Used for error-checking the header. It ensures the integrity of the header data. The Checksum is calculated by considering the entire IP header as a sequence of 16-bit words, adding them up using ones’ complement arithmetic, and taking the ones’ complement of the result. SourceAddr (Source Address, 32 bits): The IP address of the sender. DestinationAddr (Destination Address, 32 bits): The IP address of the intended recipient. Options (Variable length): Optional field that can be used for various purposes, such as security, routing, and network management. If not used, the header ends after the destination address. Pad (Variable length): Used to ensure that the header length is a multiple of 32 bits. Data: The actual payload being transported by the IP packet. This can be of variable length and contains the higher-level protocol data, such as TCP or UDP segments. IP Address # Network Part Identification: An IP address is typically divided into two parts: the network part and the host part. The division of these parts is determined by the subnet mask associated with the IP address. IP addresses that share the same network part, when masked with the subnet mask, belong to the same IP network or subnet.\nSame IP Network: If two devices have IP addresses with the same network part (and therefore are on the same subnet), they are generally considered to be on the same logical network. This means that they can potentially communicate directly with each other without the need for routing through an intermediate device.\nExceptions # Subnetting and VLANs: Modern network configurations often involve advanced subnetting and virtual LANs (VLANs). Even if two IP addresses appear to be on the same subnet based on their network address, network policies or configurations like VLAN segmentation might prevent them from communicating directly. In such cases, even though they share the network part of the IP address, they could be isolated at the Data Link layer. Datagram Forwarding in IP # Every IP datagram contains the IP address of the destination host.\nThe network part of an IP address uniquely identifies a single physical network that is part of the larger Internet.\nAll hosts and routers that share the same network part of their address are connected to the same physical network and can thus communicate with each other by sending frames over that network.\nEvery physical network that is part of the Internet has at least one router that, by definition, is also connected to at least one other physical network; this router can exchange packets with hosts or routers on either network.\nA datagram is sent from a source host to a destination host, possibly passing through several routers along the way. Any node, whether it is a host or a router, first tries to establish whether it is connected to the same physical network as the destination. To do this, it compares the network part of the destination address with the network part of the address of each of its network interfaces. (Hosts normally have only one interface, while routers normally have two or more, since they are typically connected to two or more networks.) If a match occurs, then that means that the destination lies on the same physical network as the interface, and the packet can be directly delivered over that network.\nif (NetworkNum of destination = NetworkNum of one of my interfaces) then deliver packet to destination over that interface else if (NetworkNum of destination is in my forwarding table) then deliver packet to NextHop router else deliver packet to default router Complete Forwarding table for Router R2 # NetworkNum NextHop 1 R1 2 Interface 1 3 Interface 2 4 R3 Routers now contain forwarding tables that list only a set of network numbers(e.g. 128.96) rather than all the nodes in the network.\nSubnetting and Classless Addressing # The original intent of IP addresses was that the network part would uniquely identify exactly one physical network.\nSubnetting provides a first step to reducing total number of network numbers that are assigned. The idea is to take a single IP network number and allocate the IP addresses with that network number to several physical networks, which are now referred to as subnets.\nSubnet Mask # The mechanism by which a single network number can be shared among multiple networks involves configuring all the nodes on each subnet with a subnet mask. With simple IP addresses, all hosts on the same network must have the same network number. The subnet mask enables us to introduce a subnet number; all hosts on the same physical network will have the same subnet number, which means that hosts may be on different physical networks but share a single network number.\nAll hosts on a given subnet are configured with the same mask; that is, there is exactly one subnet mask per subnet. New Route Table # The forwarding table of a router also changes slightly when we introduce subnetting. Recall that we previously had a forwarding table that consisted of entries of the form (NetworkNum, NextHop). To support subnetting, the table must now hold entries of the form (SubnetNumber, SubnetMask, NextHop). To find the right entry in the table, the router ANDs the packet’s destination address with the SubnetMaskfor each entry in turn; if the result matches the SubnetNumber of the entry, then this is the right entry to use, and it forwards the packet to the next hop router indicated.\nSubnetNumber SubnetMask NextHop 128.96.34.0 255.255.255.128 Interface 0 128.96.34.128 255.255.255.128 Interface 1 128.96.33.0 255.255.255.0 R2 D = destination IP address for each forwarding table entry (SubnetNumber, SubnetMask, NextHop) D1 = SubnetMask \u0026amp; D if D1 = SubnetNumber if NextHop is an interface deliver datagram directly to destination else deliver datagram to NextHop (a router) CIDR(Classless Interdomain Routing) # CIDR, tries to balance the desire to minimize the number of routes that a router needs to know against the need to hand out addresses efficiently. To do this, CIDR helps us to aggregate routes.(e.g. 16 * ClassC record VS 1 * ClassB record)\nCIDR requires a new type of notation to represent network numbers, or prefixes as they are known, because the prefixes can be of any length. The convention is to place a /X after the prefix, where X is the prefix length in bits. So, for the example above, the 20-bit prefix for all the networks 192.4.16 through 192.4.31 is represented as 192.4.16/20. By contrast, if we wanted to represent a single class C network number, which is 24 bits long, we would write it 192.4.16/24.\nCIDR means that prefixes may be of any length, from 2 to 32 bits. Furthermore, it is sometimes possible to have prefixes in the forwarding table that “overlap,” in the sense that some addresses may match more than one prefix. For example, we might find both 171.69 (a 16-bit prefix) and 171.69.10 (a 24-bit prefix) in the forwarding table of a single router. In this case, a packet destined to, say, 171.69.10.5 clearly matches both prefixes. The rule in this case is based on the principle of “longest match”; that is, the packet matches the longest prefix, which would be 171.69.10 in this example.\nARP(Address Translation) # A more general solution would be for each host to maintain a table of address pairs; that is, the table would map IP addresses into physical addresses.\nIf a host wants to send an IP datagram to a host (or router) that it knows to be on the same network(i.e., the sending and receiving nodes have the same IP network number), it first checks for a mapping in the cache. If no mapping is found, it needs to invoke the Address Resolution Protocol over the network. It does this by broadcasting an ARP query onto the network. This query contains the IP address in question (the target IP address). Each host receives the query and checks to see if it matches its IP address. If it does match, the host sends a response message that contains its link-layer address back to the originator of the query. The originator adds the information contained in this response to its ARP table.\nThe query message also includes the IP address and link-layer address of the sending host. Thus, when a host broadcasts a query message, each host on the network can learn the sender’s link-level and IP addresses and place that information in its ARP table. However, not every host adds this information to its ARP table. If the host already has an entry for that host in its table, it “refreshes” this entry; that is, it resets the length of time until it discards the entry. If that host is the target of the query, then it adds the information about the sender to its table, even if it did not already have an entry for that host. This is because there is a good chance that the source host is about to send it an application-level message, and it may eventually have to send a response or ACK back to the source; it will need the source’s physical address to do this. If a host is not the target and does not already have an entry for the source in its ARP table, then it does not add an entry for the source. This is because there is no reason to believe that this host will ever need the source’s link-level address; there is no need to clutter its ARP table with this information.\nDHCP(Dynamic Host Configuration Protocol) # Broadcast Address # Special IP address (255.255.255.255) that is an IP broadcast address. This means it will be received by all hosts and routers on that network. (Routers do not forward such packets onto other networks, preventing broadcast to the entire Internet.)\nDHCP Process # The DHCP operates based on a client-server model and involves four key steps, often referred to as DORA (Discover, Offer, Request, Acknowledge):\nDHCP Discover:\nThe client sends a DHCP Discover message, a broadcast packet (to all devices) on the network, to find available DHCP servers. This message is sent because the client needs to obtain the necessary network configuration parameters to operate in the network. DHCP Offer:\nAny DHCP server that receives the Discover message responds to the client with a DHCP Offer message. This message is also a broadcast and contains the IP address that the server is offering, the subnet mask, the duration of the lease (how long the client can use the IP address), and possibly other configuration details like the DNS server address and default gateway. DHCP Request:\nOnce the client receives one or more offers, it selects one and responds with a DHCP Request message. This message is sent back to the selected DHCP server to indicate acceptance of the offered settings and to inform the other DHCP servers that their offers are declined. This message includes the IP address it has chosen from the offers it received. DHCP Acknowledge:\nUpon receiving the DHCP Request message from the client, the DHCP server sends a DHCP Acknowledge message to the client. This message confirms that the IP address has been officially leased to the client. The server also configures its own data to mark the IP address as assigned. Additional Details # Lease Time: The DHCP server assigns a lease time for each IP address, which is the time period that the client can use the IP address without renewing it. Once the lease time expires, the client must request a new IP address or renew its current address. Renewal: Before the lease expires, the client typically begins the renewal process using a DHCP Request message directed specifically to the server that originally granted the IP address. If renewed successfully, the server responds with a DHCP Acknowledge. IP Address Reuse: DHCP allows for efficient use of IP addresses by reassigning them once they are no longer in use or when the lease has expired. Configuration Options: Besides IP addresses, DHCP can also configure clients with the necessary routing and DNS information required for fully functional network connectivity. DHCP is widely used in all types of networks because it reduces the administrative burden of assigning IP addresses manually and ensures that each device has all the necessary configuration details to communicate on the network effectively.\nError Reporting (ICMP) # When a router does not know how to forward the datagram or when one fragment of a datagram fails to arrive at the destination—it does not necessarily fail silently. IP is always configured with a companion protocol, known as the Internet Control Message Protocol (ICMP), that defines a collection of error messages that are sent back to the source host whenever a router or host is unable to process an IP datagram successfully.\nICMP also provides the basis for two widely used debugging tools, ping and traceroute. ping uses ICMP echo messages to determine if a node is reachable and alive. traceroute uses a slightly non-intuitive technique to determine the set of routers along the path to a destination.\nVirtual Networks and Tunnels # "},{"id":183,"href":"/docs/systems-and-networking/networking/basic-concepts/socket/","title":"Socket","section":"Basic Concepts","content":"It is important to keep two concerns separate in your mind. Each protocol provides a certain set of services, and the API provides a syntax by which those services can be invoked on a particular computer system. The implementation is then responsible for mapping the tangible set of operations and objects defined by the API onto the abstract set of services defined by the protocol. If you have done a good job of defining the interface, then it will be possible to use the syntax of the interface to invoke the services of many different protocols. Such generality was certainly a goal of the socket interface, although it’s far from perfect.\nThe first step is to create a socket, which is done with the following operation:\nint socket(int domain, int type, int protocol); The return value from socket is a handle for the newly created socket—that is, an identifier by which we can refer to the socket in the future. It is given as an argument to subsequent operations on this socket.\nThe next step depends on whether you are a client or a server. On a server machine, the application process performs a passive open—the server says that it is prepared to accept connections, but it does not actually establish a connection. The server does this by invoking the following three operations:\nint bind(int socket, struct sockaddr *address, int addr_len); int listen(int socket, int backlog); int accept(int socket, struct sockaddr *address, int *addr_len); On the client machine, the application process performs an active open; that is, it says who it wants to communicate with by invoking the following single operation:\nint connect(int socket, struct sockaddr *address, int addr_len); Once a connection is established, the application processes invoke the following two operations to send and receive data:\nint send(int socket, char *message, int msg_len, int flags); int recv(int socket, char *buffer, int buf_len, int flags); "},{"id":184,"href":"/docs/systems-and-networking/networking/basic-concepts/tcp/","title":"Tcp","section":"Basic Concepts","content":" Transport Level Protocol # The application-level processes that use its services have certain requirements. The following list itemizes some of the common properties that a transport protocol can be expected to provide:\nGuarantees message delivery\nDelivers messages in the same order they are sent\nDelivers at most one copy of each message\nSupports arbitrarily large messages\nSupports synchronization between the sender and the receiver\nAllows the receiver to apply flow control to the sender\nSupports multiple application processes on each host\nLower Level Protocol(best-effort service) # The underlying network upon which the transport protocol operates has certain limitations in the level of service it can provide. Some of the more typical limitations of the network are that it may\nDrop messages\nReorder messages\nDeliver duplicate copies of a given message\nLimit messages to some finite size\nDeliver messages after an arbitrarily long delay\nTCP’s demux key # (SrcPort, SrcIPAddr, DstPort, DstIPAddr)\nWhy TCP uses three-way handshake? # Because the client and server have to make sure the opposite side know what sequence number they are going to accept.\nSo, to realize this goal, we need two round packets(4 in total: 2 * (SYN + ACK)).\nHowever, we can combine the SYN and ACK in the second/third steps to realize three-way handshake.\nQUESTION: Why not 2/4/\u0026hellip; way handshake? Is this related to two generals question? # 2-way handshake is not enough.\nA two-way handshake would involve only a SYN and an ACK (which also carries the server’s SYN). While this might establish a connection, it lacks an explicit final acknowledgment from the client that it is ready and has received the server’s initial sequence number correctly. This could potentially lead to scenarios where the server starts sending data without confirmation that the client is properly prepared, leading to data loss or connection errors right at the start. 4+-way handshakes are redundant.\nExtending to a four-way handshake would involve additional messages beyond the SYN, SYN-ACK, and ACK. This could be used for further negotiations or settings (like in SSL/TLS protocols during HTTPS communications), but for TCP’s purpose of merely establishing a reliable connection, a four-way handshake would introduce unnecessary complexity and delay in establishing the connection. TCP\u0026rsquo;s three-way handshake strikes a balance between reliability and efficiency. Both of TWO-GENERAL-QUESTION and THREE-WAY HANDSHAKE involve the challenges of coordinating and ensuring reliable communication over an unreliable channel. While the Two Generals\u0026rsquo; Problem illustrates the need for reliable communication strategies in theory, it does not dictate the use of a three-way handshake specifically. The choice of a three-way handshake in TCP is more a product of practical engineering requirements—balancing reliability, speed, and simplicity—than a direct solution to the Two Generals\u0026rsquo; Problem. The TCP protocol design effectively addresses real-world network conditions and aims to establish a reliable and efficient connection with minimal overhead.\nQUESTION: Will there be multiple connection in TCP? # No.\nInitial Sequence Numbers (ISNs)\nTCP uses a sequence number in the SYN packet to uniquely identify the start of a new connection. Each new connection attempt from a client includes a unique ISN, which is chosen based on a time-based algorithm that makes it very unlikely to repeat recent sequence numbers.\nClient Response to Unexpected SYN-ACK\nIf a duplicate SYN packet (with the same ISN as the original) reaches the server after the original connection has been established, the server\u0026rsquo;s response (a SYN-ACK with what should be the next sequence number) will not align with the client\u0026rsquo;s expectations for a new connection (as the client would use a new ISN for a truly new connection).\nIf the client receives a SYN-ACK that does not match any active connection attempt (meaning the ACK number doesn\u0026rsquo;t match the ISN+1 of any SYN it sent), it will respond with a RST (reset) packet. This RST informs the server that the SYN-ACK it sent was unexpected and effectively closes that pseudo-connection from the server\u0026rsquo;s side, preventing resource waste and potential confusion.\nServer Handling of RST\nUpon receiving the RST, the server understands that the client did not initiate the connection it is acknowledging and can safely discard it, thus cleaning up any state or resources allocated to what it initially perceived as a valid connection.\nSliding Window in data link layer / transport layer # Data Link Layer: Uses sliding window protocols like Go-Back-N and Selective Repeat for reliable frame delivery between directly connected nodes.\nTransport Layer (TCP): Uses a sliding window mechanism to ensure reliable, ordered delivery of data segments over a network.\nSliding window in TCP serves several purposes:\n(1) it guarantees the reliable delivery of data, (2) it ensures that data is delivered in order, and (3) it enforces flow control between the sender and the receiver. TCP’s use of the sliding window algorithm is the same as at the link level in the case of the first two of these three functions.\nFlow Control # The only real difference is that this time we elaborated on the fact that the sending and receiving application processes are filling and emptying their local buffer, respectively.\nTCP use AdvertisedWindow in the header to realize FLOW CONTROL.\nTCP always sends a segment in response to a received data segment, and this response contains the latest values for the Acknowledge and AdvertisedWindow fields, even if these values have not changed since the last time they were sent.\nWhenever the other side advertises a window size of 0, the sending side persists in sending a segment with 1 byte of data every so often. It knows that this data will probably not be accepted, but it tries anyway, because each of these 1-byte segments triggers a response that contains the current advertised window. Eventually, one of these 1-byte probes triggers a response that reports a nonzero advertised window.\nNote that these 1-byte messages are called Zero Window Probes and in practice they are sent every 5 to 60 seconds. As for what single byte of data to send in the probe: it’s the next byte of actual data just outside the window. (It has to be real data in case it’s accepted by the receiver.)\nProtecting Against Wraparound # TCP’s SequenceNum field is 32 bits long -\u0026gt; 2^32 AdvertisedWindow field is 16 bits long -\u0026gt; 2^16\nThe 32-bit sequence number space is adequate at modest bandwidths, but given that OC-192 links are now common in the Internet backbone, and that most servers now come with 10Gig Ethernet (or 10 Gbps) interfaces, we’re now well-past the point where 32 bits is too small. Fortunately, the IETF has worked out an extension to TCP that effectively extends the sequence number space to protect against the sequence number wrapping around.\nTriggering Transmission # TCP maintains a variable, typically called the maximum segment size (MSS), and it sends a segment as soon as it has collected MSS bytes from the sending process. MSS is usually set to the size of the largest segment TCP can send without causing the local IP to fragment. That is, MSS is set to the maximum transmission unit (MTU) of the directly connected network, minus the size of the TCP and IP headers.\nSilly Window Syndrome # Receiver Side:\nWhen the receiving application reads data from the TCP buffer slowly, the receiver advertises small window sizes to the sender. As a result, the sender can only transmit small amounts of data, leading to inefficient use of the network.\nSender Side:\nWhen the sender generates data slowly or in small segments, it can result in sending small packets. This can happen if the sender doesn\u0026rsquo;t wait to accumulate a larger amount of data before transmitting, thus leading to a high number of small packets.\nNagle’s Algorithm # This algorithm is implemented on the sender side to prevent small packet transmissions. It works by combining a number of small outgoing messages and sending them all at once. Specifically, it prevents the sender from sending more than one small packet per round-trip time (RTT) and waits until it can send a full-sized segment or until all outstanding data has been acknowledged.\nSome applications cannot afford such a delay for each write it does to a TCP connection, the socket interface allows the application to turn off Nagle’s algorithm by setting the TCP_NODELAY option. Setting this option means that data is transmitted as soon as possible.\nAdaptive Retransmission # Adaptive Retransmission in TCP is a mechanism that adjusts the retransmission timeout dynamically based on the observed round-trip times. It involves calculating the smoothed RTT and RTT variance to determine an appropriate RTO, balancing timely retransmissions with network efficiency. This approach enhances TCP\u0026rsquo;s ability to handle varying network conditions, leading to improved reliability and performance.\nTCP Extensions # The first extension helps to improve TCP’s timeout mechanism. The second extension addresses the problem of TCP’s 32-bit SequenceNum field wrapping around too soon on a high-speed network. The third extension allows TCP to advertise a larger window, thereby allowing it to fill larger delay × bandwidth pipes that are made possible by high-speed networks. The fourth extension allows TCP to augment its cumulative acknowledgment with selective acknowledgments of any additional segments that have been received but aren’t contiguous with all previously received segments. Alternative Design Choices (SCTP, QUIC) # stream-oriented protocols like TCP and request/reply protocols like RPC. TCP is a full-duplex protocol\nNo Message Boundary\nSince TCP deals with a continuous stream of bytes, it does not provide any mechanism to distinguish where one message ends and another begins. It simply ensures that the bytes are delivered in the same order they were sent.\nMessage Orientation Protocol\nRequest/reply applications, such as HTTP or SMTP, operate with discrete messages. A message is a distinct unit of data that represents a complete request or response, often with clear boundaries (e.g., start and end markers).\nIn these applications, the concept of a \u0026ldquo;message\u0026rdquo; is defined at the application layer, not by the transport layer (TCP). The application layer protocol must handle the parsing and recognition of message boundaries within the byte stream provided by TCP.\nThe first complication is that TCP is a byte-oriented protocol rather than a message-oriented protocol, and request/reply applications always deal with messages.\nthe second complication is that in those situations where both the request message and the reply message fit in a single network packet, a well-designed request/reply protocol needs only two packets to implement the exchange, whereas TCP would need at least nine: three to establish the connection, two for the message exchange, and four to tear down the connection.\n"},{"id":185,"href":"/docs/systems-and-networking/networking/basic-concepts/udp/","title":"UDP","section":"Basic Concepts","content":" Basic Characteristics of UDP # Connectionless Protocol: UDP does not establish a connection before sending data. It sends data without ensuring that the receiver is ready or available to receive it, which contrasts sharply with TCP (Transmission Control Protocol), a connection-oriented protocol. No Guarantee of Delivery: UDP does not guarantee that data sent will reach its destination, as it does not track delivery. There is no acknowledgment mechanism that the data has been received. No Error Correction: UDP does not offer any error correction. If a packet is lost or arrives with errors, UDP will not attempt to resend it. Error handling (if any) must be implemented at the application level. No Order Guarantee: Data packets (datagrams) may arrive in a different order than they were sent. It\u0026rsquo;s up to the application to reorder them if necessary. Structure of a UDP Datagram # A UDP datagram consists of a header and a data section. The UDP header is very simple compared to the TCP header:\nSource Port (16 bits): The port number of the sending process. Destination Port (16 bits): The port number of the receiving process. Length (16 bits): The length of the UDP header and data. The minimum value is 8 bytes (the size of the header). Checksum (16 bits): Used for error-checking of the header and data. This field is optional in IPv4 (may be zero) but mandatory in IPv6. Advantages of UDP # Speed: Because it lacks the overhead of connection setup, acknowledgments, and other features of TCP, UDP is typically faster and more suitable for applications that need rapid transmission of data, such as video streaming or online gaming. Simple: The simplicity of UDP reduces the resource requirements of the application, both in terms of code complexity and system overhead. Real-time Applications: UDP is often used in real-time applications where timely delivery of data is more critical than perfect delivery. Drawbacks # Unreliable: The lack of delivery and order guarantees, and error correction can be problematic for applications requiring reliable data transmission. Security: UDP is susceptible to spoofing and DoS attacks because it does not verify the source of the datagrams. Conclusion # UDP is a fundamental protocol that offers simplicity and efficiency where the integrity and sequence of the data packets can be managed or tolerated at the application level. It is particularly useful for services where the speed of communication is more crucial than the precision of the data received.\nReal-world Scenarios for Using UDP # Video Streaming Services: For live video streaming, such as sports events or concerts, UDP is preferred because it allows for continuous data flow without the interruption that can occur with TCP’s requirement for acknowledgment and retransmission. This keeps the video playing smoothly even if some packets are lost.\nOnline Multiplayer Games: In fast-paced online games, the speed of updates is crucial. UDP allows game state updates (like player positions and actions) to be sent quickly and frequently. While some information might be lost, the next update can make the corrections, which is acceptable as slight inaccuracies are less critical than real-time interaction.\nVoice over IP (VoIP): Applications like Skype or Zoom may use UDP because it reduces latency in voice transmission. If packets are lost, they are usually not retransmitted, as doing so would cause delays that might make conversation difficult or unnatural.\nDNS Queries: UDP is used for DNS queries because they typically involve single request-response actions, and the data size is small. If a response is lost, the query can simply be resent, and using UDP avoids the overhead of setting up and tearing down a TCP connection.\nIoT Devices: Many IoT applications use UDP because these devices often operate on limited power and need to transmit small amounts of data without the overhead of establishing a connection.\nUnlike TCP, UDP is a message-oriented protocol # UDP operates with datagrams, which are discrete packets of data. Each datagram is a self-contained message with a defined beginning and end.\nUnlike TCP, which treats data as a continuous stream of bytes, UDP preserves message boundaries. Each datagram sent by the sender is received as a distinct, complete message by the receiver.\nUDP is connectionless, meaning that it does not establish or maintain a connection between the sender and receiver. Each datagram is sent independently, and there is no guarantee of delivery, order, or error correction.\nUDP does not segment data into smaller units or reassemble them. Each datagram must fit within the size limits imposed by the network (usually up to 65,535 bytes, including the UDP header).\nHandling Large Message # UDP (User Datagram Protocol) is designed to send discrete packets of data called datagrams, each of which must fit within the size limits imposed by the protocol and the underlying network infrastructure. The maximum size of a UDP datagram, including the UDP header, is 65,535 bytes. This includes:\nUDP Header: 8 bytes Payload: Up to 65,527 bytes Therefore, a single UDP datagram cannot exceed 65,535 bytes. If an application needs to send a message larger than this, it must split the message into smaller chunks, each fitting within the size limit, and send each chunk as a separate UDP datagram.\nWhen an application needs to send data larger than 65,535 bytes using UDP, it must implement its own mechanism to fragment and reassemble the data.\nSimple Video Streaming Example # Video Encoding(Sender) Fragmentation into Chunks Adding Sequence Numbers Creating UDP Datagrams Receiving UDP Datagrams(Receiver) Buffering Handling Loss and Out-of-Order Packets Reconstructing Frames Decoding and Playback "},{"id":186,"href":"/docs/systems-and-networking/networking/protocols/","title":"Protocols","section":"Networking","content":"Network Protocols\n"},{"id":187,"href":"/docs/systems-and-networking/networking/protocols/definition/","title":"Definition","section":"Protocols","content":" What is a protocol? # Protocol is the abstract objects that make up the layers of a network system are called protocols. That is, a protocol provides a communication service that higher-level objects (such as application processes, or perhaps higher-level protocols) use to exchange messages.\ntecture-layer.png)\nEach protocol defines two different interfaces. First, it defines a service interface to the other objects on the same computer that want to use its communication services. This service interface defines the operations that local objects can perform on the protocol. For example, a request/reply protocol would support operations by which an application can send and receive messages. An implementation of the HTTP protocol could support an operation to fetch a page of hypertext from a remote server. An application such as a web browser would invoke such an operation whenever the browser needs to obtain a new page (e.g., when the user clicks on a link in the currently displayed page).\nSecond, a protocol defines a peer interface to its counterpart (peer) on another machine. This second interface defines the form and meaning of messages exchanged between protocol peers to implement the communication service. This would determine the way in which a request/reply protocol on one machine communicates with its peer on another machine. In the case of HTTP, for example, the protocol specification defines in detail how a GET command is formatted, what arguments can be used with the command, and how a web server should respond when it receives such a command.\nIP is the core # Of these three attributes of the Internet architecture, the hourglass design philosophy is important enough to bear repeating. The hourglass’s narrow waist represents a minimal and carefully chosen set of global capabilities that allows both higher-level applications and lower-level communication technologies to coexist, share capabilities, and evolve rapidly. The narrow-waisted model is critical to the Internet’s ability to adapt to new user demands and changing technologies.\n"},{"id":188,"href":"/docs/systems-and-networking/networking/security/","title":"Security","section":"Networking","content":"Network Security\n"},{"id":189,"href":"/docs/systems-and-networking/operating-systems/","title":"Operating Systems","section":"Docs","content":"Operating Systems\n"},{"id":190,"href":"/docs/systems-and-networking/operating-systems/basic-concepts/","title":"Basic Concepts","section":"Operating Systems","content":"Operating System Basic Concepts\n"},{"id":191,"href":"/docs/systems-and-networking/operating-systems/basic-concepts/clock-tick/","title":"Clock Tick","section":"Basic Concepts","content":" How a timer is implemented in Operating System? # Implementing a timer under the hood in an operating system or application involves several key components and mechanisms. Here\u0026rsquo;s an overview of how timers work:\n1. Timer Creation and Storage # When a timer is created, it typically specifies a duration (e.g., 5 seconds) and an action to be executed when the timer expires. The system stores this timer in a data structure, often sorted by the time when each timer is set to expire. Common data structures used include: Priority queues or min-heaps, where the root always contains the timer with the closest expiration time. Sorted linked lists, where timers are inserted in chronological order of expiration. 2. Handling Timer Expiration # Periodic System Tick: Most operating systems rely on a periodic interrupt, often called a \u0026ldquo;system tick\u0026rdquo; or \u0026ldquo;clock tick.\u0026rdquo; This is a hardware interrupt that occurs at a fixed interval, such as every millisecond.\nWhen the system tick occurs, the operating system updates its internal timekeeping. The system then checks the timer data structure to see if any timers have expired. If so, it triggers the associated actions. Efficient Checking: The system does not need to iterate through all timers each time the system tick occurs. Instead, because the timers are sorted by expiration time (in a min-heap or similar structure), the system often only needs to check the first timer in the structure.\nIf the next timer is not yet due to expire, the system knows that subsequent timers (which have later expiration times) also haven\u0026rsquo;t expired. 3. Signal and Event Triggering # When a timer expires, the system typically signals this event by:\nSending a signal to the process or thread that created the timer. Enqueuing an event in an event queue that the process or system can handle. Direct execution of a callback function, depending on the system\u0026rsquo;s design. Software Interrupts: Some systems may use software interrupts or signals to notify the operating system that the timer has expired, which can then be handled immediately or scheduled for execution.\n4. Optimization and Timer Coalescing # Timer Coalescing: To reduce the overhead of managing a large number of timers, modern operating systems often coalesce timers that are set to expire around the same time. This means that multiple timers expiring close together can be handled in a single system tick, reducing the need to wake the CPU multiple times.\nLazy Evaluation: Some systems might not check timers on every tick but instead rely on a lower frequency of checks, or they might use a combination of system ticks and events from other sources (like I/O operations) to determine when to check timers.\nSummary # The system generally relies on a periodic clock tick to check if timers have expired, using efficient data structures to minimize the overhead of this checking. When a timer expires, the system triggers an event, signal, or callback to handle the expiration.\nThis mechanism ensures that the system can handle timers with a relatively low overhead while still being responsive to timer events as they occur.\nDoes the timer in modern programming languages count on Operating System clock tick? # Yes, in most modern programming languages like Go, Java, and others, the timers you use are generally based on the underlying system\u0026rsquo;s clock tick mechanism. Here’s how it works:\n1. System Clock Tick as the Basis # Underlying Mechanism: When you set a timer in languages like Go or Java, the language runtime typically relies on the operating system\u0026rsquo;s clock tick for timing accuracy. The timer you set (e.g., using time.Sleep in Go or java.util.Timer in Java) is managed by the language runtime, which in turn depends on the system\u0026rsquo;s periodic clock tick to check if the timer has expired.\nGranularity: The granularity of these timers is often tied to the clock tick resolution, meaning the smallest interval a timer can accurately measure is usually equal to or greater than the clock tick interval (e.g., 1 millisecond).\n2. High-Level Language Implementations # Go:\nIn Go, timers are managed by the Go runtime. When you create a timer using time.NewTimer or time.AfterFunc, the runtime uses the system’s clock tick to periodically check if the timer’s duration has elapsed. The Go runtime might also coalesce multiple timers to optimize performance, checking timers less frequently when it’s known that no timers are near expiration. Java:\nJava’s java.util.Timer or the ScheduledExecutorService also rely on the system clock. These utilities internally manage a list or priority queue of tasks that are scheduled to run at specific times, with the system clock being used to determine when each task should execute. Java\u0026rsquo;s timer utilities also work with the system clock tick, meaning they are subject to the same limitations in terms of resolution. 3. High-Resolution Timers # High-Resolution APIs: Some languages and environments offer high-resolution timer APIs that can measure time intervals smaller than the system clock tick. For example, in C or C++, you might use clock_gettime with CLOCK_MONOTONIC for high-resolution timing. Advanced Uses: However, for most general-purpose uses in languages like Go or Java, standard timers are sufficient and are inherently tied to the system\u0026rsquo;s clock tick mechanism. 4. Custom Implementations # Custom Timing Mechanisms: If you require more precise timing than the system clock tick allows, you might implement custom timing mechanisms using busy-wait loops, high-resolution timers, or hardware timers. These methods, however, come with their own trade-offs, such as increased CPU usage or the need for more complex code. Conclusion # Yes, the timers provided by modern programming languages like Go and Java generally depend on the system clock tick for their operation. The clock tick serves as the foundation for measuring time intervals, and most timers in these languages are designed to work with the granularity provided by the system’s clock tick. High-resolution timing is possible but typically requires specialized APIs or custom implementations.\n"},{"id":192,"href":"/docs/systems-and-networking/operating-systems/basic-concepts/coroutine/","title":"Coroutine","section":"Basic Concepts","content":" Processes # Separate instances of programs. Independent execution units Heavyweight, more memory and overhead. Isolated memory space. Require IPC for communication. Good for high isolation, parallelism. Threads # Units of execution within a process. Preemptive multitasking — operating system’s scheduler determines when a thread should be paused and resumed Lighter weight, less overhead. Share memory space. Direct communication. Good for concurrency and shared data. Coroutines # Cooperative units within a thread. Cooperative multitasking — they decide when to yield control back to the scheduler or event loop voluntarily. Very lightweight. Share thread’s memory space. Efficient suspension and resumption. Good for asynchronous tasks, I/O-bound operations. Coroutines vs Threads # — Concurrency Model:\nCoroutines: Coroutines are cooperative in nature, meaning they decide when to yield control back to the scheduler or event loop voluntarily. They explicitly define points at which they can be paused and resumed using constructs like await (in languages like Python) or similar keywords. Threads: Threads are preemptive, which means that the operating system’s scheduler determines when a thread should be paused and resumed. Threads can be interrupted at any time, and the scheduler switches between threads based on a pre-defined time slice (time-sharing). — Blocking vs. Non-blocking:\nCoroutines: Coroutines are generally non-blocking by design. When a coroutine encounters a blocking operation (e.g., I/O), it yields control back to the event loop, allowing other coroutines to execute in the meantime. This means coroutines can efficiently handle I/O-bound tasks without creating many threads. Threads: Threads can block due to various reasons, such as waiting for I/O, synchronization primitives, or other resource locks. Blocking threads can lead to inefficient resource utilization if not managed carefully. — Context Switching:\nCoroutines: Context switching between coroutines is typically less expensive than context switching between threads. This is because coroutines are explicitly designed to be paused and resumed at defined points, and switching between them often involves less overhead. Threads: Context switching between threads can be more expensive due to the need to save and restore the entire thread’s execution context, including its stack and registers. — Parallelism:\nCoroutines: Coroutines don’t inherently provide parallelism, as they often run within a single thread. However, they can still achieve concurrency by interleaving the execution of multiple tasks. Threads: Threads can achieve true parallelism when executed on multi-core processors, as multiple threads can run simultaneously on different cores. This makes threads suitable for CPU-bound tasks that can be divided into parallel subtasks. — Resource Consumption:\nCoroutines: Coroutines generally consume fewer system resources compared to threads, as they can be managed within a single thread. This makes them more suitable for scenarios with a large number of concurrent tasks. Threads: Threads consume more resources due to the overhead of maintaining separate stacks and execution contexts for each thread. "},{"id":193,"href":"/docs/systems-and-networking/operating-systems/basic-concepts/process/","title":"Process","section":"Basic Concepts","content":"TODO\n"},{"id":194,"href":"/docs/systems-and-networking/operating-systems/basic-concepts/thread/","title":"Thread","section":"Basic Concepts","content":" Thread Scheduling VS Process Scheduling # "},{"id":195,"href":"/docs/systems-and-networking/operating-systems/concurrent-programming/","title":"Concurrent Programming","section":"Operating Systems","content":"Concurrent Programming\n"},{"id":196,"href":"/docs/systems-and-networking/operating-systems/questions/","title":"Questions","section":"Operating Systems","content":"Questions in Operating Systems\n"},{"id":197,"href":"/docs/systems-and-networking/operating-systems/questions/use-macro/","title":"Use Macro","section":"Questions","content":" Why do the Linux use macro over function? # int __sys_socket(int family, int type, int protocol) { struct socket *sock; int flags; sock = __sys_socket_create(family, type, update_socket_protocol(family, type, protocol)); if (IS_ERR(sock)) return PTR_ERR(sock); flags = type \u0026amp; ~SOCK_TYPE_MASK; if (SOCK_NONBLOCK != O_NONBLOCK \u0026amp;\u0026amp; (flags \u0026amp; SOCK_NONBLOCK)) flags = (flags \u0026amp; ~SOCK_NONBLOCK) | O_NONBLOCK; return sock_map_fd(sock, flags \u0026amp; (O_CLOEXEC | O_NONBLOCK)); } SYSCALL_DEFINE3(socket, int, family, int, type, int, protocol) { return __sys_socket(family, type, protocol); } The use of macros instead of functions in this code is primarily for performance and code size reasons. Let\u0026rsquo;s break down the reasons for using macros in this specific context:\nPerformance: Macros are expanded inline at compile time, meaning there is no overhead of function call, parameter passing, and return. In performance-critical code paths, avoiding function calls can lead to better performance.\nCode Size: By using macros, the code size can be reduced because there is no need to generate function prologue and epilogue for each call. This can be significant in embedded systems or environments where code size is a concern.\nSimplicity and Clarity: In this specific case, the macro SYSCALL_DEFINE3 is likely a macro provided by the Linux kernel for defining system call handlers. It simplifies the process of defining system calls and makes the code more readable by encapsulating the necessary boilerplate code.\nIntegration with Kernel Environment: The use of macros might be dictated by the coding conventions of the Linux kernel. The Linux kernel has its own set of conventions and best practices, and macros are often used in such environments for various purposes, including performance optimization and code readability.\nStatic Analysis and Optimization: Macros can sometimes enable certain optimizations that might not be possible with function calls. The compiler can perform more aggressive optimizations when dealing with macros, especially if they are simple and don\u0026rsquo;t involve complex logic.\nIn summary, the use of macros in this code snippet is likely a deliberate choice made for performance, code size, simplicity, and adherence to kernel coding conventions.\n"},{"id":198,"href":"/docs/systems-and-networking/operating-systems/system-calls/","title":"System Calls","section":"Operating Systems","content":"System Call\n"},{"id":199,"href":"/docs/systems-and-networking/operating-systems/virtualization/","title":"Virtualization","section":"Operating Systems","content":"Virtualization in OS\n"},{"id":200,"href":"/docs/systems-and-networking/shell-and-commandline-tools/","title":"Shell and Commandline Tools","section":"Docs","content":"Shell Commands\n"},{"id":201,"href":"/docs/systems-and-networking/shell-and-commandline-tools/cron-jobs/","title":"Cron Jobs","section":"Shell and Commandline Tools","content":"Cron Jobs\n"},{"id":202,"href":"/docs/systems-and-networking/shell-and-commandline-tools/networking-commands-and-tools/","title":"Networking Commands and Tools","section":"Shell and Commandline Tools","content":"Network Related Commands\n"},{"id":203,"href":"/docs/systems-and-networking/shell-and-commandline-tools/networking-commands-and-tools/curl/","title":"Curl","section":"Networking Commands and Tools","content":" How to send HTTPS request with curl? # To send an HTTPS request using Curl, pass the destination endpoint that supports SSL connections on the Curl command line. Curl will automatically establish an SSL connection with the server. When Curl sends a request to an HTTPS URL, it checks the SSL certificate against the certificate store of the local CA. Curl returns the error message Certificate Verify Failed for expired and self-signed certificates. You can bypass certificate checking by passing -k or \u0026ndash;insecure to Curl. Click Run to execute the Curl HTTPS request online and see the results.\nReference: # https://reqbin.com/req/c-lfozgltr/curl-https-request "},{"id":204,"href":"/docs/systems-and-networking/shell-and-commandline-tools/package-management-tools/","title":"Package Management Tools","section":"Shell and Commandline Tools","content":"Package Related Commands\n"},{"id":205,"href":"/docs/systems-and-networking/shell-and-commandline-tools/shell-basic-commands/","title":"Shell Basic Commands","section":"Shell and Commandline Tools","content":"Basic Shell Commands\n"},{"id":206,"href":"/docs/systems-and-networking/shell-and-commandline-tools/shell-basic-commands/commands/","title":"Commands","section":"Shell Basic Commands","content":" Here String # diff \u0026lt;(echo \u0026#34;string1\u0026#34;) \u0026lt;(echo \u0026#34;string2\u0026#34;) \u0026lt;(echo \u0026quot;string\u0026quot;) construct creates a temporary file-like object that contains the string.\nSo in this way, we can walk around the file as input requirement of diff command.\nChecking the exit status of ANY command in a pipeline # It\u0026rsquo;s a pretty common thing in a shell script to want to check the exit status of the previous command. You can do this with the $? variable, as is widely known:\nCommand X echo $?\nWhat gets difficult is when you execute a pipeline:\nWhat you get is the result of the tee command, which writes the results to the display as well as to the results.txt file.\nCommand X | tee result.txt echo $?(This will return the result of tee result.txt)\nTo find out what grep returned, $? is of no use.\nInstead, use the ${PIPESTATUS[]} array variable. ${PIPESTATUS[0]} tells us what grep returned, while ${PIPESTATUS[1]} tells us what tee returned.\nCommand X | tee result.txt echo ${PIPESTATUS[0]} (This will return the result of Command X)\nOR\nCommand X | tee result.txt RC=(${PIPESTATUS[@]})\nTest command # test command or [ ] (which is equivalent to test)\nExpression Description ( EXPRESSION ) EXPRESSION is true ! EXPRESSION EXPRESSION is false EXPRESSION1 -a EXPRESSION2 both EXPRESSION1 and EXPRESSION2 are true EXPRESSION1 -o EXPRESSION2 either EXPRESSION1 or EXPRESSION2 is true -n STRING the length of STRING is nonzero STRING equivalent to -n STRING -z STRING the length of STRING is zero STRING1 = STRING2 the strings are equal STRING1 != STRING2 the strings are not equal INTEGER1 -eq INTEGER2 INTEGER1 equals INTEGER2 INTEGER1 -ge INTEGER2 INTEGER1 is greater than or equal to INTEGER2 INTEGER1 -gt INTEGER2 INTEGER1 is greater than INTEGER2 INTEGER1 -le INTEGER2 INTEGER1 is less than or equal to INTEGER2 INTEGER1 -lt INTEGER2 INTEGER1 is less than INTEGER2 INTEGER1 -ne INTEGER2 INTEGER1 is not equal to INTEGER2 FILE1 -ef FILE2 FILE1 and FILE2 have the same device and inode numbers FILE1 -nt FILE2 FILE1 is newer (modification date) than FILE2 FILE1 -ot FILE2 FILE1 is older than FILE2 -b FILE FILE exists and is block special -c FILE FILE exists and is character special -d FILE FILE exists and is a directory -e FILE FILE exists -f FILE FILE exists and is a regular file -g FILE FILE exists and is set-group-ID -G FILE FILE exists and is owned by the effective group ID -h FILE FILE exists and is a symbolic link (same as -L) -k FILE FILE exists and has its sticky bit set -L FILE FILE exists and is a symbolic link (same as -h) -O FILE FILE exists and is owned by the effective user ID -p FILE FILE exists and is a named pipe -r FILE FILE exists and read permission is granted -s FILE FILE exists and has a size greater than zero -S FILE FILE exists and is a socket -t FD file descriptor FD is opened on a terminal -u FILE FILE exists and its set-user-ID bit is set -w FILE FILE exists and write permission is granted -x FILE FILE exists and execute (or search) permission is granted Echo(Don\u0026rsquo;t Interpret Newline Characters) # On macos, echo will interpret \\n as a new line implicitly.\nHowever, in CircleCI echo will not interpret \\n implicitly. To interpret \\n as a new line, we should use echo -e. To not interpret \\n as a new line explicitly, we should use echo -E.\nSSH/SCP with Jumphost # These only work when A can connect to B implicitly.\nscp -J username@B local/path username@C:/remote/path\nscp -i privateKeyOfC -J username@B local/path username@C:/remote/path\nssh -J username@B username@C\nIf A requires private key to connect to B, use the command below:\nscp -oProxyCommand=\u0026quot;ssh -i privateKeyOfB -W %h:%p userB@hostB\u0026quot; local/path userC@hostC:remote/path\nssh -oProxyCommand=\u0026quot;ssh -i privateKeyOfB -W %h:%p userB@hostB\u0026quot; userC@hostC \u0026quot;ls\u0026quot;\nWhich private key is used for this command? # ssh -i hostC.id_rsa -oProxyCommand=\u0026quot;ssh -i hostB.id_rsa -W %h:%p ubuntu@ec2-XXX.us-west-2.compute.amazonaws.com\u0026quot; ubuntu@ec2-YYY.us-west-2.compute.amazonaws.com\nhostC.id_rsa is stored on host A. hostB.id_rsa is stored on host A. No auto SSH connection to host C need to be set up on host B. No hostC.id_rsa need to be stored on host B. ssh-keyscan # To add the fingerprint of a host manually:\nssh-keyscan -H DEST_HOSTNAME \u0026gt;\u0026gt; ~/.ssh/known_hosts\nHow ssh-keyscan Works: # Scanning Hosts: You provide ssh-keyscan with a list of hostnames or IP addresses. It then connects to the SSH port of each host (default is port 22).\nCollecting Public Keys: For each host, ssh-keyscan retrieves the public key that the host presents during the SSH handshake process. This is the same key that a SSH client would see the first time it connects to the host.\nOutput: The utility then outputs the collected keys, typically in a format that can be directly added to an SSH known hosts file.\nTo resolve the hostname of a private instance, can I obtain the fingerprint from a third-party instance which can connect to the private instance, and then copy the fingerprint to an outside instance? # Yes, you can. But the weird thing is, this can solve the unknown hosts issue on my local machine, but not in the Github Action.\nOn my local machine, if I copy the fingerprint into the known_hosts, then there won\u0026rsquo;t be warning anymore The authenticity of host 'ec2-XX.XX.XX.XX.us-west-2.compute.amazonaws.com (\u0026lt;no hostip for proxy command\u0026gt;)' can't be established. No matter if the instance is public or private.\nBut in the Github Action, it is not working if the instance is private, only works for the public instance. As for the private instance, there is still Host key verification failed. error.\nSo how to solve the Host Key Verification Failed issue when I want to connect to a private instance from Github Action? # If you are running in certain remote/scripting situations where you lack interactive access to the prompt-to-add-hostkey(the default ssh config StrictHostKeyChecking=ask), work around it like this:\nCopy the fingerprint of hostB and hostC to the known_hosts file and use -o StrictHostKeyChecking=accept-new.\nhttps://man7.org/linux/man-pages/man5/ssh_config.5.html\nWhat is a host fingerprint? # The fingerprint is based on the host\u0026rsquo;s public key, usually based on the /etc/ssh/ssh_host_rsa_key.pub file. Generally it\u0026rsquo;s for easy identification/verification of the host you are connecting to.\nRepresentation of the Public Key: The fingerprint is a shorter, more easily readable representation of a machine\u0026rsquo;s public SSH key. It is created by applying a cryptographic hash function to the public key.\nUnique Identifier: Just like the public key, its fingerprint is unique to each key (under normal cryptographic assumptions). This uniqueness allows it to reliably represent the public key for verification purposes.\nPurpose: The main purpose of a fingerprint is to provide a convenient way for users and system administrators to verify the identity of a host. It\u0026rsquo;s much easier to compare and confirm fingerprints, which are short strings, than to compare the full public keys, which are long and complex.\nSecurity Checks: When you connect to an SSH server for the first time, the SSH client will display the fingerprint of the server\u0026rsquo;s public key. You can then verify this fingerprint by comparing it to a trusted source (like a list provided by your organization or a verification call to a system admin). This step is crucial for ensuring that you are connecting to the legitimate server and not a malicious impersonator (to avoid man-in-the-middle attacks).\nChanges in Fingerprint: If the public key of a host changes (due to key regeneration or for other reasons), its fingerprint will also change. This change will be evident when you next try to connect to the host, and your SSH client will warn you about this (as it could potentially indicate a security issue).\ninstall command # The install command in Unix and Linux systems is quite versatile and is used for more than just creating directories. Its primary function is to copy files and set file attributes in the process. Here\u0026rsquo;s a summary of its key functionalities:\nCopying Files: install can be used to copy files from one location to another. This is similar to the cp command, but with additional capabilities.\nSetting Permissions: When copying files, install allows you to set the permissions of the target file directly. This is a significant feature because, with standard copy commands, the copied file inherits the permissions of the original file, and you might need a separate command (like chmod) to change permissions.\nSetting Ownership: install can also set the owner and group of the file being copied. This is similar to what you might do with the chown command, but install does it in the same step as copying the file.\nCreating Directories: As you\u0026rsquo;ve seen, install can create directories with specific permissions, which is a feature not available in the basic mkdir command.\nStripping Debugging Symbols: When installing executables, install can strip debugging symbols from the file. This reduces the size of the installed binaries, which is often desirable in a production environment.\nPreserving Timestamps: The command can preserve or set the timestamps of files when they are installed. This can be important for certain applications where file timestamps need to be maintained.\nUse in Makefiles: install is particularly popular in makefiles for compiling and installing software. It automates the process of copying compiled binaries, scripts, and other files to their designated directories with the appropriate permissions and ownership.\n. command # . is exactly the same as source command.\nredirecting permission issue # command X | cat \u0026gt; The file you don't have permission will raise an error -bash: /XXX: Permission denied\nHowever, sudo may not work as you expected.\ncommand X | sudo cat \u0026gt; The file you don't have permission will raise the same error -bash: /XXX: Permission denied\nWhen you use a redirection with sudo, the redirection is performed by your shell, not by sudo. Since your shell doesn\u0026rsquo;t have permission to write to the file, the redirection would fail. tee handles this by running with sudo, thus having the necessary permissions to write to the file.\ncommand X | sudo tee THE_FILE \u0026gt; /dev/null this will work with root permission and doesn\u0026rsquo;t show any output.\n"},{"id":207,"href":"/docs/systems-and-networking/shell-and-commandline-tools/shell-basic-commands/ln/","title":"Ln","section":"Shell Basic Commands","content":" ln command # The ln command links the file designated in the SourceFile parameter to the file designated by the TargetFile parameter or to the same file name in another directory specified by the TargetDirectory parameter. By default, the ln command creates hard links. To use the ln command to create symbolic links, designate the -s flag.\nGot Too Many Levels of Symbolic Links Error # The issue happens when the folder name is the same because the relative path can unintentionally create a circular reference. When a symbolic link is created using a relative path and the folder has the same name, the system may resolve the symlink incorrectly, causing it to point back to itself. This leads to an infinite loop, which is why you get the \u0026ldquo;Too many levels of symbolic links\u0026rdquo; error.\nThe ln -s command relies on how the relative path is specified in relation to the symlink location, not the current directory from which you run the command.\nHow to Avoid This Problem: # Use Absolute Paths: One simple way to avoid this issue is to use absolute paths. Absolute paths don\u0026rsquo;t depend on the current directory, so the system can easily locate the correct target without getting confused by relative paths:\nln -s /tmp/test2 static/test1\nUse a correct Relative Path: ln test1 static/test2 this command will create a link test2 in the static folder, pointing to static/test1, not the test1 file in where you type the command. So use ln ../test1 static/test2 if you want it point to test1 file at your current folder.\n"},{"id":208,"href":"/docs/systems-and-networking/shell-and-commandline-tools/shell-basic-commands/ssh/","title":"SSH","section":"Shell Basic Commands","content":" Use SSH to run command remotely # ssh \u0026lt;\u0026lt; redis_instance__username \u0026gt;\u0026gt;@\u0026lt;\u0026lt; redis_instance__hostname \u0026gt;\u0026gt; \\ \u0026#34;redis-cli -h \u0026lt;\u0026lt; parameters.redis_host \u0026gt;\u0026gt; -p \u0026lt;\u0026lt; parameters.redis_port \u0026gt;\u0026gt; FLUSHALL\u0026#34; "},{"id":209,"href":"/docs/systems-and-networking/shell-and-commandline-tools/text-processing-commands-and-tools/","title":"Text Processing Commands and Tools","section":"Shell and Commandline Tools","content":"Text Processing Related Commands\n"},{"id":210,"href":"/docs/systems-and-networking/shell-and-commandline-tools/text-processing-commands-and-tools/grep/","title":"Grep","section":"Text Processing Commands and Tools","content":" grep failed with exit code 1 in CircleCI workflow # grep will throw an error when nothing matched $ echo \u0026#34;anything\u0026#34; | grep a a $ echo $? 0 $ echo \u0026#34;anything\u0026#34; | grep b $ echo $? 1 This command behaves the same across different system environments.\nHow to make grep return 0 if nothing matched? # Add || true. If the first part of the command \u0026ldquo;fails\u0026rdquo; (meaning grep e returns a non-zero exit code) then the part after the || is executed, succeeds and returns zero as the exit code (true always returns zero).\n$ echo \u0026#34;anything\u0026#34; | grep b || true $ echo $? 0 Can we use | wc -l to decide whether anything is matched instead of using || true? Typically, we can. I tried on local machine as well as CircleCI SSH server CLI.\n# On local machine OR CircleCI SSH server $ echo \u0026#34;anything\u0026#34; | grep b | wc -l 0 $ echo $? 0 However, in the CircleCI automation workflow, it will raise error code 1 too.\n# In CircleCI CICD workflow $ echo \u0026#34;anything\u0026#34; | grep b | wc -l 0 Exited with code exit status 1 "},{"id":211,"href":"/docs/web-development/","title":"Web Development","section":"Docs","content":"Web Development\n"},{"id":212,"href":"/docs/web-development/backend-techs/","title":"Backend Techs","section":"Web Development","content":"Backend Techs\n"},{"id":213,"href":"/docs/web-development/backend-techs/api-architectural-styles/","title":"API Architectural Styles","section":"Backend Techs","content":"API Architectural Styles\n"},{"id":214,"href":"/docs/web-development/backend-techs/nginx/","title":"Nginx","section":"Backend Techs","content":"Nginx\n"},{"id":215,"href":"/docs/web-development/backend-techs/nginx/config/","title":"Config","section":"Nginx","content":" QUESTION # What is the difference between listen 80 and listen 80 default_server?\nANSWER # The listen directive in Nginx is used to define what IP address and port number a server block should listen for incoming requests on. The default_server parameter is an optional argument that can be added to the listen directive to specify that a particular server block should be used as the default response for requests that do not match any other server block.\nHere\u0026rsquo;s the difference between them:\nlisten 80;: This tells Nginx to listen for incoming HTTP requests on port 80. If there are multiple server blocks listening on port 80 and a request comes in that does not match any other server_name, Nginx will use the first server block in the configuration file order.\nlisten 80 default_server;: This also tells Nginx to listen on port 80, but it goes further by designating that server block as the default for that port. If a request comes in that doesn\u0026rsquo;t match any server_name directives in any server block, Nginx will default to using this server block. If there are multiple default_server directives for the same port, Nginx will return an error when you try to restart or reload the service, because it won\u0026rsquo;t know which one to prioritize.\nYou should only have one default_server for each IP/port combination across your Nginx configuration. If you don\u0026rsquo;t specify a default_server, Nginx will use the first server block that appears in the configuration as the default for that IP/port combination.\nQUESTION # Why dose the wildcard domain _ not work?\nserver { listen 80; server_name test; location /health-check-1 { return 200 \u0026#39;healthy-1-test\\n\u0026#39;; add_header Content-Type text/plain; } } server { listen 80; server_name localhost; location /health-check-1 { return 200 \u0026#39;healthy-1-localhost\\n\u0026#39;; add_header Content-Type text/plain; } } server { listen 80; server_name _; location /health-check-1 { return 200 \u0026#39;healthy-1-wild\\n\u0026#39;; add_header Content-Type text/plain; } } ANSWER # Because in Nginx, there is no built-in wildcard mechanism in server_name.\n_ is just a domain that people don\u0026rsquo;t use.\ncurl localhost:80/health-check-1 -H \u0026#34;Host: _\u0026#34; healthy-1-wild So the Nginx will choose the first one server block as the default block for that port, if there is no default_server being declared explicitly.\nAlso, the server name is not exactly matched, but matched based on prefix.\nReference: https://www.digitalocean.com/community/tutorials/understanding-nginx-server-and-location-block-selection-algorithms\nQUESTION # How to solve this issue: HTTPS -\u0026gt; Load Balancer(Decrypt HTTPS to HTTP) -\u0026gt; HTTP -\u0026gt; Nginx -\u0026gt; HTTP -\u0026gt; Application?\nThe application treats the request as an HTTP request, but it should not be processed as an HTTP request.\nANSWER # To solve this issue, we want the application knows that what protocol that the origin request uses: It is an HTTPS or HTTP request at the beginning.\nSo what we need to do is to pass HTTPS info in the header to application, but not HTTP.\nserver { listen 80; server_name auth.scrawlrapi.com; index index.php; root /var/www/global/auth/public; # Keep the logs error_log /var/log/nginx/error_auth.log; access_log /var/log/nginx/access_auth.log; location ~ \\.php$ { try_files $uri =404; fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_pass auth_app:9000; fastcgi_index index.php; \u0026#39;\u0026#39;\u0026#39;proxy_set_header X-Forwarded-Proto $http_x_forwarded_proto;\u0026#39;\u0026#39;\u0026#39; include fastcgi_params; fastcgi_param SCRIPT_FILENAME /var/www/public/$fastcgi_script_name; fastcgi_param PATH_INFO $fastcgi_path_info; } location / { try_files $uri $uri/ /index.php?$query_string; gzip_static on; } } We added proxy_set_header X-Forwarded-Proto $http_x_forwarded_proto; into the config file, so that the nginx can forward header info from the load balancer to the application.\nReference # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-Proto\nhttps://serverfault.com/questions/1056391/proxy-forwarding-not-woking-on-nginx-laravel-elastic-load-balancer\nhttps://stackoverflow.com/questions/59970998/lumen-does-not-detect-https-behind-a-reverse-proxy\nhttps://stackoverflow.com/questions/52517614/laravel-5-7-doesnt-generate-https-routes-if-the-webserver-has-no-encryption-lay\nQUESTION # What is the meaning of the tilde after the location block in the nginx configuration? What is the difference between with and without the \u0026ldquo;~\u0026rdquo; ?\nlocation ~ \\.php$ { ... } ANSWER # The tilde instructs nginx to perform a case-sensitive regular expression match, instead of a straight string comparison.\nIf there is no tilde ~, then Nginx will only match file name as \u0026ldquo;.php\u0026rdquo; but not treat it as a regex.\n"},{"id":216,"href":"/docs/web-development/backend-techs/nginx/errors/","title":"Errors","section":"Nginx","content":" ERROR # CODE\nserver { listen 80; server_name _; # This is a catch-all server name location /health-check { return 200 \u0026#39;healthy\\n\u0026#39;; add_header Content-Type text/plain; } # The rest of your server configuration goes here... } RUN\nsudo nginx -t\nRESULT\nnginx: [warn] conflicting server name \u0026ldquo;_\u0026rdquo; on 0.0.0.0:80, ignored\nEXPLANATION\nThere are multiple blocks have the same wildcard server listening\n"},{"id":217,"href":"/docs/web-development/backend-techs/nginx/file-upload/","title":"File Upload","section":"Nginx","content":" How does the Nginx process file-upload requests and reject large files? # Nginx processes the request body incrementally rather than downloading the entire content first before checking its size. Here\u0026rsquo;s a more detailed explanation of how it handles uploads:\nReceiving the Request: When Nginx receives a request with a body (e.g., a file upload), it starts reading the body in chunks as they arrive, rather than waiting for the entire body to be received.\nIncremental Size Check: As Nginx reads each chunk of the request body, it keeps a running total of the size of the body received so far. If at any point the total size exceeds the client_max_body_size limit, Nginx will immediately stop processing the request and return a 413 Request Entity Too Large error. This prevents Nginx from unnecessarily processing and storing large amounts of data that it will ultimately reject.\nStoring the Data: If the size check passes, Nginx can then store the received data (in temporary files, buffers, or directly pass it upstream, depending on the configuration and the request).\nPassing Data to Upstream: Once the entire request body is received and validated, Nginx can pass the data to an upstream server, such as a PHP-FPM backend, for further processing (like saving the uploaded file to disk or processing form data).\nHere is a summary of the process:\nNginx receives the request and starts reading the body. It checks the size of each chunk as it is received. If the cumulative size exceeds the client_max_body_size limit, Nginx returns a 413 error and discards the request body. If the body is within the size limit, Nginx continues to process the request as configured. This approach ensures efficient handling of request bodies, especially for large file uploads, by rejecting oversized bodies as early as possible.\n"},{"id":218,"href":"/docs/web-development/backend-techs/web-security/","title":"Web Security","section":"Backend Techs","content":"Web Security\n"},{"id":219,"href":"/docs/web-development/backend-techs/web-security/captcha-recaptcha/","title":"Captcha Re Captcha","section":"Web Security","content":"TBD\n"},{"id":220,"href":"/docs/web-development/backend-techs/web-security/csrf/","title":"Csrf","section":"Web Security","content":" What is CSRF(Cross-Site Request Forgery)? # CSRF stands for Cross-Site Request Forgery, which is a type of security vulnerability typically found in web applications. It allows an attacker to induce users to perform actions that they do not intend to do. A CSRF attack specifically targets state-changing requests, not theft of data, since the attacker has no way to see the response to the forged request.\nHere\u0026rsquo;s a basic example of how CSRF could work:\nA user logs into www.example.com, where they have authentication privileges. The site\u0026rsquo;s response includes a session cookie which is stored in the user\u0026rsquo;s browser. The user later visits a malicious site, www.attacker.com. www.attacker.com contains a code, say an HTML form or JavaScript, that makes a request to www.example.com (like a fund transfer, a password change, etc.). Since the user\u0026rsquo;s browser still contains the session cookie for www.example.com, this site considers the request to be legitimate and executes it. This vulnerability exists because websites typically cannot distinguish between legitimate requests and forged requests. To mitigate CSRF, websites use various strategies like CSRF tokens, which are unique to each session and must be included as part of a valid request, making it difficult for an attacker to forge a request. Other strategies include checking the Referer header or using custom headers that are harder for an attacker to reproduce.\n"},{"id":221,"href":"/docs/web-development/backend-techs/web-security/ssl-tls/","title":"Ssl Tls","section":"Web Security","content":" How the SSL/TLS is used when users access websites via HTTPS? # SSL/TLS (Secure Sockets Layer/Transport Layer Security) is a protocol for securing internet communications. It\u0026rsquo;s used extensively for securing data transfer, especially in HTTPS, which is the secure version of HTTP used for web browsing. Here\u0026rsquo;s a simplified overview of how SSL/TLS works when you access a website via HTTPS:\nClient-Server Handshake Initiation\nClient Hello: When you enter an HTTPS URL in your browser (the client), it initiates a connection to the server hosting the website. The client sends a \u0026ldquo;Client Hello\u0026rdquo; message, which includes the TLS version it supports, a list of supported cipher suites (algorithms for encryption, decryption, and authentication), and a randomly generated number (Client Random). Server Response\nServer Hello: The server replies with a \u0026ldquo;Server Hello\u0026rdquo; message, selecting a TLS version and a cipher suite from the list provided by the client. It also sends a randomly generated number (Server Random). Certificate: The server sends its SSL/TLS certificate, which contains its public key and is digitally signed by a trusted certificate authority (CA). This ensures the authenticity of the server. Server Key Exchange (if needed): For some cipher suites, the server sends additional key exchange information. Client Verification and Key Exchange\nCertificate Verification: The client verifies the server\u0026rsquo;s certificate against a list of trusted CAs. If the verification fails, the connection is terminated (e.g., warning about an untrusted certificate). Pre-Master Secret: The client generates a \u0026ldquo;Pre-Master Secret,\u0026rdquo; encrypts it with the server\u0026rsquo;s public key (from the certificate), and sends it to the server. Key Generation: Both the client and server use the Pre-Master Secret along with the Client Random and Server Random to generate a \u0026ldquo;Master Secret.\u0026rdquo; From this, they independently generate a set of symmetric keys (used for encryption and decryption of data). Handshake Finalization\nClient Finished: The client sends a \u0026ldquo;Finished\u0026rdquo; message, encrypted with the symmetric key, indicating the end of the handshake process from its side. Server Finished: The server responds with its own \u0026ldquo;Finished\u0026rdquo; message, similarly encrypted. Secure Data Transfer\nOnce the handshake is complete, all data transferred between the client and server is encrypted with the symmetric keys. This includes the HTTP requests and responses. Session Closure\nEither party can send a message to close the session securely. The symmetric keys are discarded at the end of the session. This process ensures that the data transferred between your browser and the server is encrypted, protecting it from eavesdroppers. Additionally, the verification of the server\u0026rsquo;s certificate ensures that you are communicating with the intended server, not an imposter (man-in-the-middle attack prevention).\nHow the SSL/TLS certificate is issued? # The process of verifying the server\u0026rsquo;s SSL/TLS certificate against a list of trusted Certificate Authorities (CAs) is a crucial aspect of the SSL/TLS protocol, ensuring the authenticity and integrity of the certificate. Here\u0026rsquo;s how this process works:\nCertificate Issuance: When a server obtains an SSL/TLS certificate from a CA, it first generates a key pair consisting of a public key and a private key. The server keeps the private key secret and sends a Certificate Signing Request (CSR) to the CA, which includes the public key and the server\u0026rsquo;s identifying information.\nCertificate Signing: The CA verifies the identity and legitimacy of the server. Upon successful verification, the CA signs the server\u0026rsquo;s certificate using the CA\u0026rsquo;s own private key. This signature is an essential component of the SSL/TLS certificate, as it links the server\u0026rsquo;s public key to its identity and to the CA\u0026rsquo;s trustworthiness.\nCertificate Distribution: The signed certificate is returned to the server. This certificate includes the server\u0026rsquo;s public key, identifying information, and the CA\u0026rsquo;s signature.\nClient Verification Process:\nWhen a client (like a web browser) connects to the server via HTTPS, the server presents its SSL/TLS certificate. The client checks whether the certificate is signed by a trusted CA. Operating systems and browsers come with a pre-installed list of trusted CAs and their public keys. The client uses the CA\u0026rsquo;s public key (from its own trust store) to verify the CA\u0026rsquo;s signature on the server\u0026rsquo;s certificate. If the signature is valid, it means the certificate is authentic and the server is who it claims to be. The client also checks other certificate attributes like the validity period, domain name, and whether the certificate has been revoked. Role of the CA\u0026rsquo;s Private Key: The CA\u0026rsquo;s private key is used only for signing certificates and is securely stored by the CA.\n"},{"id":222,"href":"/docs/web-development/data-transmission/","title":"Data Transmission","section":"Web Development","content":"Data Transmission\n"},{"id":223,"href":"/docs/web-development/data-transmission/file-upload-limitation/","title":"File Upload Limitation","section":"Data Transmission","content":" How can I upload large files? # The limitation on the server configuration size # Let take PHP+Nginx as an example, we need to update the following parameters to increase the file upload limitations:\nNginx: client_max_body_size PHP: post_max_size upload_max_filesize max_file_uploads Reference: # https://serverfault.com/questions/611239/increase-php-fpms-max-upload-post-size\nRequest payload limit with AWS API Gateway # Maximum payload to API gateway is 10 MB and maximum payload for Lambda is 6 MB, which cannot be increased.\nPlease see API gateway payload limits here\nPlease see Lambda payload limits here\nBut there is an alternative way (a work around) to achieve the same by uploading data to an S3 bucket if your size is more that 10 MB. Please read the below article for details (Unofficial document):\nhttps://sookocheff.com/post/api/uploading-large-payloads-through-api-gateway/\nReference: # https://stackoverflow.com/questions/46358922/request-payload-limit-with-aws-api-gateway\nhttps://aws.amazon.com/cn/blogs/compute/patterns-for-building-an-api-to-upload-files-to-amazon-s3/#:~:text=Using%20Amazon%20API%20Gateway%20as%20a%20direct%20proxy\u0026text=This%20pattern%20allows%20you%20to,payload%20size%20of%2010%20MB.\n"},{"id":224,"href":"/docs/web-development/frontend-techs/","title":"Frontend Techs","section":"Web Development","content":"Frontend Techs\n"},{"id":225,"href":"/docs/web-development/frontend-techs/authentication-and-authorization/callback-url/","title":"Callback URL","section":"Authentication and Authorization","content":" What is callback URL? # A callback URL is a web address that an application will redirect to after a certain event has occurred, often with some additional data. This is a common pattern in web development, especially in scenarios involving authentication, webhooks, or integrating third-party services.\nFor example, when you log into a website using a third-party service (like logging into a site with your Google account), after successful authentication, Google will redirect you back to the original website using a callback URL provided by that website. This URL often includes important data like an authentication token.\nIn simpler terms, a callback URL is like giving someone a return address; once they\u0026rsquo;ve completed their task (like authenticating your login), they\u0026rsquo;ll \u0026ldquo;send\u0026rdquo; you (redirect your browser) back to that address, often with some extra information needed for the next step of the process.\n"},{"id":226,"href":"/docs/web-development/frontend-techs/authentication-and-authorization/cookie/","title":"Cookie","section":"Authentication and Authorization","content":" What is Cookie? # A cookie is a small piece of data sent from a website and stored on the user\u0026rsquo;s computer by the user\u0026rsquo;s web browser while the user is browsing.\nCookies are used to remember information about the user for the duration of the visit (session cookies) or for repeat visits (persistent cookies). They can store preferences, session information, and other data to improve the user experience or track user behavior.\nCookies can be vulnerable to theft (via XSS attacks) or interception (if not secured with attributes like HttpOnly and Secure).\nName: sessionToken Value: abc123 Domain: example.com Path: / Expires: Wed, 09 Jun 2024 10:18:14 GMT Secure: true HttpOnly: true SameSite: Strict A web cookie is essentially a small piece of plain text, not a JSON object or any other complex data structure.\nUsage Example # Imagine a user visiting an e-commerce website for the first time. The website wants to track the user\u0026rsquo;s session to keep items in the shopping cart as the user browses.\nAction: The user adds an item to their shopping cart. Behind the Scenes: The server sends a response to the user\u0026rsquo;s browser with a Set-Cookie header. This header might include a cookie like session_id=12345. Browser\u0026rsquo;s Role: The user\u0026rsquo;s browser stores this cookie and sends it back to the server with every subsequent request. Result: The server identifies the session_id cookie in each request, allowing it to retrieve and maintain the state of that user\u0026rsquo;s shopping cart. Difference under the hood # The key differences among these methods(Cookie, Session, JWT) lie in how and where user data is stored and managed, as well as the specific use cases they are best suited for.\nStorage Location: Directly on the user\u0026rsquo;s browser. Data Storage: Can store data directly (like preferences, session IDs, etc.). Management: Managed by the browser. The server can set, read, and delete cookies via HTTP headers. Use Case: Ideal for simple, small pieces of data that need to persist across sessions, like user preferences or session identifiers. Security: Since they are stored client-side, they are more vulnerable to attacks like cross-site scripting (XSS). Secure attributes like HttpOnly and Secure can mitigate some risks. Example Code # Python # from flask import Flask, request, make_response app = Flask(__name__) @app.route(\u0026#39;/set_theme/\u0026lt;theme\u0026gt;\u0026#39;) def set_theme(theme): resp = make_response(f\u0026#34;Theme set to {theme}\u0026#34;) resp.set_cookie(\u0026#39;theme\u0026#39;, theme) return resp @app.route(\u0026#39;/\u0026#39;) def index(): theme = request.cookies.get(\u0026#39;theme\u0026#39;, \u0026#39;default\u0026#39;) return f\u0026#34;The current theme is {theme}\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: app.run() Golang # package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34; ) func setCookieHandler(c *gin.Context) { c.SetCookie(\u0026#34;user\u0026#34;, \u0026#34;John Doe\u0026#34;, 3600, \u0026#34;/\u0026#34;, \u0026#34;localhost\u0026#34;, false, false) c.String(http.StatusOK, \u0026#34;Cookie has been set\u0026#34;) } func deleteCookieHandler(c *gin.Context) { c.SetCookie(\u0026#34;user\u0026#34;, \u0026#34;\u0026#34;, -1, \u0026#34;/\u0026#34;, \u0026#34;localhost\u0026#34;, false, true) c.String(http.StatusOK, \u0026#34;Cookie has been deleted\u0026#34;) } func getCookieHandler(c *gin.Context) { cookie, err := c.Cookie(\u0026#34;user\u0026#34;) if err != nil { c.String(http.StatusNotFound, \u0026#34;Cookie not found\u0026#34;) return } c.String(http.StatusOK, \u0026#34;Cookie value: %s\u0026#34;, cookie) } func helloWorld(c *gin.Context) { c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;Hello, world!\u0026#34;, }) getCookieHandler(c) } func main() { r := gin.Default() r.GET(\u0026#34;/\u0026#34;, helloWorld) r.GET(\u0026#34;/login\u0026#34;, setCookieHandler) r.GET(\u0026#34;/logout\u0026#34;, deleteCookieHandler) r.Run() // Listen and serve on 0.0.0.0:8080 } document.cookie returns an empty string # If document.cookie in the browser console is returning an empty string despite setting a cookie using Gin (or any server-side framework), there are several potential reasons for this behavior:\nHttpOnly Flag:\nIf the cookie is set with the HttpOnly flag, it cannot be accessed via JavaScript for security reasons. This flag is often used to prevent access to cookies that are meant for server-side use only, especially to mitigate the risk of Cross-Site Scripting (XSS) attacks.\n"},{"id":227,"href":"/docs/web-development/frontend-techs/authentication-and-authorization/jwt/","title":"JWT","section":"Authentication and Authorization","content":" What is JWT? # JWT is an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object.\nJWTs are commonly used for authorization and information exchange. They can encode user credentials and are often used in token-based authentication systems.\nJWTs are secure because they can be digitally signed. However, they should be transmitted securely and stored safely. Since JWTs can contain sensitive information, they should not be exposed to untrusted environments.\nUsage Example # Now, suppose the user wants to access a restricted area of the website that requires an authentication token.\nLogin Process: Upon successful login, instead of or in addition to creating a session, the server creates a JWT containing user information and digitally signs it. Token Structure: The JWT typically consists of three parts - header, payload, and signature. The payload might include user identification info. Sending the Token: This token is sent to the user\u0026rsquo;s browser. Browser\u0026rsquo;s Role: The browser stores this token (often in local storage) and sends it in the HTTP Authorization header with requests to access protected resources. Server Verification: The server verifies the token\u0026rsquo;s signature to ensure it\u0026rsquo;s valid and hasn\u0026rsquo;t been tampered with and then grants or denies access based on the token. Difference under the hood # Storage Location: Can be stored in the browser (often in local storage) or sent in each HTTP request\u0026rsquo;s headers. Data Storage: Encodes data as a token, which includes information and a signature. The token can contain claims (data) about the user. Management: Created and signed by the server. The browser simply stores and transmits the token. Use Case: Ideal for scenarios where stateless authentication is needed, especially in distributed systems or APIs. Security: Secure as they are digitally signed. Vulnerable to theft, so it\u0026rsquo;s crucial not to store sensitive information directly in the token and to transmit securely. JWT structure # Header: The header typically consists of two parts: the type of the token, which is JWT, and the signing algorithm being used, such as HMAC SHA256 or RSA.\nExample:\n{ \u0026#34;alg\u0026#34;: \u0026#34;HS256\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34; } This JSON is Base64Url encoded to form the first part of the JWT.\nPayload: The second part of the token is the payload, which contains the claims. Claims are statements about an entity (typically, the user) and additional data. There are three types of claims: registered, public, and private claims.\nExample:\n{ \u0026#34;sub\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;admin\u0026#34;: true } This JSON is also Base64Url encoded to form the second part of the JWT.\nSignature: To create the signature part, you have to take the encoded header, the encoded payload, a secret, the algorithm specified in the header, and sign that.\nFor example, if you\u0026rsquo;re using the HMAC SHA256 algorithm, the signature will be created in the following way:\nHMACSHA256( base64UrlEncode(header) + \u0026#34;.\u0026#34; + base64UrlEncode(payload), secret) The signature is used to verify that the sender of the JWT is who it says it is and to ensure that the message wasn\u0026rsquo;t changed along the way.\nWhere to Use Public and Private Keys # Private Key:\nLocation: Stored securely on the server that generates and signs the JWTs. Usage: Used to sign the JWT before sending it to the client. Public Key:\nLocation: Can be distributed to any server or service that needs to verify the JWT. It can be public because it cannot be used to generate valid tokens, only to verify them. Usage: Used to verify the JWT\u0026rsquo;s signature when a client presents the token for authentication. Example Flow with Public/Private Key: # Server A (Authentication Service) generates a JWT after a user logs in and signs it with its private key. Server B (Resource Server) receives a request with the JWT and uses the public key to verify the token. If valid, Server B allows access to the requested resource. "},{"id":228,"href":"/docs/web-development/frontend-techs/authentication-and-authorization/multi-factor-authentication/","title":"Multi Factor Authentication","section":"Authentication and Authorization","content":"TBD\n"},{"id":229,"href":"/docs/web-development/frontend-techs/authentication-and-authorization/oauth-oidc/","title":"Oauth Oidc","section":"Authentication and Authorization","content":"TBD\n"},{"id":230,"href":"/docs/web-development/frontend-techs/authentication-and-authorization/session/","title":"Session","section":"Authentication and Authorization","content":" What is Session? # A session is a server-side storage of information that is related to a particular user or browser.\nSessions are used to persist user data across multiple HTTP requests. When a session is started, the server creates a unique identifier (session ID) which is typically passed back to the browser via a cookie.\nThe main security concern is session hijacking. Protecting the session ID, especially in transit (using HTTPS), is critical.\n{ \u0026#34;session_id\u0026#34;: \u0026#34;ABCDEFG1234567\u0026#34;, \u0026#34;user_id\u0026#34;: 123, \u0026#34;auth\u0026#34;: true, \u0026#34;last_accessed\u0026#34;: \u0026#34;2024-01-15T12:34:56\u0026#34; } session_id: Unique identifier for the session. user_id: Identifier for the user. auth: A flag indicating whether the user is authenticated. last_accessed: The last time the session was accessed. Remember, this data is stored on the server. The client only has the session_id, typically in a cookie.\nUsage Example # Action: The user logs in with their credentials. Behind the Scenes: The server verifies the credentials and creates a session on the server side, assigning it a unique session ID, say session_id=ABCDE. Cookie Integration: This session ID is sent back to the browser as a cookie. Subsequent Requests: Each time the user makes a new request, the browser sends back the session ID. The server uses this ID to retrieve session data (like user authentication status, user preferences, etc.) and knows it\u0026rsquo;s the same user across different requests. Security: The session data itself is stored server-side, and only the session ID is exchanged with the browser, enhancing security. Difference under the hood # Storage Location: On the server. Data Storage: Stores user data on the server. A session ID is usually stored in a cookie on the client side to identify the session. Management: Managed by the server. The session data is linked to a session ID, which the browser sends with each request. Use Case: Suited for storing more sensitive or larger amounts of data that shouldn\u0026rsquo;t be exposed to or managed by the client. Security: More secure as the data is stored server-side. The main risk is session hijacking, which can be mitigated by secure transmission of the session ID. Example Code # from flask import Flask, session, redirect, url_for, request app = Flask(__name__) app.secret_key = \u0026#39;your_secret_key\u0026#39; # Set a secret key for session management @app.route(\u0026#39;/login\u0026#39;, methods=[\u0026#39;GET\u0026#39;, \u0026#39;POST\u0026#39;]) def login(): if request.method == \u0026#39;POST\u0026#39;: session[\u0026#39;username\u0026#39;] = request.form[\u0026#39;username\u0026#39;] return redirect(url_for(\u0026#39;index\u0026#39;)) return \u0026#39;\u0026#39;\u0026#39; \u0026lt;form method=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;p\u0026gt;\u0026lt;input type=text name=username\u0026gt; \u0026lt;p\u0026gt;\u0026lt;input type=submit value=Login\u0026gt; \u0026lt;/form\u0026gt; \u0026#39;\u0026#39;\u0026#39; @app.route(\u0026#39;/\u0026#39;) def index(): if \u0026#39;username\u0026#39; in session: return f\u0026#39;Logged in as {session[\u0026#34;username\u0026#34;]}\u0026#39; return \u0026#39;You are not logged in\u0026#39; @app.route(\u0026#39;/logout\u0026#39;) def logout(): session.pop(\u0026#39;username\u0026#39;, None) return redirect(url_for(\u0026#39;index\u0026#39;)) if __name__ == \u0026#39;__main__\u0026#39;: app.run() Golang # func incrHandler(c *gin.Context) { session := sessions.Default(c) var count int v := session.Get(\u0026#34;count\u0026#34;) if v == nil { count = 0 } else { count = v.(int) count++ } session.Set(\u0026#34;count\u0026#34;, count) session.Save() c.JSON(200, gin.H{\u0026#34;count\u0026#34;: count}) } session hijacking # Security Concern # We should assume that the sessionID in the user\u0026rsquo;s local computer is safe. As for the interception issue, we can ignore that due to the popularization of HTTPS. "},{"id":231,"href":"/docs/web-development/frontend-techs/css/","title":"Css","section":"Frontend Techs","content":"CSS Basics\n"},{"id":232,"href":"/docs/web-development/frontend-techs/html/","title":"HTML","section":"Frontend Techs","content":"HTML Basics\n"},{"id":233,"href":"/docs/web-development/frontend-techs/html/html/","title":"HTML","section":"HTML","content":"TBD\n"},{"id":234,"href":"/docs/web-development/frontend-techs/javascript/","title":"Javascript","section":"Frontend Techs","content":"JavaScript Basics\n"},{"id":235,"href":"/docs/web-development/frontend-techs/node.js/","title":"Node.js","section":"Frontend Techs","content":" What is NodeJS? # Javascript is a computer programming language that is used to build scripts for websites. Only browsers are capable of running Javascript.\nNodeJS is a Javascript runtime environment.\nWith the support of NodeJS, Javascript can be executed outside of the browser.\n"},{"id":236,"href":"/docs/web-development/frontend-techs/node.js/issues/","title":"Issues","section":"Node.js","content":" Node 20 cannot bind port less than 1024 without root permission # The same code to listen on 0.0.0.0:443 works well on Node 18, but got an error on Node 20:\n\u0026#34;Error: listen EACCES: permission denied 0.0.0.0:1023 at Http2SecureServer.setupListenHandle [as _listen2] (node:net:1880:21) at listenInCluster (node:net:1945:12) at doListen (node:net:2109:7) at process.processTicksAndRejections (node:internal/process/task_queues:83:21)\u0026#34;, \u0026#34;code\u0026#34;:\u0026#34;EACCES\u0026#34;, \u0026#34;errno\u0026#34;:-13, \u0026#34;syscall\u0026#34;:\u0026#34;listen\u0026#34;, \u0026#34;address\u0026#34;:\u0026#34;0.0.0.0\u0026#34;, \u0026#34;port\u0026#34;:1023}, \u0026#34;msg\u0026#34;:\u0026#34;listen EACCES: permission denied 0.0.0.0:1023\u0026#34; So I tried to listen on port 1023 and 1024, and the project can listen well on 1024 but not 1023.\n"},{"id":237,"href":"/docs/web-development/frontend-techs/package-management/","title":"Package Management","section":"Frontend Techs","content":"Frontend Package Management\n"},{"id":238,"href":"/docs/web-development/frontend-techs/react/","title":"React","section":"Frontend Techs","content":"React Basics\n"},{"id":239,"href":"/docs/web-development/frontend-techs/security/","title":"Security","section":"Frontend Techs","content":"Frontend Web Security\n"},{"id":240,"href":"/docs/web-development/frontend-techs/security/cors/","title":"Cors","section":"Security","content":" What will happen if I allow localhost in CORS policy? # When you allow http://localhost:3000 in your CORS policy, anyone running a frontend on http://localhost:3000 on their own machine will be able to make requests to your backend service. Here\u0026rsquo;s how it works:\nCORS and Localhost: The http://localhost:3000 you allow in your CORS policy refers to any instance of a service running on port 3000 on localhost. This localhost is specific to the computer that the service is running on.\nDifferent People, Different localhost: If multiple people run frontend services on their own localhost:3000, they are running those services on their own machines. Therefore, if your backend service allows requests from http://localhost:3000, it will accept requests from any frontend running at localhost:3000 on any machine.\nKey Point: # Not Limited to Your Machine: Allowing http://localhost:3000 in CORS does not limit requests to just your machine. It means that any machine with a service running at http://localhost:3000 can make cross-origin requests to your backend service.\nSo, if someone else runs a frontend on their own computer at http://localhost:3000, and your backend allows requests from http://localhost:3000, their frontend can access your backend service.\nExample: # Your Development Setup: You may allow http://localhost:3000 in CORS because your frontend is running locally at localhost:3000 and you want it to communicate with your backend. Another Developer\u0026rsquo;s Setup: Another person on their own computer could also run a frontend at http://localhost:3000. If your backend is publicly accessible and allows requests from http://localhost:3000, their frontend could also make requests to your backend. Important Considerations: # Backend Authentication: While CORS allows cross-origin requests, it doesn\u0026rsquo;t handle authentication. Even if someone can send requests to your backend from http://localhost:3000, your backend should still have proper authentication and authorization mechanisms to control access. Local Development vs. Production: Typically, allowing localhost in CORS is for development purposes. In production, you would restrict CORS to trusted domains (e.g., your deployed frontend\u0026rsquo;s domain) to prevent unintended access. In summary, allowing http://localhost:3000 in CORS means that anyone running a service on their own localhost:3000 can access your backend, not just you on your own machine.\n"},{"id":241,"href":"/docs/web-development/frontend-techs/typescript/","title":"Typescript","section":"Frontend Techs","content":"Typescript Basics\n"},{"id":242,"href":"/docs/web-development/frontend-techs/vue/","title":"Vue","section":"Frontend Techs","content":"VUE Basics\n"},{"id":243,"href":"/docs/web-development/header/","title":"Header","section":"Web Development","content":"HTTP Headers\n"},{"id":244,"href":"/docs/web-development/header/forwarded/","title":"Forwarded","section":"Header","content":" RFC 7239 # The Forwarded header is standardized in RFC 7239 and is designed to convey information about the client and the proxy chain.\n"},{"id":245,"href":"/docs/web-development/high-throughput/","title":"High Throughput","section":"Web Development","content":"High Throughput Topics\n"},{"id":246,"href":"/docs/web-development/testing/","title":"Testing","section":"Web Development","content":"Web Testing\n"}]