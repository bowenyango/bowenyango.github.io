<!DOCTYPE html><html lang="en-us" class="__variable_0aa4ae scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/2d141e1a38819612-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/_next/static/media/logo.a3d92d8f.png"/><link rel="stylesheet" href="/_next/static/css/5beba742ae93b864.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/f719ff1083adf929.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/e36d2c8a31781d2e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-8a3a6504f64badee.js"/><script src="/_next/static/chunks/fd9d1056-7b6151dbf9b37af2.js" async=""></script><script src="/_next/static/chunks/23-ba11cb3b863464a4.js" async=""></script><script src="/_next/static/chunks/main-app-4250bb735c1c7796.js" async=""></script><script src="/_next/static/chunks/231-42eeaa612179830e.js" async=""></script><script src="/_next/static/chunks/113-8a0c0daa2bfc088c.js" async=""></script><script src="/_next/static/chunks/app/layout-a7a2551425c87cc3.js" async=""></script><script src="/_next/static/chunks/173-af5b99c330035292.js" async=""></script><script src="/_next/static/chunks/459-6c5dcdc51f43b997.js" async=""></script><script src="/_next/static/chunks/app/projects/%5B...slug%5D/page-b93535745cdfa673.js" async=""></script><link rel="preload" href="https://analytics.umami.is/script.js" as="script"/><title>Building a Fault-Tolerant Sharded Key-Value Store with Paxos | CodeWisdom</title><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://codewisdom.io/projects/paxos"/><link rel="alternate" type="application/rss+xml" href="https://codewisdom.io/feed.xml"/><meta property="og:title" content="Building a Fault-Tolerant Sharded Key-Value Store with Paxos"/><meta property="og:url" content="https://codewisdom.io/projects/paxos"/><meta property="og:site_name" content="CodeWisdom"/><meta property="og:locale" content="en_US"/><meta property="og:image" content="https://codewisdom.io/static/images/logo-round.png"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2024-11-07T00:00:00.000Z"/><meta property="article:modified_time" content="2024-11-07T00:00:00.000Z"/><meta property="article:author" content="Bowen Y"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Building a Fault-Tolerant Sharded Key-Value Store with Paxos"/><meta name="twitter:image" content="https://codewisdom.io/static/images/logo-round.png"/><meta name="next-size-adjust"/><link rel="apple-touch-icon" sizes="76x76" href="/static/favicons/logo-round.png"/><link rel="icon" type="image/png" sizes="32x32" href="/static/favicons/logo-round.png"/><link rel="icon" type="image/png" sizes="16x16" href="/static/favicons/logo-round.png"/><link rel="manifest" href="/static/favicons/site.webmanifest"/><meta name="msapplication-TileColor" content="#000000"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="#fff"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="bg-white pl-[calc(100vw-100%)] text-black antialiased dark:bg-gray-950 dark:text-white"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><section class="mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0"><header class="flex items-center w-full bg-white dark:bg-gray-950 justify-between py-10"><a class="break-words" aria-label="CodeWisdom" href="/"><div class="flex items-center justify-between"><div class="mr-3"><img src="/_next/static/media/logo.a3d92d8f.png" alt="logo" class="h-10 w-14"/></div><div class="hidden h-6 text-2xl font-semibold sm:block">CodeWisdom</div></div></a><div class="flex items-center space-x-4 leading-5 sm:space-x-6"><div class="no-scrollbar hidden max-w-40 items-center space-x-4 overflow-x-auto sm:flex sm:space-x-6 md:max-w-72 lg:max-w-96"><a class="block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400" href="/blog">Blog</a><a class="block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400" href="/tags">Tags</a><a class="block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400" href="/projects">Projects</a><a class="block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400" href="/about">About</a></div><button aria-label="Search"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="h-6 w-6 text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 10.607z"></path></svg></button><div class="mr-5 flex items-center"><div class="relative inline-block text-left" data-headlessui-state=""><div class="flex items-center justify-center hover:text-primary-500 dark:hover:text-primary-400"><button aria-label="Theme switcher" id="headlessui-menu-button-:Rn6jaba:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><svg class="h-6 w-6"></svg></button></div></div></div><button aria-label="Toggle Menu" class="sm:hidden"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="h-8 w-8 text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg></button><div hidden="" style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div></div></header><main class="mb-auto"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Project","headline":"Building a Fault-Tolerant Sharded Key-Value Store with Paxos","datePublished":"2024-11-07T00:00:00.000Z","dateModified":"2024-11-07T00:00:00.000Z","image":"/static/images/logo-round.png","url":"https://codewisdom.io/projects/paxos","author":[{"@type":"Person","name":"Bowen Y"}]}</script><section class="mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0"><div class="fixed bottom-8 right-8 hidden flex-col gap-3 md:hidden"><button aria-label="Scroll To Comment" class="rounded-full bg-gray-200 p-2 text-gray-500 transition-all hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-400 dark:hover:bg-gray-600"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M18 10c0 3.866-3.582 7-8 7a8.841 8.841 0 01-4.083-.98L2 17l1.338-3.123C2.493 12.767 2 11.434 2 10c0-3.866 3.582-7 8-7s8 3.134 8 7zM7 9H5v2h2V9zm8 0h-2v2h2V9zM9 9h2v2H9V9z" clip-rule="evenodd"></path></svg></button><button aria-label="Scroll To Top" class="rounded-full bg-gray-200 p-2 text-gray-500 transition-all hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-400 dark:hover:bg-gray-600"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z" clip-rule="evenodd"></path></svg></button></div><article><div class="xl:divide-y xl:divide-gray-200 xl:dark:divide-gray-700"><header class="pt-6 xl:pb-6"><div class="space-y-1 text-center"><dl class="space-y-10"><div><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2024-11-07T00:00:00.000Z">Wednesday, November 6, 2024</time></dd></div></dl><div><h1 class="text-3xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 sm:text-4xl sm:leading-10 md:text-5xl md:leading-14">Building a Fault-Tolerant Sharded Key-Value Store with Paxos</h1></div></div></header><div class="grid-rows-[auto_1fr] divide-y divide-gray-200 pb-8 dark:divide-gray-700 xl:grid xl:grid-cols-4 xl:gap-x-6 xl:divide-y-0"><dl class="pb-10 pt-6 xl:border-b xl:border-gray-200 xl:pt-11 xl:dark:border-gray-700"><dt class="sr-only">Authors</dt><dd><ul class="flex flex-wrap justify-center gap-4 sm:space-x-12 xl:block xl:space-x-0 xl:space-y-8"><li class="flex items-center space-x-2"><img alt="avatar" loading="lazy" width="38" height="38" decoding="async" data-nimg="1" class="h-10 w-10 rounded-full" style="color:transparent" src="/static/images/pixel-avatar.png"/><dl class="whitespace-nowrap text-sm font-medium leading-5"><dt class="sr-only">Name</dt><dd class="text-gray-900 dark:text-gray-100">Bowen Y</dd><dt class="sr-only">Twitter</dt><dd></dd></dl></li></ul></dd></dl><div class="divide-y divide-gray-200 dark:divide-gray-700 xl:col-span-3 xl:row-span-2 xl:pb-0"><div class="prose max-w-none pb-8 pt-10 dark:prose-invert"><h2 class="content-header" id="project-overview"><a class="break-words" href="#project-overview" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Project Overview</h2><p>In the context of the University of Washington&#x27;s CSE 452 course on Distributed Systems, I developed a fault-tolerant, sharded key-value (KV) storage system in Java. This project involved implementing an exactly-once Remote Procedure Call (RPC) mechanism, a primary-backup server architecture utilizing Viewstamped Replication for fault tolerance, and a sophisticated multi-Paxos replica system. The design integrated two-phase commit, two-phase locking, and Paxos replica groups to create a simplified, Spanner-like sharded KV-store system.</p><h2 class="content-header" id="course-context-cse-452---distributed-systems"><a class="break-words" href="#course-context-cse-452---distributed-systems" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Course Context: CSE 452 - Distributed Systems</h2><p>CSE 452 is a senior-level course at the University of Washington that covers abstractions and implementation techniques for constructing distributed systems. Topics include client-server computing, cloud computing, peer-to-peer systems, and distributed storage systems. The course emphasizes hands-on projects to reinforce concepts such as remote procedure calls, consistency maintenance, fault tolerance, high availability, and scalability.</p><h2 class="content-header" id="implementation-overview"><a class="break-words" href="#implementation-overview" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Implementation Overview</h2><h3 class="content-header" id="1-exactly-once-rpc-mechanism-and-primary-backup-server"><a class="break-words" href="#1-exactly-once-rpc-mechanism-and-primary-backup-server" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>1. Exactly-Once RPC Mechanism and Primary-Backup Server</h3><p>To ensure reliable communication between clients and servers, I implemented an exactly-once RPC mechanism. This approach guarantees that each RPC is executed precisely once, even in the presence of network failures or retries. The primary-backup server architecture was enhanced using Viewstamped Replication, which provides fault tolerance by maintaining consistency between the primary server and its backups. This setup ensures that the system can recover from failures without data loss or inconsistency.</p><h3 class="content-header" id="2-multi-paxos-replica-system"><a class="break-words" href="#2-multi-paxos-replica-system" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>2. Multi-Paxos Replica System</h3><p>Building upon the primary-backup architecture, I designed a multi-Paxos replica system to handle more complex scenarios involving multiple replicas. This system employed the PMMC (Paxos Made Moderately Complex) design, integrating:</p><ul><li><strong>Two-Phase Commit</strong>: To coordinate transactions across multiple nodes, ensuring all or none of the operations are committed.</li><li><strong>Two-Phase Locking</strong>: To manage concurrent access to resources, preventing conflicts and ensuring data consistency.</li><li><strong>Paxos Replica Groups</strong>: To achieve consensus among replicas, allowing the system to function correctly even if some replicas fail.</li></ul><p>This combination enabled the creation of a sharded KV-store system with properties similar to Google&#x27;s Spanner, providing scalability and strong consistency across distributed nodes.</p><h2 class="content-header" id="implementation-details"><a class="break-words" href="#implementation-details" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Implementation Details</h2><h3 class="content-header" id="two-phase-commit-2pc"><a class="break-words" href="#two-phase-commit-2pc" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a><strong>Two-Phase Commit (2PC)</strong></h3><p>To coordinate transactions across multiple nodes, ensuring all or none of the operations are committed.</p><p><strong>Introduction and Explanation:</strong></p><p>The Two-Phase Commit (2PC) protocol is a distributed algorithm that ensures all participating nodes in a distributed transaction agree on a common commit or abort decision. It is widely used in distributed database systems and transaction processing systems to maintain atomicity and consistency across multiple nodes or databases.</p><p><strong>Phases of Two-Phase Commit:</strong></p><ol><li><strong>Prepare Phase (Voting Phase):</strong><ul><li><strong>Initiation:</strong> The coordinator (transaction manager) initiates the commit process by sending a <code>PREPARE</code> request to all participating nodes (participants).</li><li><strong>Local Transaction Execution:</strong> Each participant performs all necessary operations associated with the transaction but does not commit them. This stage ensures that the transaction is ready to be committed.</li><li><strong>Vote Collection:</strong> Participants respond to the coordinator with a vote:<ul><li><strong><code>VOTE_COMMIT</code>:</strong> If the participant&#x27;s local operations are successful and it is ready to commit.</li><li><strong><code>VOTE_ABORT</code>:</strong> If the participant encounters any issues that prevent it from committing.</li></ul></li></ul></li><li><strong>Commit Phase (Decision Phase):</strong><ul><li><strong>Decision Making:</strong> The coordinator collects all votes from participants.<ul><li><strong>Global Commit:</strong> If all participants vote <code>VOTE_COMMIT</code>, the coordinator decides to commit the transaction.</li><li><strong>Global Abort:</strong> If any participant votes <code>VOTE_ABORT</code>, the coordinator decides to abort the transaction.</li></ul></li><li><strong>Notification:</strong> The coordinator sends a <code>GLOBAL_COMMIT</code> or <code>GLOBAL_ABORT</code> message to all participants based on the decision.</li><li><strong>Finalization:</strong> Participants act on the coordinator&#x27;s decision:<ul><li><strong>Commit:</strong> If the decision is <code>GLOBAL_COMMIT</code>, participants commit their local transactions.</li><li><strong>Rollback:</strong> If the decision is <code>GLOBAL_ABORT</code>, participants rollback any changes made during the transaction.</li></ul></li></ul></li></ol><p><strong>Key Features:</strong></p><ul><li><strong>Atomicity:</strong> Ensures that all nodes commit or none do, maintaining data integrity.</li><li><strong>Reliability:</strong> Handles failures by allowing participants to recover and reach a consistent state.</li><li><strong>Coordination:</strong> Centralizes decision-making through a coordinator, simplifying the commit process.</li></ul><p><strong>Potential Issues:</strong></p><ul><li><strong>Blocking Protocol:</strong> If the coordinator fails during the commit phase, participants may become blocked, waiting indefinitely.</li><li><strong>Single Point of Failure:</strong> The coordinator&#x27;s failure can halt the entire transaction process.</li><li><strong>Overhead:</strong> Increased message passing and logging can impact system performance.</li></ul><hr/><h3 class="content-header" id="two-phase-locking-2pl"><a class="break-words" href="#two-phase-locking-2pl" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a><strong>Two-Phase Locking (2PL)</strong></h3><p>To manage concurrent access to resources, preventing conflicts and ensuring data consistency.</p><p><strong>Introduction and Explanation:</strong></p><p>Two-Phase Locking (2PL) is a concurrency control method used in database systems to ensure serializability, which means transactions are executed in a manner equivalent to some serial execution. It achieves this by controlling how transactions acquire and release locks on data items.</p><p><strong>Phases of Two-Phase Locking:</strong></p><ol><li><strong>Growing Phase:</strong><ul><li><strong>Lock Acquisition:</strong> A transaction may acquire locks on data items it needs.</li><li><strong>No Lock Release:</strong> During this phase, the transaction does not release any locks.</li><li><strong>Purpose:</strong> Ensures that once a transaction starts acquiring locks, it continues to do so until it reaches its peak resource usage.</li></ul></li><li><strong>Shrinking Phase:</strong><ul><li><strong>Lock Release:</strong> The transaction starts releasing the locks it holds.</li><li><strong>No New Locks:</strong> No new locks can be acquired during this phase.</li><li><strong>Purpose:</strong> Guarantees that after a transaction starts releasing locks, it cannot acquire any more, preventing cyclic dependencies.</li></ul></li></ol><p><strong>Types of Locks:</strong></p><ul><li><strong>Exclusive Lock (Write Lock):</strong> Allows a transaction to both read and modify a data item. No other transaction can acquire any lock on that item.</li><li><strong>Shared Lock (Read Lock):</strong> Allows a transaction to read a data item. Other transactions can also acquire a shared lock on the same item but cannot write to it.</li></ul><p><strong>Key Features:</strong></p><ul><li><strong>Serializability:</strong> Ensures that concurrent transactions produce the same result as if they were executed serially.</li><li><strong>Strict 2PL Variant:</strong> A stricter version where all exclusive locks are held until the transaction commits or aborts, preventing cascading aborts.</li></ul><p><strong>Potential Issues:</strong></p><ul><li><strong>Deadlock Possibility:</strong> Transactions may become deadlocked if each waits for locks held by the other.</li><li><strong>Reduced Concurrency:</strong> Overly restrictive locking can limit parallelism, affecting system throughput.</li><li><strong>Complexity:</strong> Managing locks and detecting deadlocks adds complexity to the system.</li></ul><hr/><h3 class="content-header" id="paxos-replica-groups"><a class="break-words" href="#paxos-replica-groups" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a><strong>Paxos Replica Groups</strong></h3><p>To achieve consensus among replicas, allowing the system to function correctly even if some replicas fail.</p><p><strong>Introduction and Explanation:</strong></p><p>Paxos is a family of consensus algorithms designed to achieve agreement on a single value among distributed systems or replicas, even in the presence of failures. It ensures that a network of unreliable processors can agree on a value, making it fundamental for building fault-tolerant distributed systems.</p><p><strong>Roles in Paxos:</strong></p><ol><li><strong>Proposers:</strong><ul><li><strong>Function:</strong> Suggest values to be agreed upon.</li><li><strong>Responsibility:</strong> Initiate the consensus process by proposing values to acceptors.</li></ul></li><li><strong>Acceptors:</strong><ul><li><strong>Function:</strong> Decide which proposed value to accept.</li><li><strong>Responsibility:</strong> Participate in voting and ensure that once a value is chosen, it remains consistent.</li></ul></li><li><strong>Learners:</strong><ul><li><strong>Function:</strong> Learn the final agreed-upon value.</li><li><strong>Responsibility:</strong> Update their state based on the outcome of the consensus process.</li></ul></li></ol><p><strong>Phases of Paxos:</strong></p><ol><li><strong>Phase 1: Prepare Phase</strong><ul><li><strong>Proposal Number Generation:</strong> The proposer generates a unique proposal number <code>n</code>.</li><li><strong>Prepare Request:</strong> The proposer sends a <code>Prepare(n)</code> message to a majority of acceptors.</li><li><strong>Promise Response:</strong> Acceptors respond with a <code>Promise(n, accepted_n, accepted_value)</code> if <code>n</code> is greater than any previous proposal number they have seen.<ul><li><strong>No Further Acceptances:</strong> Acceptors promise not to accept any proposals numbered less than <code>n</code>.</li></ul></li></ul></li><li><strong>Phase 2: Accept Phase</strong><ul><li><strong>Proposal Value Selection:</strong> The proposer selects the value with the highest accepted proposal number from the acceptors&#x27; responses; if none, it can use its own value.</li><li><strong>Accept Request:</strong> The proposer sends an <code>Accept(n, value)</code> message to the acceptors.</li><li><strong>Acceptance:</strong> Acceptors accept the proposal if <code>n</code> matches their promised proposal number and persist the accepted value.</li><li><strong>Learning the Value:</strong> Acceptors inform learners about the accepted value, completing the consensus process.</li></ul></li></ol><div><img alt="Paxos Algorithm" loading="lazy" width="850" height="604" decoding="async" data-nimg="1" style="color:transparent" src="/static/images/paxos-flow.png"/></div><p><strong>Key Features:</strong></p><ul><li><strong>Fault Tolerance:</strong> Can tolerate failures of some acceptors as long as a majority can communicate.</li><li><strong>Consistency:</strong> Ensures that all non-faulty processes agree on the same value.</li><li><strong>Asynchronous Operation:</strong> Does not rely on synchronized clocks or timing assumptions.</li><li><strong>Safety Over Liveness:</strong> Prioritizes correctness even if progress (liveness) is temporarily hindered.</li></ul><p><strong>Potential Issues:</strong></p><ul><li><strong>Complexity:</strong> The algorithm is intricate, which can make implementation challenging.</li><li><strong>Performance Overhead:</strong> Multiple rounds of communication can impact performance in high-latency networks.</li><li><strong>Progress Under Failures:</strong> While safety is guaranteed, liveness can be affected if proposers continually fail.</li></ul><p><strong>Applications:</strong></p><ul><li><strong>Distributed Databases:</strong> Ensures consistency across database replicas.</li><li><strong>Distributed File Systems:</strong> Maintains file system metadata consistency.</li><li><strong>Coordination Services:</strong> Underpins services like Google&#x27;s Chubby and Apache ZooKeeper.</li></ul><hr/><h3 class="content-header" id="sharding-with-paxos-and-atomic-multi-key-transactions"><a class="break-words" href="#sharding-with-paxos-and-atomic-multi-key-transactions" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a><strong>Sharding with Paxos and Atomic Multi-Key Transactions</strong></h3><p>To add sharding of the key-value store across server groups (each of which implements Paxos), with dynamic load balancing and atomic (transactional) multi-key updates across server groups.</p><p><strong>Introduction and Explanation:</strong></p><p>Sharding is a database architecture pattern that partitions data across multiple servers, allowing the system to scale horizontally and handle increased load. In this context, sharding the key-value store distributes the data and workload across multiple server groups. Each server group manages a subset of the data (a shard) and uses Paxos to maintain consistency within the group.</p><p>The addition of dynamic load balancing and atomic multi-key updates introduces complexity, as transactions may span multiple shards. Implementing these features requires careful coordination to ensure data consistency and system reliability.</p><p><strong>Implementation Details:</strong></p><ol><li><strong>Data Sharding Across Server Groups:</strong><ul><li><strong>Partitioning Strategy:</strong><ul><li><strong>Hash-Based Sharding:</strong> Use a consistent hashing function to map keys to specific shards. This method evenly distributes data and simplifies the addition or removal of shards with minimal data reshuffling.</li><li><strong>Range-Based Sharding:</strong> Assign contiguous key ranges to different shards. While efficient for range queries, it may require more complex load balancing to prevent hotspots.</li></ul></li><li><strong>Shard Mapping:</strong><ul><li>Maintain a global shard map that keeps track of which keys or key ranges belong to which shards.</li><li>The shard map should be replicated across all nodes and clients to ensure consistent routing of requests.</li></ul></li></ul></li><li><strong>Server Groups Implementing Paxos:</strong><ul><li><strong>Replica Management:</strong><ul><li>Each shard is managed by a server group comprising multiple replicas.</li><li>Paxos is used within each group to achieve consensus on updates, ensuring consistency even if some replicas fail.</li></ul></li><li><strong>Leader Election:</strong><ul><li>Paxos facilitates leader election within each server group. The leader handles client requests and coordinates updates, improving efficiency.</li></ul></li><li><strong>Fault Tolerance:</strong><ul><li>The system tolerates failures as long as a majority of replicas within a group are operational.</li></ul></li></ul></li><li><strong>Dynamic Load Balancing:</strong><ul><li><strong>Monitoring Load Metrics:</strong><ul><li>Continuously monitor each shard&#x27;s load based on metrics like request rate, latency, and resource utilization.</li></ul></li><li><strong>Rebalancing Mechanisms:</strong><ul><li><strong>Shard Splitting:</strong> When a shard becomes overloaded, split it into smaller shards and redistribute the data across new server groups.</li><li><strong>Shard Merging:</strong> Underutilized shards can be merged to optimize resource usage.</li></ul></li><li><strong>Data Migration:</strong><ul><li>Migrate data between shards carefully to maintain consistency.</li><li>Use Paxos to coordinate data movement, ensuring that all replicas agree on the migration process.</li></ul></li><li><strong>Updating the Shard Map:</strong><ul><li>Atomically update the shard map during rebalancing to prevent inconsistencies.</li><li>Clients and server groups should be notified of shard map changes promptly.</li></ul></li></ul></li><li><strong>Atomic Multi-Key Updates Across Shards:</strong><ul><li><strong>Distributed Transactions:</strong><ul><li>Implement transactions that can span multiple shards while ensuring atomicity and consistency.</li><li>Transactions must be coordinated to either fully commit or fully abort across all involved shards.</li></ul></li><li><strong>Two-Phase Commit Protocol Across Shards:</strong><ul><li><strong>Phase 1 (Prepare):</strong><ul><li>A transaction coordinator sends a prepare request to all shards involved in the transaction.</li><li>Each shard executes the transaction locally and votes to commit or abort.</li></ul></li><li><strong>Phase 2 (Commit/Abort):</strong><ul><li>If all shards vote to commit, the coordinator sends a commit message; otherwise, it sends an abort message.</li><li>Shards finalize the transaction based on the coordinator&#x27;s decision.</li></ul></li></ul></li><li><strong>Concurrency Control:</strong><ul><li><strong>Two-Phase Locking (2PL):</strong><ul><li>Transactions acquire locks on required keys during the growing phase.</li><li>Locks are held until the transaction commits or aborts, preventing other transactions from conflicting.</li></ul></li><li><strong>Deadlock Detection:</strong><ul><li>Implement mechanisms to detect and resolve deadlocks that may occur due to distributed locking.</li></ul></li></ul></li></ul></li><li><strong>Client Interaction and Request Routing:</strong><ul><li><strong>Shard Map Utilization:</strong><ul><li>Clients use the shard map to determine the appropriate shard for each key.</li><li>For multi-key transactions, the client interacts with a transaction coordinator that manages communication with all relevant shards.</li></ul></li><li><strong>Handling Shard Map Changes:</strong><ul><li>Clients must handle updates to the shard map gracefully, possibly by fetching the latest version upon receiving a notification or encountering a routing error.</li></ul></li></ul></li><li><strong>System Components:</strong><ul><li><strong>Transaction Coordinator:</strong><ul><li>Manages distributed transactions across multiple shards.</li><li>Ensures atomicity by coordinating the two-phase commit process.</li></ul></li><li><strong>Shard Manager:</strong><ul><li>Oversees shard creation, splitting, merging, and deletion.</li><li>Maintains the shard map and handles its distribution to clients and server groups.</li></ul></li><li><strong>Load Balancer:</strong><ul><li>Monitors system performance and triggers rebalancing actions when necessary.</li><li>Works closely with the shard manager to redistribute data.</li></ul></li><li><strong>Logging and Recovery:</strong><ul><li>Implement logging mechanisms to record transaction states and shard configurations.</li><li>Facilitate recovery procedures in case of failures or crashes.</li></ul></li></ul></li></ol><p><strong>Key Considerations:</strong></p><ul><li><strong>Consistency Models:</strong><ul><li>Decide between strong consistency and eventual consistency based on application requirements.</li><li>Strong consistency ensures immediate visibility of writes but may impact performance.</li><li>Eventual consistency allows for higher availability and performance at the cost of temporary inconsistencies.</li></ul></li><li><strong>Scalability:</strong><ul><li>Design the system to scale horizontally by adding more server groups as data volume and traffic increase.</li><li>Ensure that the addition of new shards and server groups does not disrupt existing operations.</li></ul></li><li><strong>Fault Tolerance and High Availability:</strong><ul><li>Utilize replication and consensus algorithms (like Paxos) within server groups to handle server failures.</li><li>Implement failover mechanisms for transaction coordinators and shard managers.</li></ul></li><li><strong>Performance Optimization:</strong><ul><li>Minimize cross-shard transactions when possible, as they introduce overhead.</li><li>Cache shard map information on clients to reduce lookup latency.</li></ul></li><li><strong>Security and Access Control:</strong><ul><li>Implement authentication and authorization mechanisms to protect data.</li><li>Ensure secure communication channels between clients and servers.</li></ul></li></ul><h2 class="content-header" id="challenges-and-learnings"><a class="break-words" href="#challenges-and-learnings" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Challenges and Learnings</h2><p>Developing this system presented several challenges:</p><ul><li><strong>Concurrency Control</strong>: Implementing two-phase locking required careful handling of deadlocks and ensuring that locks were acquired and released in a manner that maintained system performance and consistency.</li><li><strong>Consensus Protocols</strong>: Integrating Paxos into the system necessitated a deep understanding of consensus algorithms and their practical implementation challenges, such as dealing with network partitions and ensuring liveness.</li><li><strong>Fault Tolerance</strong>: Ensuring the system could recover gracefully from various failure scenarios involved rigorous testing and validation of the replication and recovery mechanisms.</li></ul><p>Through this project, I gained hands-on experience with the complexities of building distributed systems, particularly in achieving fault tolerance and consistency in a sharded environment.</p><h2 class="content-header" id="conclusion"><a class="break-words" href="#conclusion" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Conclusion</h2><p>The development of a fault-tolerant, sharded KV storage system in CSE 452 provided invaluable insights into the design and implementation of distributed systems. By integrating exactly-once RPC mechanisms, primary-backup architectures with Viewstamped Replication, and multi-Paxos replica systems, I was able to create a robust and scalable storage solution. This experience has deepened my understanding of distributed consensus, fault tolerance, and the practical challenges of building reliable distributed applications.</p><h2 class="content-header" id="more-reading"><a class="break-words" href="#more-reading" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>More Reading</h2><ul><li><a class="break-words" href="/static/papers/lamport-paxos.pdf">Lamport&#x27;s Paxos Algorithm</a></li><li><a class="break-words" href="/static/papers/pmmc.pdf">Paxos Made Moderately Complex</a></li><li><a class="break-words" href="/static/papers/viewstamped-replication.pdf">Viewstamped Replication</a></li></ul></div></div><footer><div class="divide-gray-200 text-sm font-medium leading-5 dark:divide-gray-700 xl:col-start-1 xl:row-start-2 xl:divide-y"><div class="py-4 xl:py-8"><h2 class="text-xs uppercase tracking-wide text-gray-500 dark:text-gray-400">Tags</h2><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/distributed-systems">Distributed-Systems</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/paxos">Paxos</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/java">Java</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/cse-452">CSE-452</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/university-of-washington">University-of-Washington</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/key-value-store">Key-Value-Store</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/fault-tolerance">Fault-Tolerance</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/replication">Replication</a></div></div><div class="flex justify-between py-4 xl:block xl:space-y-8 xl:py-8"><div><h2 class="text-xs uppercase tracking-wide text-gray-500 dark:text-gray-400">Previous Article</h2><div class="text-primary-500 hover:text-primary-600 dark:hover:text-primary-400"><a class="break-words" href="/projects/nft-market">Decentralized GameNFT Marketplace</a></div></div></div></div><div class="pt-4 xl:pt-8"><a class="text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" aria-label="Back to the projects" href="/projects">← Back to the projects</a></div></footer></div></div></article></section></main><footer><div class="mt-16 flex flex-col items-center"><div class="mb-2 flex space-x-2 text-sm text-gray-500 dark:text-gray-400"><div>Bowen Y</div><div> • </div><div>© 2024</div><div> • </div><a class="break-words" href="/">CodeWisdom</a></div></div></footer></section><script src="/_next/static/chunks/webpack-8a3a6504f64badee.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/2d141e1a38819612-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/5beba742ae93b864.css\",\"style\"]\n3:HL[\"/_next/static/css/f719ff1083adf929.css\",\"style\"]\n4:HL[\"/_next/static/css/e36d2c8a31781d2e.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"5:I[5751,[],\"\"]\n8:I[9275,[],\"\"]\na:I[1343,[],\"\"]\nb:I[8700,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"113\",\"static/chunks/113-8a0c0daa2bfc088c.js\",\"185\",\"static/chunks/app/layout-a7a2551425c87cc3.js\"],\"ThemeProviders\"]\nc:I[4080,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"113\",\"static/chunks/113-8a0c0daa2bfc088c.js\",\"185\",\"static/chunks/app/layout-a7a2551425c87cc3.js\"],\"\"]\nd:I[9032,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"113\",\"static/chunks/113-8a0c0daa2bfc088c.js\",\"185\",\"static/chunks/app/layout-a7a2551425c87cc3.js\"],\"KBarSearchProvider\"]\ne:I[231,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"173\",\"static/chunks/173-af5b99c330035292.js\",\"459\",\"static/chunks/459-6c5dcdc51f43b997.js\",\"785\",\"static/chunks/app/projects/%5B...slug%5D/page-b93535745cdfa673.js\"],\"\"]\nf:I[509,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"113\",\"static/chunks/113-8a0c0daa2bfc088c.js\",\"185\",\"static/chunks/app/layout-a7a2551425c87cc3.js\"],\"KBarButton\"]\n10:I[1398,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"113\",\"static/chunks/113-8a0c0daa2bfc088c.js\",\"185\",\"static/chunks/app/layout-a7a2551425c87cc3.js\"],\"default\"]\n11:I[8976,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"113\",\"static/chunks/113-8a0c0daa2bfc088c.js\",\"185\",\"static/chunks/app/layout-a7a2551425c87cc3.js\"],\"default\"]\n13:I[6130,[],\"\"]\n9:[\"slug\",\"paxos\",\"c\"]\n14:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5beba742ae93b864.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/f719ff1083adf929.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"buildId\":\"gPZ8ET5SZwyuIVWxpISNr\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/projects/paxos\",\"initialTree\":[\"\",{\"children\":[\"projects\",{\"children\":[[\"slug\",\"paxos\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"paxos\\\"]}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"projects\",{\"children\":[[\"slug\",\"paxos\",\"c\"],{\"children\":[\"__PAGE__\",{},[[\"$L6\",\"$L7\"],null],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"projects\",\"children\",\"$9\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/e36d2c8a31781d2e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]}],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"projects\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"en-us\",\"className\":\"__variable_0aa4ae scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"link\",null,{\"rel\":\"apple-touch-icon\",\"sizes\":\"76x76\",\"href\":\"/static/favicons/logo-round.png\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"sizes\":\"32x32\",\"href\":\"/static/favicons/logo-round.png\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"sizes\":\"16x16\",\"href\":\"/static/favicons/logo-round.png\"}],[\"$\",\"link\",null,{\"rel\":\"manifest\",\"href\":\"/static/favicons/site.webmanifest\"}],[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#000000\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: light)\",\"content\":\"#fff\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: dark)\",\"content\":\"#000\"}],[\"$\",\"link\",null,{\"rel\":\"alternate\",\"type\":\"application/rss+xml\",\"href\":\"/feed.xml\"}],[\"$\",\"body\",null,{\"className\":\"bg-white pl-[calc(100vw-100%)] text-black antialiased dark:bg-gray-950 dark:text-white\",\"children\":[\"$\",\"$Lb\",null,{\"children\":[[\"$undefined\",\"$undefined\",\"$undefined\",[\"$\",\"$Lc\",null,{\"async\":true,\"defer\":true,\"data-website-id\":\"$undefined\",\"src\":\"https://analytics.umami.is/script.js\"}],\"$undefined\",\"$undefined\"],[\"$\",\"section\",null,{\"className\":\"mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0\",\"children\":[[\"$\",\"$Ld\",null,{\"kbarConfig\":{\"searchDocumentsPath\":\"/search.json\"},\"children\":[[\"$\",\"header\",null,{\"className\":\"flex items-center w-full bg-white dark:bg-gray-950 justify-between py-10\",\"children\":[[\"$\",\"$Le\",null,{\"className\":\"break-words\",\"href\":\"/\",\"aria-label\":\"CodeWisdom\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-between\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mr-3\",\"children\":[\"$\",\"img\",null,{\"src\":\"/_next/static/media/logo.a3d92d8f.png\",\"alt\":\"logo\",\"className\":\"h-10 w-14\"}]}],[\"$\",\"div\",null,{\"className\":\"hidden h-6 text-2xl font-semibold sm:block\",\"children\":\"CodeWisdom\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-4 leading-5 sm:space-x-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"no-scrollbar hidden max-w-40 items-center space-x-4 overflow-x-auto sm:flex sm:space-x-6 md:max-w-72 lg:max-w-96\",\"children\":[[\"$\",\"$Le\",null,{\"className\":\"block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400\",\"href\":\"/blog\",\"children\":\"Blog\"}],[\"$\",\"$Le\",null,{\"className\":\"block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400\",\"href\":\"/tags\",\"children\":\"Tags\"}],[\"$\",\"$Le\",null,{\"className\":\"block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400\",\"href\":\"/projects\",\"children\":\"Projects\"}],[\"$\",\"$Le\",null,{\"className\":\"block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400\",\"href\":\"/about\",\"children\":\"About\"}]]}],[\"$\",\"$Lf\",null,{\"aria-label\":\"Search\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 24 24\",\"strokeWidth\":1.5,\"stroke\":\"currentColor\",\"className\":\"h-6 w-6 text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 10.607z\"}]}]}],[\"$\",\"$L10\",null,{}],[\"$\",\"$L11\",null,{}]]}]]}],[\"$\",\"main\",null,{\"className\":\"mb-auto\",\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-start justify-start md:mt-24 md:flex-row md:items-center md:justify-center md:space-x-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"space-x-2 pb-8 pt-6 md:space-y-5\",\"children\":[\"$\",\"h1\",null,{\"className\":\"text-6xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 md:border-r-2 md:px-6 md:text-8xl md:leading-14\",\"children\":\"404\"}]}],[\"$\",\"div\",null,{\"className\":\"max-w-md\",\"children\":[[\"$\",\"p\",null,{\"className\":\"mb-4 text-xl font-bold leading-normal md:text-2xl\",\"children\":\"Sorry we couldn't find this page.\"}],[\"$\",\"p\",null,{\"className\":\"mb-8\",\"children\":\"But dont worry, you can find plenty of other things on our homepage.\"}],[\"$\",\"$Le\",null,{\"className\":\"focus:shadow-outline-blue inline rounded-lg border border-transparent bg-blue-600 px-4 py-2 text-sm font-medium leading-5 text-white shadow transition-colors duration-150 hover:bg-blue-700 focus:outline-none dark:hover:bg-blue-500\",\"href\":\"/\",\"children\":\"Back to homepage\"}]]}]]}],\"notFoundStyles\":[],\"styles\":null}]}]]}],[\"$\",\"footer\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"mt-16 flex flex-col items-center\",\"children\":[\"$\",\"div\",null,{\"className\":\"mb-2 flex space-x-2 text-sm text-gray-500 dark:text-gray-400\",\"children\":[[\"$\",\"div\",null,{\"children\":\"Bowen Y\"}],[\"$\",\"div\",null,{\"children\":\" • \"}],[\"$\",\"div\",null,{\"children\":\"© 2024\"}],[\"$\",\"div\",null,{\"children\":\" • \"}],[\"$\",\"$Le\",null,{\"className\":\"break-words\",\"href\":\"/\",\"children\":\"CodeWisdom\"}]]}]}]}]]}]]}]}]]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[false,\"$L12\"],\"globalErrorComponent\":\"$13\",\"missingSlots\":\"$W14\"}]]\n"])</script><script>self.__next_f.push([1,"15:I[4347,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"173\",\"static/chunks/173-af5b99c330035292.js\",\"459\",\"static/chunks/459-6c5dcdc51f43b997.js\",\"785\",\"static/chunks/app/projects/%5B...slug%5D/page-b93535745cdfa673.js\"],\"default\"]\n16:I[8173,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"173\",\"static/chunks/173-af5b99c330035292.js\",\"459\",\"static/chunks/459-6c5dcdc51f43b997.js\",\"785\",\"static/chunks/app/projects/%5B...slug%5D/page-b93535745cdfa673.js\"],\"Image\"]\n"])</script><script>self.__next_f.push([1,"7:[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Project\\\",\\\"headline\\\":\\\"Building a Fault-Tolerant Sharded Key-Value Store with Paxos\\\",\\\"datePublished\\\":\\\"2024-11-07T00:00:00.000Z\\\",\\\"dateModified\\\":\\\"2024-11-07T00:00:00.000Z\\\",\\\"image\\\":\\\"/static/images/logo-round.png\\\",\\\"url\\\":\\\"https://codewisdom.io/projects/paxos\\\",\\\"author\\\":[{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"Bowen Y\\\"}]}\"}}],[\"$\",\"section\",null,{\"className\":\"mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0\",\"children\":[[\"$\",\"$L15\",null,{}],[\"$\",\"article\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"xl:divide-y xl:divide-gray-200 xl:dark:divide-gray-700\",\"children\":[[\"$\",\"header\",null,{\"className\":\"pt-6 xl:pb-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-1 text-center\",\"children\":[[\"$\",\"dl\",null,{\"className\":\"space-y-10\",\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"dt\",null,{\"className\":\"sr-only\",\"children\":\"Published on\"}],[\"$\",\"dd\",null,{\"className\":\"text-base font-medium leading-6 text-gray-500 dark:text-gray-400\",\"children\":[\"$\",\"time\",null,{\"dateTime\":\"2024-11-07T00:00:00.000Z\",\"children\":\"Wednesday, November 6, 2024\"}]}]]}]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"h1\",null,{\"className\":\"text-3xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 sm:text-4xl sm:leading-10 md:text-5xl md:leading-14\",\"children\":\"Building a Fault-Tolerant Sharded Key-Value Store with Paxos\"}]}]]}]}],[\"$\",\"div\",null,{\"className\":\"grid-rows-[auto_1fr] divide-y divide-gray-200 pb-8 dark:divide-gray-700 xl:grid xl:grid-cols-4 xl:gap-x-6 xl:divide-y-0\",\"children\":[[\"$\",\"dl\",null,{\"className\":\"pb-10 pt-6 xl:border-b xl:border-gray-200 xl:pt-11 xl:dark:border-gray-700\",\"children\":[[\"$\",\"dt\",null,{\"className\":\"sr-only\",\"children\":\"Authors\"}],[\"$\",\"dd\",null,{\"children\":[\"$\",\"ul\",null,{\"className\":\"flex flex-wrap justify-center gap-4 sm:space-x-12 xl:block xl:space-x-0 xl:space-y-8\",\"children\":[[\"$\",\"li\",\"Bowen Y\",{\"className\":\"flex items-center space-x-2\",\"children\":[[\"$\",\"$L16\",null,{\"src\":\"/static/images/pixel-avatar.png\",\"width\":38,\"height\":38,\"alt\":\"avatar\",\"className\":\"h-10 w-10 rounded-full\"}],[\"$\",\"dl\",null,{\"className\":\"whitespace-nowrap text-sm font-medium leading-5\",\"children\":[[\"$\",\"dt\",null,{\"className\":\"sr-only\",\"children\":\"Name\"}],[\"$\",\"dd\",null,{\"className\":\"text-gray-900 dark:text-gray-100\",\"children\":\"Bowen Y\"}],[\"$\",\"dt\",null,{\"className\":\"sr-only\",\"children\":\"Twitter\"}],[\"$\",\"dd\",null,{\"children\":\"$undefined\"}]]}]]}]]}]}]]}],[\"$\",\"div\",null,{\"className\":\"divide-y divide-gray-200 dark:divide-gray-700 xl:col-span-3 xl:row-span-2 xl:pb-0\",\"children\":[\"$\",\"div\",null,{\"className\":\"prose max-w-none pb-8 pt-10 dark:prose-invert\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"project-overview\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#project-overview\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Project Overview\"]}],[\"$\",\"p\",null,{\"children\":\"In the context of the University of Washington's CSE 452 course on Distributed Systems, I developed a fault-tolerant, sharded key-value (KV) storage system in Java. This project involved implementing an exactly-once Remote Procedure Call (RPC) mechanism, a primary-backup server architecture utilizing Viewstamped Replication for fault tolerance, and a sophisticated multi-Paxos replica system. The design integrated two-phase commit, two-phase locking, and Paxos replica groups to create a simplified, Spanner-like sharded KV-store system.\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"course-context-cse-452---distributed-systems\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#course-context-cse-452---distributed-systems\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Course Context: CSE 452 - Distributed Systems\"]}],[\"$\",\"p\",null,{\"children\":\"CSE 452 is a senior-level course at the University of Washington that covers abstractions and implementation techniques for constructing distributed systems. Topics include client-server computing, cloud computing, peer-to-peer systems, and distributed storage systems. The course emphasizes hands-on projects to reinforce concepts such as remote procedure calls, consistency maintenance, fault tolerance, high availability, and scalability.\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"implementation-overview\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#implementation-overview\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Implementation Overview\"]}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"1-exactly-once-rpc-mechanism-and-primary-backup-server\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#1-exactly-once-rpc-mechanism-and-primary-backup-server\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"1. Exactly-Once RPC Mechanism and Primary-Backup Server\"]}],[\"$\",\"p\",null,{\"children\":\"To ensure reliable communication between clients and servers, I implemented an exactly-once RPC mechanism. This approach guarantees that each RPC is executed precisely once, even in the presence of network failures or retries. The primary-backup server architecture was enhanced using Viewstamped Replication, which provides fault tolerance by maintaining consistency between the primary server and its backups. This setup ensures that the system can recover from failures without data loss or inconsistency.\"}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"2-multi-paxos-replica-system\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#2-multi-paxos-replica-system\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"2. Multi-Paxos Replica System\"]}],[\"$\",\"p\",null,{\"children\":\"Building upon the primary-backup architecture, I designed a multi-Paxos replica system to handle more complex scenarios involving multiple replicas. This system employed the PMMC (Paxos Made Moderately Complex) design, integrating:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Two-Phase Commit\"}],\": To coordinate transactions across multiple nodes, ensuring all or none of the operations are committed.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Two-Phase Locking\"}],\": To manage concurrent access to resources, preventing conflicts and ensuring data consistency.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Paxos Replica Groups\"}],\": To achieve consensus among replicas, allowing the system to function correctly even if some replicas fail.\"]}]]}],[\"$\",\"p\",null,{\"children\":\"This combination enabled the creation of a sharded KV-store system with properties similar to Google's Spanner, providing scalability and strong consistency across distributed nodes.\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"implementation-details\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#implementation-details\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Implementation Details\"]}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"two-phase-commit-2pc\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#two-phase-commit-2pc\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],[\"$\",\"strong\",null,{\"children\":\"Two-Phase Commit (2PC)\"}]]}],[\"$\",\"p\",null,{\"children\":\"To coordinate transactions across multiple nodes, ensuring all or none of the operations are committed.\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Introduction and Explanation:\"}]}],[\"$\",\"p\",null,{\"children\":\"The Two-Phase Commit (2PC) protocol is a distributed algorithm that ensures all participating nodes in a distributed transaction agree on a common commit or abort decision. It is widely used in distributed database systems and transaction processing systems to maintain atomicity and consistency across multiple nodes or databases.\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Phases of Two-Phase Commit:\"}]}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Prepare Phase (Voting Phase):\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Initiation:\"}],\" The coordinator (transaction manager) initiates the commit process by sending a \",[\"$\",\"code\",null,{\"children\":\"PREPARE\"}],\" request to all participating nodes (participants).\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Local Transaction Execution:\"}],\" Each participant performs all necessary operations associated with the transaction but does not commit them. This stage ensures that the transaction is ready to be committed.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Vote Collection:\"}],\" Participants respond to the coordinator with a vote:\",[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"VOTE_COMMIT\"}],\":\"]}],\" If the participant's local operations are successful and it is ready to commit.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"VOTE_ABORT\"}],\":\"]}],\" If the participant encounters any issues that prevent it from committing.\"]}]]}]]}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Commit Phase (Decision Phase):\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Decision Making:\"}],\" The coordinator collects all votes from participants.\",[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Global Commit:\"}],\" If all participants vote \",[\"$\",\"code\",null,{\"children\":\"VOTE_COMMIT\"}],\", the coordinator decides to commit the transaction.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Global Abort:\"}],\" If any participant votes \",[\"$\",\"code\",null,{\"children\":\"VOTE_ABORT\"}],\", the coordinator decides to abort the transaction.\"]}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Notification:\"}],\" The coordinator sends a \",[\"$\",\"code\",null,{\"children\":\"GLOBAL_COMMIT\"}],\" or \",[\"$\",\"code\",null,{\"children\":\"GLOBAL_ABORT\"}],\" message to all participants based on the decision.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Finalization:\"}],\" Participants act on the coordinator's decision:\",[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Commit:\"}],\" If the decision is \",[\"$\",\"code\",null,{\"children\":\"GLOBAL_COMMIT\"}],\", participants commit their local transactions.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Rollback:\"}],\" If the decision is \",[\"$\",\"code\",null,{\"children\":\"GLOBAL_ABORT\"}],\", participants rollback any changes made during the transaction.\"]}]]}]]}]]}]]}]]}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Key Features:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Atomicity:\"}],\" Ensures that all nodes commit or none do, maintaining data integrity.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Reliability:\"}],\" Handles failures by allowing participants to recover and reach a consistent state.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Coordination:\"}],\" Centralizes decision-making through a coordinator, simplifying the commit process.\"]}]]}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Potential Issues:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Blocking Protocol:\"}],\" If the coordinator fails during the commit phase, participants may become blocked, waiting indefinitely.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Single Point of Failure:\"}],\" The coordinator's failure can halt the entire transaction process.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Overhead:\"}],\" Increased message passing and logging can impact system performance.\"]}]]}],[\"$\",\"hr\",null,{}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"two-phase-locking-2pl\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#two-phase-locking-2pl\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],[\"$\",\"strong\",null,{\"children\":\"Two-Phase Locking (2PL)\"}]]}],[\"$\",\"p\",null,{\"children\":\"To manage concurrent access to resources, preventing conflicts and ensuring data consistency.\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Introduction and Explanation:\"}]}],[\"$\",\"p\",null,{\"children\":\"Two-Phase Locking (2PL) is a concurrency control method used in database systems to ensure serializability, which means transactions are executed in a manner equivalent to some serial execution. It achieves this by controlling how transactions acquire and release locks on data items.\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Phases of Two-Phase Locking:\"}]}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Growing Phase:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Lock Acquisition:\"}],\" A transaction may acquire locks on data items it needs.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"No Lock Release:\"}],\" During this phase, the transaction does not release any locks.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Purpose:\"}],\" Ensures that once a transaction starts acquiring locks, it continues to do so until it reaches its peak resource usage.\"]}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Shrinking Phase:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Lock Release:\"}],\" The transaction starts releasing the locks it holds.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"No New Locks:\"}],\" No new locks can be acquired during this phase.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Purpose:\"}],\" Guarantees that after a transaction starts releasing locks, it cannot acquire any more, preventing cyclic dependencies.\"]}]]}]]}]]}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Types of Locks:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Exclusive Lock (Write Lock):\"}],\" Allows a transaction to both read and modify a data item. No other transaction can acquire any lock on that item.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Shared Lock (Read Lock):\"}],\" Allows a transaction to read a data item. Other transactions can also acquire a shared lock on the same item but cannot write to it.\"]}]]}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Key Features:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Serializability:\"}],\" Ensures that concurrent transactions produce the same result as if they were executed serially.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Strict 2PL Variant:\"}],\" A stricter version where all exclusive locks are held until the transaction commits or aborts, preventing cascading aborts.\"]}]]}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Potential Issues:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Deadlock Possibility:\"}],\" Transactions may become deadlocked if each waits for locks held by the other.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Reduced Concurrency:\"}],\" Overly restrictive locking can limit parallelism, affecting system throughput.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Complexity:\"}],\" Managing locks and detecting deadlocks adds complexity to the system.\"]}]]}],[\"$\",\"hr\",null,{}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"paxos-replica-groups\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#paxos-replica-groups\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],[\"$\",\"strong\",null,{\"children\":\"Paxos Replica Groups\"}]]}],[\"$\",\"p\",null,{\"children\":\"To achieve consensus among replicas, allowing the system to function correctly even if some replicas fail.\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Introduction and Explanation:\"}]}],[\"$\",\"p\",null,{\"children\":\"Paxos is a family of consensus algorithms designed to achieve agreement on a single value among distributed systems or replicas, even in the presence of failures. It ensures that a network of unreliable processors can agree on a value, making it fundamental for building fault-tolerant distributed systems.\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Roles in Paxos:\"}]}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Proposers:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Function:\"}],\" Suggest values to be agreed upon.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Responsibility:\"}],\" Initiate the consensus process by proposing values to acceptors.\"]}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Acceptors:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Function:\"}],\" Decide which proposed value to accept.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Responsibility:\"}],\" Participate in voting and ensure that once a value is chosen, it remains consistent.\"]}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Learners:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Function:\"}],\" Learn the final agreed-upon value.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Responsibility:\"}],\" Update their state based on the outcome of the consensus process.\"]}]]}]]}]]}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Phases of Paxos:\"}]}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Phase 1: Prepare Phase\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Proposal Number Generation:\"}],\" The proposer generates a unique proposal number \",[\"$\",\"code\",null,{\"children\":\"n\"}],\".\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Prepare Request:\"}],\" The proposer sends a \",[\"$\",\"code\",null,{\"children\":\"Prepare(n)\"}],\" message to a majority of acceptors.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Promise Response:\"}],\" Acceptors respond with a \",[\"$\",\"code\",null,{\"children\":\"Promise(n, accepted_n, accepted_value)\"}],\" if \",[\"$\",\"code\",null,{\"children\":\"n\"}],\" is greater than any previous proposal number they have seen.\",[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"No Further Acceptances:\"}],\" Acceptors promise not to accept any proposals numbered less than \",[\"$\",\"code\",null,{\"children\":\"n\"}],\".\"]}]}]]}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Phase 2: Accept Phase\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Proposal Value Selection:\"}],\" The proposer selects the value with the highest accepted proposal number from the acceptors' responses; if none, it can use its own value.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Accept Request:\"}],\" The proposer sends an \",[\"$\",\"code\",null,{\"children\":\"Accept(n, value)\"}],\" message to the acceptors.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Acceptance:\"}],\" Acceptors accept the proposal if \",[\"$\",\"code\",null,{\"children\":\"n\"}],\" matches their promised proposal number and persist the accepted value.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Learning the Value:\"}],\" Acceptors inform learners about the accepted value, completing the consensus process.\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"$L16\",null,{\"src\":\"/static/images/paxos-flow.png\",\"alt\":\"Paxos Algorithm\",\"width\":\"850\",\"height\":\"604\"}]}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Key Features:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Fault Tolerance:\"}],\" Can tolerate failures of some acceptors as long as a majority can communicate.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Consistency:\"}],\" Ensures that all non-faulty processes agree on the same value.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Asynchronous Operation:\"}],\" Does not rely on synchronized clocks or timing assumptions.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Safety Over Liveness:\"}],\" Prioritizes correctness even if progress (liveness) is temporarily hindered.\"]}]]}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Potential Issues:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Complexity:\"}],\" The algorithm is intricate, which can make implementation challenging.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Performance Overhead:\"}],\" Multiple rounds of communication can impact performance in high-latency networks.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Progress Under Failures:\"}],\" While safety is guaranteed, liveness can be affected if proposers continually fail.\"]}]]}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Applications:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Distributed Databases:\"}],\" Ensures consistency across database replicas.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Distributed File Systems:\"}],\" Maintains file system metadata consistency.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Coordination Services:\"}],\" Underpins services like Google's Chubby and Apache ZooKeeper.\"]}]]}],[\"$\",\"hr\",null,{}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"sharding-with-paxos-and-atomic-multi-key-transactions\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#sharding-with-paxos-and-atomic-multi-key-transactions\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],[\"$\",\"strong\",null,{\"children\":\"Sharding with Paxos and Atomic Multi-Key Transactions\"}]]}],[\"$\",\"p\",null,{\"children\":\"To add sharding of the key-value store across server groups (each of which implements Paxos), with dynamic load balancing and atomic (transactional) multi-key updates across server groups.\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Introduction and Explanation:\"}]}],[\"$\",\"p\",null,{\"children\":\"Sharding is a database architecture pattern that partitions data across multiple servers, allowing the system to scale horizontally and handle increased load. In this context, sharding the key-value store distributes the data and workload across multiple server groups. Each server group manages a subset of the data (a shard) and uses Paxos to maintain consistency within the group.\"}],[\"$\",\"p\",null,{\"children\":\"The addition of dynamic load balancing and atomic multi-key updates introduces complexity, as transactions may span multiple shards. Implementing these features requires careful coordination to ensure data consistency and system reliability.\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Implementation Details:\"}]}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Data Sharding Across Server Groups:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Partitioning Strategy:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Hash-Based Sharding:\"}],\" Use a consistent hashing function to map keys to specific shards. This method evenly distributes data and simplifies the addition or removal of shards with minimal data reshuffling.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Range-Based Sharding:\"}],\" Assign contiguous key ranges to different shards. While efficient for range queries, it may require more complex load balancing to prevent hotspots.\"]}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Shard Mapping:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Maintain a global shard map that keeps track of which keys or key ranges belong to which shards.\"}],[\"$\",\"li\",null,{\"children\":\"The shard map should be replicated across all nodes and clients to ensure consistent routing of requests.\"}]]}]]}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Server Groups Implementing Paxos:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Replica Management:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Each shard is managed by a server group comprising multiple replicas.\"}],[\"$\",\"li\",null,{\"children\":\"Paxos is used within each group to achieve consensus on updates, ensuring consistency even if some replicas fail.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Leader Election:\"}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"Paxos facilitates leader election within each server group. The leader handles client requests and coordinates updates, improving efficiency.\"}]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Fault Tolerance:\"}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"The system tolerates failures as long as a majority of replicas within a group are operational.\"}]}]]}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Dynamic Load Balancing:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Monitoring Load Metrics:\"}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"Continuously monitor each shard's load based on metrics like request rate, latency, and resource utilization.\"}]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Rebalancing Mechanisms:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Shard Splitting:\"}],\" When a shard becomes overloaded, split it into smaller shards and redistribute the data across new server groups.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Shard Merging:\"}],\" Underutilized shards can be merged to optimize resource usage.\"]}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Data Migration:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Migrate data between shards carefully to maintain consistency.\"}],[\"$\",\"li\",null,{\"children\":\"Use Paxos to coordinate data movement, ensuring that all replicas agree on the migration process.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Updating the Shard Map:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Atomically update the shard map during rebalancing to prevent inconsistencies.\"}],[\"$\",\"li\",null,{\"children\":\"Clients and server groups should be notified of shard map changes promptly.\"}]]}]]}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Atomic Multi-Key Updates Across Shards:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Distributed Transactions:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Implement transactions that can span multiple shards while ensuring atomicity and consistency.\"}],[\"$\",\"li\",null,{\"children\":\"Transactions must be coordinated to either fully commit or fully abort across all involved shards.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Two-Phase Commit Protocol Across Shards:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Phase 1 (Prepare):\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"A transaction coordinator sends a prepare request to all shards involved in the transaction.\"}],[\"$\",\"li\",null,{\"children\":\"Each shard executes the transaction locally and votes to commit or abort.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Phase 2 (Commit/Abort):\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"If all shards vote to commit, the coordinator sends a commit message; otherwise, it sends an abort message.\"}],[\"$\",\"li\",null,{\"children\":\"Shards finalize the transaction based on the coordinator's decision.\"}]]}]]}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Concurrency Control:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Two-Phase Locking (2PL):\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Transactions acquire locks on required keys during the growing phase.\"}],[\"$\",\"li\",null,{\"children\":\"Locks are held until the transaction commits or aborts, preventing other transactions from conflicting.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Deadlock Detection:\"}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"Implement mechanisms to detect and resolve deadlocks that may occur due to distributed locking.\"}]}]]}]]}]]}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Client Interaction and Request Routing:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Shard Map Utilization:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Clients use the shard map to determine the appropriate shard for each key.\"}],[\"$\",\"li\",null,{\"children\":\"For multi-key transactions, the client interacts with a transaction coordinator that manages communication with all relevant shards.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Handling Shard Map Changes:\"}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"Clients must handle updates to the shard map gracefully, possibly by fetching the latest version upon receiving a notification or encountering a routing error.\"}]}]]}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"System Components:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Transaction Coordinator:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Manages distributed transactions across multiple shards.\"}],[\"$\",\"li\",null,{\"children\":\"Ensures atomicity by coordinating the two-phase commit process.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Shard Manager:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Oversees shard creation, splitting, merging, and deletion.\"}],[\"$\",\"li\",null,{\"children\":\"Maintains the shard map and handles its distribution to clients and server groups.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Load Balancer:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Monitors system performance and triggers rebalancing actions when necessary.\"}],[\"$\",\"li\",null,{\"children\":\"Works closely with the shard manager to redistribute data.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Logging and Recovery:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Implement logging mechanisms to record transaction states and shard configurations.\"}],[\"$\",\"li\",null,{\"children\":\"Facilitate recovery procedures in case of failures or crashes.\"}]]}]]}]]}]]}]]}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Key Considerations:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Consistency Models:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Decide between strong consistency and eventual consistency based on application requirements.\"}],[\"$\",\"li\",null,{\"children\":\"Strong consistency ensures immediate visibility of writes but may impact performance.\"}],[\"$\",\"li\",null,{\"children\":\"Eventual consistency allows for higher availability and performance at the cost of temporary inconsistencies.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Scalability:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Design the system to scale horizontally by adding more server groups as data volume and traffic increase.\"}],[\"$\",\"li\",null,{\"children\":\"Ensure that the addition of new shards and server groups does not disrupt existing operations.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Fault Tolerance and High Availability:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Utilize replication and consensus algorithms (like Paxos) within server groups to handle server failures.\"}],[\"$\",\"li\",null,{\"children\":\"Implement failover mechanisms for transaction coordinators and shard managers.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Performance Optimization:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Minimize cross-shard transactions when possible, as they introduce overhead.\"}],[\"$\",\"li\",null,{\"children\":\"Cache shard map information on clients to reduce lookup latency.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Security and Access Control:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Implement authentication and authorization mechanisms to protect data.\"}],[\"$\",\"li\",null,{\"children\":\"Ensure secure communication channels between clients and servers.\"}]]}]]}]]}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"challenges-and-learnings\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#challenges-and-learnings\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Challenges and Learnings\"]}],[\"$\",\"p\",null,{\"children\":\"Developing this system presented several challenges:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Concurrency Control\"}],\": Implementing two-phase locking required careful handling of deadlocks and ensuring that locks were acquired and released in a manner that maintained system performance and consistency.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Consensus Protocols\"}],\": Integrating Paxos into the system necessitated a deep understanding of consensus algorithms and their practical implementation challenges, such as dealing with network partitions and ensuring liveness.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Fault Tolerance\"}],\": Ensuring the system could recover gracefully from various failure scenarios involved rigorous testing and validation of the replication and recovery mechanisms.\"]}]]}],[\"$\",\"p\",null,{\"children\":\"Through this project, I gained hands-on experience with the complexities of building distributed systems, particularly in achieving fault tolerance and consistency in a sharded environment.\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"conclusion\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#conclusion\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Conclusion\"]}],[\"$\",\"p\",null,{\"children\":\"The development of a fault-tolerant, sharded KV storage system in CSE 452 provided invaluable insights into the design and implementation of distributed systems. By integrating exactly-once RPC mechanisms, primary-backup architectures with Viewstamped Replication, and multi-Paxos replica systems, I was able to create a robust and scalable storage solution. This experience has deepened my understanding of distributed consensus, fault tolerance, and the practical challenges of building reliable distributed applications.\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"more-reading\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#more-reading\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"More Reading\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$Le\",null,{\"className\":\"break-words\",\"href\":\"/static/papers/lamport-paxos.pdf\",\"children\":\"Lamport's Paxos Algorithm\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$Le\",null,{\"className\":\"break-words\",\"href\":\"/static/papers/pmmc.pdf\",\"children\":\"Paxos Made Moderately Complex\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$Le\",null,{\"className\":\"break-words\",\"href\":\"/static/papers/viewstamped-replication.pdf\",\"children\":\"Viewstamped Replication\"}]}]]}]]}]}],[\"$\",\"footer\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"divide-gray-200 text-sm font-medium leading-5 dark:divide-gray-700 xl:col-start-1 xl:row-start-2 xl:divide-y\",\"children\":[[\"$\",\"div\",null,{\"className\":\"py-4 xl:py-8\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xs uppercase tracking-wide text-gray-500 dark:text-gray-400\",\"children\":\"Tags\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap\",\"children\":[[\"$\",\"$Le\",null,{\"href\":\"/tags/distributed-systems\",\"className\":\"mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"children\":\"Distributed-Systems\"}],[\"$\",\"$Le\",null,{\"href\":\"/tags/paxos\",\"className\":\"mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"children\":\"Paxos\"}],[\"$\",\"$Le\",null,{\"href\":\"/tags/java\",\"className\":\"mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"children\":\"Java\"}],[\"$\",\"$Le\",null,{\"href\":\"/tags/cse-452\",\"className\":\"mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"children\":\"CSE-452\"}],[\"$\",\"$Le\",null,{\"href\":\"/tags/university-of-washington\",\"className\":\"mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"children\":\"University-of-Washington\"}],[\"$\",\"$Le\",null,{\"href\":\"/tags/key-value-store\",\"className\":\"mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"children\":\"Key-Value-Store\"}],[\"$\",\"$Le\",null,{\"href\":\"/tags/fault-tolerance\",\"className\":\"mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"children\":\"Fault-Tolerance\"}],[\"$\",\"$Le\",null,{\"href\":\"/tags/replication\",\"className\":\"mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"children\":\"Replication\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"flex justify-between py-4 xl:block xl:space-y-8 xl:py-8\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xs uppercase tracking-wide text-gray-500 dark:text-gray-400\",\"children\":\"Previous Article\"}],[\"$\",\"div\",null,{\"className\":\"text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"children\":[\"$\",\"$Le\",null,{\"className\":\"break-words\",\"href\":\"/projects/nft-market\",\"children\":\"Decentralized GameNFT Marketplace\"}]}]]}],\"$undefined\"]}]]}],[\"$\",\"div\",null,{\"className\":\"pt-4 xl:pt-8\",\"children\":[\"$\",\"$Le\",null,{\"className\":\"text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"href\":\"/projects\",\"aria-label\":\"Back to the projects\",\"children\":\"← Back to the projects\"}]}]]}]]}]]}]}]]}]]\n"])</script><script>self.__next_f.push([1,"12:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Building a Fault-Tolerant Sharded Key-Value Store with Paxos | CodeWisdom\"}],[\"$\",\"meta\",\"3\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"4\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"5\",{\"rel\":\"canonical\",\"href\":\"https://codewisdom.io/projects/paxos\"}],[\"$\",\"link\",\"6\",{\"rel\":\"alternate\",\"type\":\"application/rss+xml\",\"href\":\"https://codewisdom.io/feed.xml\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:title\",\"content\":\"Building a Fault-Tolerant Sharded Key-Value Store with Paxos\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:url\",\"content\":\"https://codewisdom.io/projects/paxos\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:site_name\",\"content\":\"CodeWisdom\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:image\",\"content\":\"https://codewisdom.io/static/images/logo-round.png\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"13\",{\"property\":\"article:published_time\",\"content\":\"2024-11-07T00:00:00.000Z\"}],[\"$\",\"meta\",\"14\",{\"property\":\"article:modified_time\",\"content\":\"2024-11-07T00:00:00.000Z\"}],[\"$\",\"meta\",\"15\",{\"property\":\"article:author\",\"content\":\"Bowen Y\"}],[\"$\",\"meta\",\"16\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:title\",\"content\":\"Building a Fault-Tolerant Sharded Key-Value Store with Paxos\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:image\",\"content\":\"https://codewisdom.io/static/images/logo-round.png\"}],[\"$\",\"meta\",\"19\",{\"name\":\"next-size-adjust\"}]]\n6:null\n"])</script></body></html>