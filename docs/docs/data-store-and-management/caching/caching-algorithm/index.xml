<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Novel CS</title>
    <link>https://example.org/docs/data-store-and-management/caching/caching-algorithm/</link>
    <description>Recent content on Novel CS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://example.org/docs/data-store-and-management/caching/caching-algorithm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://example.org/docs/data-store-and-management/caching/caching-algorithm/arc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/docs/data-store-and-management/caching/caching-algorithm/arc/</guid>
      <description>ARC # Adaptive Replacement Cache (ARC) is a fascinating approach to improving cache performance. Unlike more straightforward caching algorithms like LRU (Least Recently Used) or LFU (Least Frequently Used), ARC dynamically adapts to changing access patterns.
How ARC Works # ARC aims to balance between recency (like LRU) and frequency (like LFU). To do this, ARC maintains four lists:
T1: Recently seen but not yet frequently used items. B1: A ghost list of items recently evicted from T1.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://example.org/docs/data-store-and-management/caching/caching-algorithm/fifo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/docs/data-store-and-management/caching/caching-algorithm/fifo/</guid>
      <description>FIFO # Practical Use Cases # FIFO can be particularly useful in scenarios where recency does not necessarily imply usefulness. For example:
Data Streams: Imagine you are buffering data from a sensor, like temperature readings. You may need a limited number of recent readings, but they are all of equal importance, and the order of their removal doesn&amp;rsquo;t need to depend on frequency or recency of use. Network Packet Management: When buffering network packets in a router, FIFO can be useful since the packets are processed in the order they arrive.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://example.org/docs/data-store-and-management/caching/caching-algorithm/lfu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/docs/data-store-and-management/caching/caching-algorithm/lfu/</guid>
      <description>LFU - Least Frequently Used (LFU) Caching # In LFU, every time a key is accessed, we increment a counter associated with it. When the cache gets full, we look for the key with the smallest count to evict. The underlying idea is simple: if a data point is hardly ever accessed, it’s less likely to be accessed again, so it&amp;rsquo;s not worth keeping in our limited space.
How LFU Works # Incoming Request for Data → Check Cache.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://example.org/docs/data-store-and-management/caching/caching-algorithm/lru-k/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/docs/data-store-and-management/caching/caching-algorithm/lru-k/</guid>
      <description>LRU-K # The LRU-K (Least Recently Used-K) is an extension of the LRU (Least Recently Used) cache replacement algorithm. It keeps track of the K-th most recent access of an item rather than just the most recent one. By doing so, LRU-K can identify more frequently accessed items over time, which improves decision-making about which items to evict from the cache.
Key Concepts of LRU-K: # Access History: Each item in the cache keeps track of its last K access times.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://example.org/docs/data-store-and-management/caching/caching-algorithm/lru/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/docs/data-store-and-management/caching/caching-algorithm/lru/</guid>
      <description>LRU # LRU Caching # The core idea revolves around using a mechanism to evict the data that&amp;rsquo;s been least recently accessed when we run out of capacity. We generally use a combination of a hash map and a doubly linked list to achieve an optimal O(1) time complexity for both put and get operations.
Data Structures for LRU: Hash Map and Doubly Linked List # Let’s break down why the combination of a hash map and doubly linked list is so effective.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://example.org/docs/data-store-and-management/caching/caching-algorithm/mru/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/docs/data-store-and-management/caching/caching-algorithm/mru/</guid>
      <description>MRU # What Makes MRU Different? # Most Recently Used (MRU) is a caching strategy where, when the cache is full and you need to make room for new data, the algorithm will remove the most recently used item to make space. Unlike Least Recently Used (LRU), which removes the item that hasn&amp;rsquo;t been used for the longest time, MRU assumes that the item just used is less likely to be needed again soon.</description>
    </item>
    
  </channel>
</rss>
