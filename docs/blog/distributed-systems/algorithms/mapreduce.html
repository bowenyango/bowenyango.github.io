<!DOCTYPE html><html lang="en-us" class="__variable_0aa4ae scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/2d141e1a38819612-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/_next/static/media/logo.a3d92d8f.png"/><link rel="stylesheet" href="/_next/static/css/5beba742ae93b864.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/f719ff1083adf929.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/e36d2c8a31781d2e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-8a3a6504f64badee.js"/><script src="/_next/static/chunks/fd9d1056-7b6151dbf9b37af2.js" async=""></script><script src="/_next/static/chunks/23-ba11cb3b863464a4.js" async=""></script><script src="/_next/static/chunks/main-app-4250bb735c1c7796.js" async=""></script><script src="/_next/static/chunks/231-42eeaa612179830e.js" async=""></script><script src="/_next/static/chunks/113-8a0c0daa2bfc088c.js" async=""></script><script src="/_next/static/chunks/app/layout-a7a2551425c87cc3.js" async=""></script><script src="/_next/static/chunks/173-af5b99c330035292.js" async=""></script><script src="/_next/static/chunks/459-6c5dcdc51f43b997.js" async=""></script><script src="/_next/static/chunks/app/blog/%5B...slug%5D/page-5ad4d86948f7f0f4.js" async=""></script><link rel="preload" href="https://analytics.umami.is/script.js" as="script"/><title>MapReduce | CodeWisdom</title><meta name="description" content="Explores the MapReduce algorithm."/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://codewisdom.io/blog/distributed-systems/algorithms/mapreduce"/><link rel="alternate" type="application/rss+xml" href="https://codewisdom.io/feed.xml"/><meta property="og:title" content="MapReduce"/><meta property="og:description" content="Explores the MapReduce algorithm."/><meta property="og:url" content="https://codewisdom.io/blog/distributed-systems/algorithms/mapreduce"/><meta property="og:site_name" content="CodeWisdom"/><meta property="og:locale" content="en_US"/><meta property="og:image" content="https://codewisdom.io/static/images/logo-round.png"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2024-10-31T00:00:00.000Z"/><meta property="article:modified_time" content="2024-10-31T00:00:00.000Z"/><meta property="article:author" content="Bowen Y"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="MapReduce"/><meta name="twitter:description" content="Explores the MapReduce algorithm."/><meta name="twitter:image" content="https://codewisdom.io/static/images/logo-round.png"/><meta name="next-size-adjust"/><link rel="apple-touch-icon" sizes="76x76" href="/static/favicons/logo-round.png"/><link rel="icon" type="image/png" sizes="32x32" href="/static/favicons/logo-round.png"/><link rel="icon" type="image/png" sizes="16x16" href="/static/favicons/logo-round.png"/><link rel="manifest" href="/static/favicons/site.webmanifest"/><meta name="msapplication-TileColor" content="#000000"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="#fff"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="bg-white pl-[calc(100vw-100%)] text-black antialiased dark:bg-gray-950 dark:text-white"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><section class="mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0"><header class="flex items-center w-full bg-white dark:bg-gray-950 justify-between py-10"><a class="break-words" aria-label="CodeWisdom" href="/"><div class="flex items-center justify-between"><div class="mr-3"><img src="/_next/static/media/logo.a3d92d8f.png" alt="logo" class="h-10 w-14"/></div><div class="hidden h-6 text-2xl font-semibold sm:block">CodeWisdom</div></div></a><div class="flex items-center space-x-4 leading-5 sm:space-x-6"><div class="no-scrollbar hidden max-w-40 items-center space-x-4 overflow-x-auto sm:flex sm:space-x-6 md:max-w-72 lg:max-w-96"><a class="block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400" href="/blog">Blog</a><a class="block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400" href="/tags">Tags</a><a class="block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400" href="/projects">Projects</a><a class="block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400" href="/about">About</a></div><button aria-label="Search"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="h-6 w-6 text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 10.607z"></path></svg></button><div class="mr-5 flex items-center"><div class="relative inline-block text-left" data-headlessui-state=""><div class="flex items-center justify-center hover:text-primary-500 dark:hover:text-primary-400"><button aria-label="Theme switcher" id="headlessui-menu-button-:Rn6jaba:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><svg class="h-6 w-6"></svg></button></div></div></div><button aria-label="Toggle Menu" class="sm:hidden"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="h-8 w-8 text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg></button><div hidden="" style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div></div></header><main class="mb-auto"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"MapReduce","datePublished":"2024-10-31T00:00:00.000Z","dateModified":"2024-10-31T00:00:00.000Z","description":"Explores the MapReduce algorithm.","image":"/static/images/logo-round.png","url":"https://codewisdom.io/blog/distributed-systems/algorithms/mapreduce","author":[{"@type":"Person","name":"Bowen Y"}]}</script><section class="mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0"><div class="fixed bottom-8 right-8 hidden flex-col gap-3 md:hidden"><button aria-label="Scroll To Comment" class="rounded-full bg-gray-200 p-2 text-gray-500 transition-all hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-400 dark:hover:bg-gray-600"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M18 10c0 3.866-3.582 7-8 7a8.841 8.841 0 01-4.083-.98L2 17l1.338-3.123C2.493 12.767 2 11.434 2 10c0-3.866 3.582-7 8-7s8 3.134 8 7zM7 9H5v2h2V9zm8 0h-2v2h2V9zM9 9h2v2H9V9z" clip-rule="evenodd"></path></svg></button><button aria-label="Scroll To Top" class="rounded-full bg-gray-200 p-2 text-gray-500 transition-all hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-400 dark:hover:bg-gray-600"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z" clip-rule="evenodd"></path></svg></button></div><article><div class="xl:divide-y xl:divide-gray-200 xl:dark:divide-gray-700"><header class="pt-6 xl:pb-6"><div class="space-y-1 text-center"><dl class="space-y-10"><div><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2024-10-31T00:00:00.000Z">Wednesday, October 30, 2024</time></dd></div></dl><div><h1 class="text-3xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 sm:text-4xl sm:leading-10 md:text-5xl md:leading-14">MapReduce</h1></div></div></header><div class="grid-rows-[auto_1fr] divide-y divide-gray-200 pb-8 dark:divide-gray-700 xl:grid xl:grid-cols-4 xl:gap-x-6 xl:divide-y-0"><dl class="pb-10 pt-6 xl:border-b xl:border-gray-200 xl:pt-11 xl:dark:border-gray-700"><dt class="sr-only">Authors</dt><dd><ul class="flex flex-wrap justify-center gap-4 sm:space-x-12 xl:block xl:space-x-0 xl:space-y-8"><li class="flex items-center space-x-2"><img alt="avatar" loading="lazy" width="38" height="38" decoding="async" data-nimg="1" class="h-10 w-10 rounded-full" style="color:transparent" src="/static/images/pixel-avatar.png"/><dl class="whitespace-nowrap text-sm font-medium leading-5"><dt class="sr-only">Name</dt><dd class="text-gray-900 dark:text-gray-100">Bowen Y</dd><dt class="sr-only">Twitter</dt><dd></dd></dl></li></ul></dd></dl><div class="divide-y divide-gray-200 dark:divide-gray-700 xl:col-span-3 xl:row-span-2 xl:pb-0"><div class="prose max-w-none pb-8 pt-10 dark:prose-invert"><h2 class="content-header" id="question-where-is-the-data-stored-for-mapreduce"><a class="break-words" href="#question-where-is-the-data-stored-for-mapreduce" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Question: Where is the data stored for Mapreduce?</h2><p>In a typical MapReduce framework, the large datasets are stored in a distributed file system, such as the Hadoop Distributed File System (HDFS). Here&#x27;s how the storage and data distribution process works:</p><h3 class="content-header" id="storage-and-data-distribution-in-mapreduce"><a class="break-words" href="#storage-and-data-distribution-in-mapreduce" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Storage and Data Distribution in MapReduce:</h3><ol><li><p><strong>Distributed File System:</strong></p><ul><li>Large datasets are stored in a distributed file system (e.g., HDFS).</li><li>The data is divided into blocks (e.g., 128 MB each) and distributed across multiple nodes in the cluster. These nodes are often called DataNodes in HDFS.</li></ul></li><li><p><strong>Replication:</strong></p><ul><li>To ensure fault tolerance and reliability, each block of data is typically replicated across multiple nodes (e.g., each block might be stored on three different nodes).</li></ul></li><li><p><strong>Map Phase:</strong></p><ul><li>When a MapReduce job is initiated, the framework splits the job into tasks. Each task is assigned to a map node, and the input data for each task is a block of data stored in the distributed file system.</li><li>The framework tries to schedule tasks on nodes where the data blocks are already stored, minimizing data transfer across the network. This concept is known as data locality.</li></ul></li></ol><h3 class="content-header" id="workflow"><a class="break-words" href="#workflow" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Workflow:</h3><ol><li><p><strong>Data Storage:</strong></p><ul><li>Data is initially ingested into the distributed file system.</li><li>Example: A dataset is divided into blocks and stored on nodes A, B, and C in the cluster, with each block replicated across multiple nodes.</li></ul></li><li><p><strong>Job Initialization:</strong></p><ul><li>A MapReduce job is started.</li><li>The job is divided into multiple map tasks, each responsible for processing a block of data.</li></ul></li><li><p><strong>Data Locality:</strong></p><ul><li>The framework schedules map tasks on nodes where the data is already located to minimize network traffic.</li><li>Example: If a data block is stored on nodes A, B, and C, a map task for that block will be scheduled on one of these nodes if possible.</li></ul></li><li><p><strong>Data Transfer:</strong></p><ul><li>If it is not possible to schedule a task on a node with the local data (e.g., due to resource constraints), the data will be transferred over the network to the node where the map task is running.</li></ul></li></ol><h3 class="content-header" id="example-scenario"><a class="break-words" href="#example-scenario" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Example Scenario:</h3><ol><li><p><strong>Data Storage:</strong></p><ul><li>Large dataset (e.g., a 1 TB file) is stored in HDFS.</li><li>The dataset is split into 128 MB blocks, resulting in approximately 8,000 blocks.</li><li>Each block is replicated three times across different nodes.</li></ul></li><li><p><strong>MapReduce Job:</strong></p><ul><li>A MapReduce job is submitted to process the dataset.</li><li>The job is divided into 8,000 map tasks, each responsible for one block of data.</li></ul></li><li><p><strong>Task Scheduling:</strong></p><ul><li>Map task 1 is scheduled on node A, which already has a copy of block 1.</li><li>Map task 2 is scheduled on node B, which has a copy of block 2, and so on.</li></ul></li><li><p><strong>Data Processing:</strong></p><ul><li>Each map task processes its local block of data, generating intermediate key-value pairs.</li><li>If a map task is scheduled on a node without the local data, the necessary block is transferred from the distributed file system to that node.</li></ul></li></ol><h3 class="content-header" id="key-points"><a class="break-words" href="#key-points" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Key Points:</h3><ul><li><strong>Data is stored in a distributed file system (e.g., HDFS), not directly on map nodes.</strong></li><li><strong>The data is divided into blocks and distributed across multiple nodes for fault tolerance and efficiency.</strong></li><li><strong>Map tasks are scheduled to run on nodes where the data blocks are located to take advantage of data locality and minimize network transfer.</strong></li></ul><p>This approach ensures efficient processing of large datasets by leveraging the distributed storage and computation capabilities of the cluster.</p><h2 class="content-header" id="question-what-protocol-is-used-in-mapreduce-data-transmission"><a class="break-words" href="#question-what-protocol-is-used-in-mapreduce-data-transmission" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Question: What protocol is used in Mapreduce data transmission?</h2><p>TCP is used in several parts of the MapReduce framework, particularly during the following phases:</p><ol><li><p><strong>Data Input and Output</strong>: When reading input data from and writing output data to distributed storage systems like HDFS (Hadoop Distributed File System), TCP is used to reliably transfer data between the storage nodes and the MapReduce nodes.</p></li><li><p><strong>Shuffle and Sort Phase</strong>: During the shuffle and sort phase, intermediate data produced by the map tasks needs to be transferred to the appropriate reduce tasks. This phase involves significant data transfer between nodes, and TCP ensures that this data is reliably delivered.</p></li><li><p><strong>Inter-Node Communication</strong>: Throughout the execution of a MapReduce job, various nodes (e.g., DataNodes, TaskTrackers/NodeManagers) need to communicate with each other. This inter-node communication, which includes heartbeats, status updates, and task assignments, relies on TCP for reliable transmission.</p></li></ol><p>To summarize, TCP is used extensively for reliable data transmission during the shuffle and sort phase, data input/output operations, and inter-node communications within the MapReduce framework.</p><h2 class="content-header" id="question-what-is-the-comprehensive-data-flow-in-mapreduce-job"><a class="break-words" href="#question-what-is-the-comprehensive-data-flow-in-mapreduce-job" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Question: What is the comprehensive data flow in MapReduce job?</h2><ol><li><p><strong>Data Storage in HDFS</strong>: Data is stored in multiple blocks across a distributed file system like HDFS.</p></li><li><p><strong>Starting a MapReduce Job</strong>:</p><ul><li>The <strong>JobTracker/ResourceManager</strong> assigns map tasks to various <strong>DataNodes</strong> where the data blocks are located. This minimizes data transfer by moving computation to where the data resides (data locality).</li><li>Map tasks read the input data blocks from HDFS using TCP.</li></ul></li><li><p><strong>Map Phase</strong>:</p><ul><li>Each <strong>map task</strong> processes the data and produces intermediate key-value pairs.</li><li>These intermediate results are often stored temporarily on the local disk of the map node.</li></ul></li><li><p><strong>Shuffle and Sort Phase</strong>:</p><ul><li>After all map tasks complete, the intermediate data is transferred to the nodes responsible for the reduce tasks. This involves moving data across the network using TCP.</li><li>The data is sorted by key during this phase to group all values associated with the same key together.</li></ul></li><li><p><strong>Reduce Phase</strong>:</p><ul><li><strong>Reduce tasks</strong> receive the sorted intermediate data, process it, and produce the final output.</li><li>The final output of the reduce tasks is written to HDFS, again using TCP for reliable data transfer.</li></ul></li><li><p><strong>Final Output Storage</strong>:</p><ul><li>The final results from the reduce tasks are stored back into HDFS or another distributed file system.</li></ul></li></ol><p>To summarize, here&#x27;s the refined process:</p><ol><li>Data blocks stored in HDFS.</li><li>Map tasks read data blocks from HDFS (using TCP).</li><li>Map tasks produce intermediate data and store it on local disk.</li><li>Intermediate data is transferred to reduce nodes during the shuffle and sort phase (using TCP).</li><li>Reduce tasks process the intermediate data and write the final results to HDFS (using TCP).</li></ol><p>It&#x27;s important to note that the shuffle and sort phase is not a separate &quot;system&quot; but rather part of the overall MapReduce process, specifically handled by the MapReduce framework to ensure data is properly sorted and grouped before reaching the reduce phase.</p><h2 class="content-header" id="question-is-there-any-centralized-component-in-a-mapreduce-system"><a class="break-words" href="#question-is-there-any-centralized-component-in-a-mapreduce-system" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Question: Is there any centralized component in a MapReduce system?</h2><p>The MapReduce framework itself is not centralized; it operates in a distributed manner, coordinating various tasks across a cluster of nodes. Here&#x27;s how it works:</p><ol><li><p><strong>JobTracker/ResourceManager</strong>:</p><ul><li>In the original Hadoop MapReduce (often referred to as MRv1), a central component called the JobTracker is responsible for managing the job and tracking the progress of each task.</li><li>In the newer Hadoop YARN (Yet Another Resource Negotiator) architecture, which is part of Hadoop 2.x and later (often referred to as MRv2), the ResourceManager is the central authority that manages resources and schedules jobs.</li></ul></li><li><p><strong>TaskTrackers/NodeManagers</strong>:</p><ul><li>In MRv1, TaskTrackers are distributed across nodes in the cluster. They execute the map and reduce tasks as assigned by the JobTracker.</li><li>In YARN, NodeManagers run on each node and are responsible for launching and monitoring containers that execute the map and reduce tasks.</li></ul></li><li><p><strong>MapReduce Job Execution</strong>:</p><ul><li>When a MapReduce job is submitted, the JobTracker/ResourceManager breaks it down into smaller tasks (map tasks and reduce tasks).</li><li>These tasks are distributed across the cluster to the nodes where the data is stored (data locality principle).</li><li>The map tasks process the input data and produce intermediate key-value pairs.</li><li>The shuffle and sort phase involves transferring intermediate data across the network to the nodes running the reduce tasks.</li><li>Reduce tasks process the intermediate data and produce the final output.</li></ul></li><li><p><strong>Fault Tolerance</strong>:</p><ul><li>The framework handles node failures by reassigning tasks to other nodes and ensures that the job completes successfully.</li></ul></li></ol><p>To summarize, while there are centralized components (JobTracker in MRv1 or ResourceManager in YARN) responsible for coordinating the overall job execution and resource management, the actual processing is distributed across the cluster. The MapReduce framework orchestrates this distributed processing to ensure efficient and reliable job completion.</p><h2 class="content-header" id="does-the-resourcemanager-in-hadoop-support-the-master-slave-mechanism"><a class="break-words" href="#does-the-resourcemanager-in-hadoop-support-the-master-slave-mechanism" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Does the ResourceManager in Hadoop support the master-slave mechanism?</h2><p>The ResourceManager in Hadoop YARN (Yet Another Resource Negotiator) is indeed a centralized component, but it operates within a master-slave architecture to provide high availability and fault tolerance. Here’s a detailed breakdown:</p><ol><li><p><strong>ResourceManager (RM)</strong>:</p><ul><li>The ResourceManager is the central authority responsible for resource allocation and job scheduling in a YARN cluster.</li><li>It consists of several components, including the Scheduler and the ApplicationManager.</li></ul></li><li><p><strong>High Availability</strong>:</p><ul><li>To avoid a single point of failure, YARN can be configured for high availability (HA). In an HA setup, there are typically two ResourceManagers: an active ResourceManager and a standby ResourceManager.</li><li>The active RM handles all the resource management and job scheduling tasks, while the standby RM remains synchronized and can take over in case the active RM fails.</li></ul></li><li><p><strong>NodeManagers (NMs)</strong>:</p><ul><li>NodeManagers are distributed across the cluster, running on each node.</li><li>They are responsible for launching and managing containers on the nodes, monitoring resource usage, and reporting back to the ResourceManager.</li></ul></li><li><p><strong>ApplicationMaster (AM)</strong>:</p><ul><li>For each application submitted to the YARN cluster, an ApplicationMaster is launched.</li><li>The ApplicationMaster negotiates resources with the ResourceManager and works with the NodeManagers to execute and monitor tasks.</li><li>Each application has its own ApplicationMaster, which provides a level of decentralization for task management.</li></ul></li></ol><p>Here&#x27;s how it works in practice:</p><ol><li><p><strong>Job Submission</strong>:</p><ul><li>When a job is submitted, the ResourceManager allocates a container for the ApplicationMaster of that job.</li><li>The ApplicationMaster then negotiates further resources with the ResourceManager to execute the map and reduce tasks.</li></ul></li><li><p><strong>Resource Allocation</strong>:</p><ul><li>The ResourceManager’s Scheduler allocates resources based on various policies (e.g., capacity, fairness).</li><li>NodeManagers communicate with the ResourceManager to report resource availability and task statuses.</li></ul></li><li><p><strong>Task Execution</strong>:</p><ul><li>The ApplicationMaster requests containers from the ResourceManager to run the tasks.</li><li>NodeManagers launch the containers and manage their execution.</li></ul></li><li><p><strong>Fault Tolerance</strong>:</p><ul><li>The ResourceManager ensures high availability with an active-standby configuration.</li><li>The ApplicationMaster can handle task failures by requesting new containers and re-executing failed tasks.</li></ul></li></ol><p>In summary, while the ResourceManager is a centralized component in YARN, it operates within a master-slave architecture with mechanisms for high availability and decentralization of task management through ApplicationMasters and NodeManagers.</p><h2 class="content-header" id="question-will-the-network-throughput-be-the-significant-bottleneck-in-the-mapreduce-job"><a class="break-words" href="#question-will-the-network-throughput-be-the-significant-bottleneck-in-the-mapreduce-job" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Question: Will the network throughput be the significant bottleneck in the MapReduce job?</h2><p>Network throughput can indeed be a significant bottleneck in a MapReduce job, especially during the shuffle and sort phase where large volumes of intermediate data are transferred between nodes. To mitigate this, several strategies are employed:</p><ol><li><p><strong>Data Locality</strong>:</p><ul><li>MapReduce frameworks try to schedule map tasks on the nodes where the data is stored or as close to those nodes as possible. This minimizes the need to transfer large amounts of input data over the network.</li></ul></li><li><p><strong>Intermediate Data Storage</strong>:</p><ul><li>Intermediate data generated by map tasks is often stored locally on the map node&#x27;s disk. This local storage reduces the amount of data that needs to be transferred over the network initially.</li></ul></li><li><p><strong>Cluster Topology Awareness</strong>:</p><ul><li>The framework is aware of the cluster topology, such as the network bandwidth between racks and nodes. This awareness helps in optimizing data transfer during the shuffle and sort phase by grouping and scheduling tasks to minimize cross-rack network traffic.</li></ul></li><li><p><strong>Combiner Function</strong>:</p><ul><li>A combiner function can be used to perform local aggregation of intermediate data on the map node before sending it over the network. This reduces the volume of data that needs to be shuffled.</li></ul></li></ol><p>Regarding your question about Google setting GFS (Google File System) and MapReduce workers on the same physical machines:</p><p>Yes, Google’s original MapReduce implementation indeed places both GFS (or later Colossus, the successor to GFS) and MapReduce workers on the same physical machines. This co-location is done to:</p><ol><li><p><strong>Improve Data Locality</strong>:</p><ul><li>By placing data storage (GFS) and data processing (MapReduce workers) on the same machines, the framework can leverage data locality to reduce network traffic. Map tasks can read input data directly from the local disk without needing to transfer it over the network.</li></ul></li><li><p><strong>Optimize Resource Utilization</strong>:</p><ul><li>This setup ensures better utilization of the cluster&#x27;s resources. Machines are used for both storage and computation, balancing the workload and avoiding the need for a separate storage cluster and a separate compute cluster.</li></ul></li><li><p><strong>Reduce Latency</strong>:</p><ul><li>Co-locating storage and compute reduces the latency associated with data access, as the data is readily available on the same machine or at least within the same rack, minimizing the distance it needs to travel.</li></ul></li></ol><h2 class="content-header" id="is-the-data-locality-a-major-consideration-nowadays"><a class="break-words" href="#is-the-data-locality-a-major-consideration-nowadays" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Is the data locality a major consideration nowadays?</h2><p>Google&#x27;s design for its distributed systems, including GFS (Google File System) and MapReduce, has evolved over time. However, the principle of co-locating storage and compute resources has remained a key strategy for optimizing data locality and minimizing network bottlenecks.</p><p>In the original MapReduce and GFS setup, co-locating the distributed file system and MapReduce workers on the same physical machines was a fundamental design choice. This approach improved performance by leveraging data locality, reducing the need to transfer large amounts of data over the network.</p><p>With advancements and the development of new systems like Colossus (the successor to GFS) and other distributed computing frameworks, Google has continued to optimize these principles. Some of the improvements and advancements include:</p><ol><li><p><strong>Colossus (GFS successor)</strong>:</p><ul><li>Colossus provides improvements in scalability, performance, and reliability over the original GFS. It is designed to handle even larger datasets and more complex workloads.</li><li>Colossus still emphasizes data locality to reduce network traffic, so co-locating storage and compute remains a beneficial practice.</li></ul></li><li><p><strong>Borg and Kubernetes</strong>:</p><ul><li>Borg, Google’s cluster management system, and its successor Kubernetes, also emphasize efficient resource management and scheduling. These systems manage both storage and compute resources across large clusters.</li><li>Kubernetes, widely adopted outside of Google, provides mechanisms for co-locating pods (containers) with data, emphasizing locality and reducing network overhead.</li></ul></li><li><p><strong>Cloud Infrastructure</strong>:</p><ul><li>In cloud environments like Google Cloud Platform (GCP), services such as Google Cloud Storage (GCS) and Google BigQuery provide highly optimized data storage and processing capabilities.</li><li>While these services may separate storage and compute physically, they are interconnected with high-speed, low-latency networks to minimize the performance impact. Data locality and efficient data transfer remain critical considerations.</li></ul></li></ol><p>In summary, while the exact implementations and technologies have evolved, the principle of optimizing data locality by co-locating storage and compute resources continues to be a key strategy for enhancing performance and reducing network bottlenecks. Google’s infrastructure improvements have built on these principles, offering more advanced and scalable solutions for handling distributed data processing tasks.</p><h2 class="content-header" id="why-does-mapreduce-use-batch-processing-instead-of-stream-processing-in-the-reduce-phase-allowing-reduce-nodes-to-avoid-waiting-for-all-map-nodes-to-finish-processing"><a class="break-words" href="#why-does-mapreduce-use-batch-processing-instead-of-stream-processing-in-the-reduce-phase-allowing-reduce-nodes-to-avoid-waiting-for-all-map-nodes-to-finish-processing" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Why does MapReduce use batch processing instead of stream processing in the reduce phase, allowing reduce nodes to avoid waiting for all map nodes to finish processing?</h2><p>MapReduce, as a programming model, is inherently designed around batch processing. This design choice affects the reduce phase and how it handles data. Here are the main reasons why MapReduce typically doesn&#x27;t use stream processing in the reduce phase:</p><ol><li><p><strong>Synchronization and Completeness</strong>:</p><ul><li>The reduce phase needs to ensure that all intermediate data generated by the map phase is available before it begins. This requirement ensures that each reducer receives a complete set of data for each key it is responsible for. If reducers started processing data as it arrived (i.e., stream processing), there is a risk of incomplete data, leading to incorrect results.</li></ul></li><li><p><strong>Data Shuffling</strong>:</p><ul><li>In MapReduce, there is a critical step called the shuffle and sort phase that occurs between the map and reduce phases. During this step, the framework sorts and groups the intermediate data by key. Stream processing would complicate this step because it would require continuous sorting and grouping as data arrives, which is much more complex and less efficient compared to batch processing.</li></ul></li><li><p><strong>Fault Tolerance</strong>:</p><ul><li>MapReduce is designed to handle faults gracefully. If a map task fails, it can be re-executed without affecting the reduce tasks since reduce tasks only start after all map tasks are completed. This batch processing approach simplifies fault tolerance because the system can ensure that all necessary data is available and correct before reducers start processing.</li></ul></li><li><p><strong>Resource Optimization</strong>:</p><ul><li>By waiting until all map tasks are complete before starting the reduce phase, the system can optimize resource allocation. Resources can be dynamically allocated to map tasks first, and once they complete, those resources can be reassigned to reduce tasks. This staged approach can lead to better utilization of cluster resources.</li></ul></li><li><p><strong>Implementation Simplicity</strong>:</p><ul><li>The original MapReduce model was designed with simplicity in mind. Batch processing in the reduce phase simplifies the implementation and logic of the MapReduce framework. Streaming data processing would require a more complex implementation to handle continuous data flow, synchronization issues, and potential partial results.</li></ul></li></ol><p>While MapReduce uses batch processing in the reduce phase for the reasons mentioned above, there are other frameworks and models designed specifically for stream processing, such as Apache Storm, Apache Flink, and Kafka Streams, which can handle continuous data streams and provide real-time processing capabilities. These frameworks address the challenges of stream processing and are better suited for scenarios where real-time data processing is required.</p></div></div><footer><div class="divide-gray-200 text-sm font-medium leading-5 dark:divide-gray-700 xl:col-start-1 xl:row-start-2 xl:divide-y"><div class="py-4 xl:py-8"><h2 class="text-xs uppercase tracking-wide text-gray-500 dark:text-gray-400">Tags</h2><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/distributed-systems">distributed-systems</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/algorithms">algorithms</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/mapreduce">mapreduce</a></div></div><div class="flex justify-between py-4 xl:block xl:space-y-8 xl:py-8"><div><h2 class="text-xs uppercase tracking-wide text-gray-500 dark:text-gray-400">Previous Article</h2><div class="text-primary-500 hover:text-primary-600 dark:hover:text-primary-400"><a class="break-words" href="/blog/distributed-systems/systems/GFS">GFS</a></div></div></div></div><div class="pt-4 xl:pt-8"><a class="text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" aria-label="Back to the projects" href="/blog">← Back to the projects</a></div></footer></div></div></article></section></main><footer><div class="mt-16 flex flex-col items-center"><div class="mb-2 flex space-x-2 text-sm text-gray-500 dark:text-gray-400"><div>Bowen Y</div><div> • </div><div>© 2024</div><div> • </div><a class="break-words" href="/">CodeWisdom</a></div></div></footer></section><script src="/_next/static/chunks/webpack-8a3a6504f64badee.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/2d141e1a38819612-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/5beba742ae93b864.css\",\"style\"]\n3:HL[\"/_next/static/css/f719ff1083adf929.css\",\"style\"]\n4:HL[\"/_next/static/css/e36d2c8a31781d2e.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"5:I[5751,[],\"\"]\n8:I[9275,[],\"\"]\na:I[1343,[],\"\"]\nb:I[8700,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"113\",\"static/chunks/113-8a0c0daa2bfc088c.js\",\"185\",\"static/chunks/app/layout-a7a2551425c87cc3.js\"],\"ThemeProviders\"]\nc:I[4080,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"113\",\"static/chunks/113-8a0c0daa2bfc088c.js\",\"185\",\"static/chunks/app/layout-a7a2551425c87cc3.js\"],\"\"]\nd:I[9032,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"113\",\"static/chunks/113-8a0c0daa2bfc088c.js\",\"185\",\"static/chunks/app/layout-a7a2551425c87cc3.js\"],\"KBarSearchProvider\"]\ne:I[231,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"173\",\"static/chunks/173-af5b99c330035292.js\",\"459\",\"static/chunks/459-6c5dcdc51f43b997.js\",\"797\",\"static/chunks/app/blog/%5B...slug%5D/page-5ad4d86948f7f0f4.js\"],\"\"]\nf:I[509,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"113\",\"static/chunks/113-8a0c0daa2bfc088c.js\",\"185\",\"static/chunks/app/layout-a7a2551425c87cc3.js\"],\"KBarButton\"]\n10:I[1398,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"113\",\"static/chunks/113-8a0c0daa2bfc088c.js\",\"185\",\"static/chunks/app/layout-a7a2551425c87cc3.js\"],\"default\"]\n11:I[8976,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"113\",\"static/chunks/113-8a0c0daa2bfc088c.js\",\"185\",\"static/chunks/app/layout-a7a2551425c87cc3.js\"],\"default\"]\n13:I[6130,[],\"\"]\n9:[\"slug\",\"distributed-systems/algorithms/mapreduce\",\"c\"]\n14:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5beba742ae93b864.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/f719ff1083adf929.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"buildId\":\"gPZ8ET5SZwyuIVWxpISNr\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/distributed-systems/algorithms/mapreduce\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"distributed-systems/algorithms/mapreduce\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"distributed-systems\\\",\\\"algorithms\\\",\\\"mapreduce\\\"]}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"distributed-systems/algorithms/mapreduce\",\"c\"],{\"children\":[\"__PAGE__\",{},[[\"$L6\",\"$L7\"],null],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"$9\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/e36d2c8a31781d2e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]}],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"en-us\",\"className\":\"__variable_0aa4ae scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"link\",null,{\"rel\":\"apple-touch-icon\",\"sizes\":\"76x76\",\"href\":\"/static/favicons/logo-round.png\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"sizes\":\"32x32\",\"href\":\"/static/favicons/logo-round.png\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"sizes\":\"16x16\",\"href\":\"/static/favicons/logo-round.png\"}],[\"$\",\"link\",null,{\"rel\":\"manifest\",\"href\":\"/static/favicons/site.webmanifest\"}],[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#000000\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: light)\",\"content\":\"#fff\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: dark)\",\"content\":\"#000\"}],[\"$\",\"link\",null,{\"rel\":\"alternate\",\"type\":\"application/rss+xml\",\"href\":\"/feed.xml\"}],[\"$\",\"body\",null,{\"className\":\"bg-white pl-[calc(100vw-100%)] text-black antialiased dark:bg-gray-950 dark:text-white\",\"children\":[\"$\",\"$Lb\",null,{\"children\":[[\"$undefined\",\"$undefined\",\"$undefined\",[\"$\",\"$Lc\",null,{\"async\":true,\"defer\":true,\"data-website-id\":\"$undefined\",\"src\":\"https://analytics.umami.is/script.js\"}],\"$undefined\",\"$undefined\"],[\"$\",\"section\",null,{\"className\":\"mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0\",\"children\":[[\"$\",\"$Ld\",null,{\"kbarConfig\":{\"searchDocumentsPath\":\"/search.json\"},\"children\":[[\"$\",\"header\",null,{\"className\":\"flex items-center w-full bg-white dark:bg-gray-950 justify-between py-10\",\"children\":[[\"$\",\"$Le\",null,{\"className\":\"break-words\",\"href\":\"/\",\"aria-label\":\"CodeWisdom\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-between\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mr-3\",\"children\":[\"$\",\"img\",null,{\"src\":\"/_next/static/media/logo.a3d92d8f.png\",\"alt\":\"logo\",\"className\":\"h-10 w-14\"}]}],[\"$\",\"div\",null,{\"className\":\"hidden h-6 text-2xl font-semibold sm:block\",\"children\":\"CodeWisdom\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-4 leading-5 sm:space-x-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"no-scrollbar hidden max-w-40 items-center space-x-4 overflow-x-auto sm:flex sm:space-x-6 md:max-w-72 lg:max-w-96\",\"children\":[[\"$\",\"$Le\",null,{\"className\":\"block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400\",\"href\":\"/blog\",\"children\":\"Blog\"}],[\"$\",\"$Le\",null,{\"className\":\"block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400\",\"href\":\"/tags\",\"children\":\"Tags\"}],[\"$\",\"$Le\",null,{\"className\":\"block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400\",\"href\":\"/projects\",\"children\":\"Projects\"}],[\"$\",\"$Le\",null,{\"className\":\"block font-medium text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400\",\"href\":\"/about\",\"children\":\"About\"}]]}],[\"$\",\"$Lf\",null,{\"aria-label\":\"Search\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 24 24\",\"strokeWidth\":1.5,\"stroke\":\"currentColor\",\"className\":\"h-6 w-6 text-gray-900 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 10.607z\"}]}]}],[\"$\",\"$L10\",null,{}],[\"$\",\"$L11\",null,{}]]}]]}],[\"$\",\"main\",null,{\"className\":\"mb-auto\",\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-start justify-start md:mt-24 md:flex-row md:items-center md:justify-center md:space-x-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"space-x-2 pb-8 pt-6 md:space-y-5\",\"children\":[\"$\",\"h1\",null,{\"className\":\"text-6xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 md:border-r-2 md:px-6 md:text-8xl md:leading-14\",\"children\":\"404\"}]}],[\"$\",\"div\",null,{\"className\":\"max-w-md\",\"children\":[[\"$\",\"p\",null,{\"className\":\"mb-4 text-xl font-bold leading-normal md:text-2xl\",\"children\":\"Sorry we couldn't find this page.\"}],[\"$\",\"p\",null,{\"className\":\"mb-8\",\"children\":\"But dont worry, you can find plenty of other things on our homepage.\"}],[\"$\",\"$Le\",null,{\"className\":\"focus:shadow-outline-blue inline rounded-lg border border-transparent bg-blue-600 px-4 py-2 text-sm font-medium leading-5 text-white shadow transition-colors duration-150 hover:bg-blue-700 focus:outline-none dark:hover:bg-blue-500\",\"href\":\"/\",\"children\":\"Back to homepage\"}]]}]]}],\"notFoundStyles\":[],\"styles\":null}]}]]}],[\"$\",\"footer\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"mt-16 flex flex-col items-center\",\"children\":[\"$\",\"div\",null,{\"className\":\"mb-2 flex space-x-2 text-sm text-gray-500 dark:text-gray-400\",\"children\":[[\"$\",\"div\",null,{\"children\":\"Bowen Y\"}],[\"$\",\"div\",null,{\"children\":\" • \"}],[\"$\",\"div\",null,{\"children\":\"© 2024\"}],[\"$\",\"div\",null,{\"children\":\" • \"}],[\"$\",\"$Le\",null,{\"className\":\"break-words\",\"href\":\"/\",\"children\":\"CodeWisdom\"}]]}]}]}]]}]]}]}]]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[false,\"$L12\"],\"globalErrorComponent\":\"$13\",\"missingSlots\":\"$W14\"}]]\n"])</script><script>self.__next_f.push([1,"15:I[4347,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"173\",\"static/chunks/173-af5b99c330035292.js\",\"459\",\"static/chunks/459-6c5dcdc51f43b997.js\",\"797\",\"static/chunks/app/blog/%5B...slug%5D/page-5ad4d86948f7f0f4.js\"],\"default\"]\n16:I[8173,[\"231\",\"static/chunks/231-42eeaa612179830e.js\",\"173\",\"static/chunks/173-af5b99c330035292.js\",\"459\",\"static/chunks/459-6c5dcdc51f43b997.js\",\"797\",\"static/chunks/app/blog/%5B...slug%5D/page-5ad4d86948f7f0f4.js\"],\"Image\"]\n"])</script><script>self.__next_f.push([1,"7:[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BlogPosting\\\",\\\"headline\\\":\\\"MapReduce\\\",\\\"datePublished\\\":\\\"2024-10-31T00:00:00.000Z\\\",\\\"dateModified\\\":\\\"2024-10-31T00:00:00.000Z\\\",\\\"description\\\":\\\"Explores the MapReduce algorithm.\\\",\\\"image\\\":\\\"/static/images/logo-round.png\\\",\\\"url\\\":\\\"https://codewisdom.io/blog/distributed-systems/algorithms/mapreduce\\\",\\\"author\\\":[{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"Bowen Y\\\"}]}\"}}],[\"$\",\"section\",null,{\"className\":\"mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0\",\"children\":[[\"$\",\"$L15\",null,{}],[\"$\",\"article\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"xl:divide-y xl:divide-gray-200 xl:dark:divide-gray-700\",\"children\":[[\"$\",\"header\",null,{\"className\":\"pt-6 xl:pb-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-1 text-center\",\"children\":[[\"$\",\"dl\",null,{\"className\":\"space-y-10\",\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"dt\",null,{\"className\":\"sr-only\",\"children\":\"Published on\"}],[\"$\",\"dd\",null,{\"className\":\"text-base font-medium leading-6 text-gray-500 dark:text-gray-400\",\"children\":[\"$\",\"time\",null,{\"dateTime\":\"2024-10-31T00:00:00.000Z\",\"children\":\"Wednesday, October 30, 2024\"}]}]]}]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"h1\",null,{\"className\":\"text-3xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 sm:text-4xl sm:leading-10 md:text-5xl md:leading-14\",\"children\":\"MapReduce\"}]}]]}]}],[\"$\",\"div\",null,{\"className\":\"grid-rows-[auto_1fr] divide-y divide-gray-200 pb-8 dark:divide-gray-700 xl:grid xl:grid-cols-4 xl:gap-x-6 xl:divide-y-0\",\"children\":[[\"$\",\"dl\",null,{\"className\":\"pb-10 pt-6 xl:border-b xl:border-gray-200 xl:pt-11 xl:dark:border-gray-700\",\"children\":[[\"$\",\"dt\",null,{\"className\":\"sr-only\",\"children\":\"Authors\"}],[\"$\",\"dd\",null,{\"children\":[\"$\",\"ul\",null,{\"className\":\"flex flex-wrap justify-center gap-4 sm:space-x-12 xl:block xl:space-x-0 xl:space-y-8\",\"children\":[[\"$\",\"li\",\"Bowen Y\",{\"className\":\"flex items-center space-x-2\",\"children\":[[\"$\",\"$L16\",null,{\"src\":\"/static/images/pixel-avatar.png\",\"width\":38,\"height\":38,\"alt\":\"avatar\",\"className\":\"h-10 w-10 rounded-full\"}],[\"$\",\"dl\",null,{\"className\":\"whitespace-nowrap text-sm font-medium leading-5\",\"children\":[[\"$\",\"dt\",null,{\"className\":\"sr-only\",\"children\":\"Name\"}],[\"$\",\"dd\",null,{\"className\":\"text-gray-900 dark:text-gray-100\",\"children\":\"Bowen Y\"}],[\"$\",\"dt\",null,{\"className\":\"sr-only\",\"children\":\"Twitter\"}],[\"$\",\"dd\",null,{\"children\":\"$undefined\"}]]}]]}]]}]}]]}],[\"$\",\"div\",null,{\"className\":\"divide-y divide-gray-200 dark:divide-gray-700 xl:col-span-3 xl:row-span-2 xl:pb-0\",\"children\":[\"$\",\"div\",null,{\"className\":\"prose max-w-none pb-8 pt-10 dark:prose-invert\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"question-where-is-the-data-stored-for-mapreduce\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#question-where-is-the-data-stored-for-mapreduce\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Question: Where is the data stored for Mapreduce?\"]}],[\"$\",\"p\",null,{\"children\":\"In a typical MapReduce framework, the large datasets are stored in a distributed file system, such as the Hadoop Distributed File System (HDFS). Here's how the storage and data distribution process works:\"}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"storage-and-data-distribution-in-mapreduce\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#storage-and-data-distribution-in-mapreduce\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Storage and Data Distribution in MapReduce:\"]}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Distributed File System:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Large datasets are stored in a distributed file system (e.g., HDFS).\"}],[\"$\",\"li\",null,{\"children\":\"The data is divided into blocks (e.g., 128 MB each) and distributed across multiple nodes in the cluster. These nodes are often called DataNodes in HDFS.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Replication:\"}]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"To ensure fault tolerance and reliability, each block of data is typically replicated across multiple nodes (e.g., each block might be stored on three different nodes).\"}]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Map Phase:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"When a MapReduce job is initiated, the framework splits the job into tasks. Each task is assigned to a map node, and the input data for each task is a block of data stored in the distributed file system.\"}],[\"$\",\"li\",null,{\"children\":\"The framework tries to schedule tasks on nodes where the data blocks are already stored, minimizing data transfer across the network. This concept is known as data locality.\"}]]}]]}]]}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"workflow\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#workflow\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Workflow:\"]}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Data Storage:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Data is initially ingested into the distributed file system.\"}],[\"$\",\"li\",null,{\"children\":\"Example: A dataset is divided into blocks and stored on nodes A, B, and C in the cluster, with each block replicated across multiple nodes.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Job Initialization:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"A MapReduce job is started.\"}],[\"$\",\"li\",null,{\"children\":\"The job is divided into multiple map tasks, each responsible for processing a block of data.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Data Locality:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"The framework schedules map tasks on nodes where the data is already located to minimize network traffic.\"}],[\"$\",\"li\",null,{\"children\":\"Example: If a data block is stored on nodes A, B, and C, a map task for that block will be scheduled on one of these nodes if possible.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Data Transfer:\"}]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"If it is not possible to schedule a task on a node with the local data (e.g., due to resource constraints), the data will be transferred over the network to the node where the map task is running.\"}]}]]}]]}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"example-scenario\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#example-scenario\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Example Scenario:\"]}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Data Storage:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Large dataset (e.g., a 1 TB file) is stored in HDFS.\"}],[\"$\",\"li\",null,{\"children\":\"The dataset is split into 128 MB blocks, resulting in approximately 8,000 blocks.\"}],[\"$\",\"li\",null,{\"children\":\"Each block is replicated three times across different nodes.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"MapReduce Job:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"A MapReduce job is submitted to process the dataset.\"}],[\"$\",\"li\",null,{\"children\":\"The job is divided into 8,000 map tasks, each responsible for one block of data.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Task Scheduling:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Map task 1 is scheduled on node A, which already has a copy of block 1.\"}],[\"$\",\"li\",null,{\"children\":\"Map task 2 is scheduled on node B, which has a copy of block 2, and so on.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Data Processing:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Each map task processes its local block of data, generating intermediate key-value pairs.\"}],[\"$\",\"li\",null,{\"children\":\"If a map task is scheduled on a node without the local data, the necessary block is transferred from the distributed file system to that node.\"}]]}]]}]]}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"key-points\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#key-points\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Key Points:\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Data is stored in a distributed file system (e.g., HDFS), not directly on map nodes.\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"The data is divided into blocks and distributed across multiple nodes for fault tolerance and efficiency.\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Map tasks are scheduled to run on nodes where the data blocks are located to take advantage of data locality and minimize network transfer.\"}]}]]}],[\"$\",\"p\",null,{\"children\":\"This approach ensures efficient processing of large datasets by leveraging the distributed storage and computation capabilities of the cluster.\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"question-what-protocol-is-used-in-mapreduce-data-transmission\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#question-what-protocol-is-used-in-mapreduce-data-transmission\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Question: What protocol is used in Mapreduce data transmission?\"]}],[\"$\",\"p\",null,{\"children\":\"TCP is used in several parts of the MapReduce framework, particularly during the following phases:\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Data Input and Output\"}],\": When reading input data from and writing output data to distributed storage systems like HDFS (Hadoop Distributed File System), TCP is used to reliably transfer data between the storage nodes and the MapReduce nodes.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Shuffle and Sort Phase\"}],\": During the shuffle and sort phase, intermediate data produced by the map tasks needs to be transferred to the appropriate reduce tasks. This phase involves significant data transfer between nodes, and TCP ensures that this data is reliably delivered.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Inter-Node Communication\"}],\": Throughout the execution of a MapReduce job, various nodes (e.g., DataNodes, TaskTrackers/NodeManagers) need to communicate with each other. This inter-node communication, which includes heartbeats, status updates, and task assignments, relies on TCP for reliable transmission.\"]}]}]]}],[\"$\",\"p\",null,{\"children\":\"To summarize, TCP is used extensively for reliable data transmission during the shuffle and sort phase, data input/output operations, and inter-node communications within the MapReduce framework.\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"question-what-is-the-comprehensive-data-flow-in-mapreduce-job\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#question-what-is-the-comprehensive-data-flow-in-mapreduce-job\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Question: What is the comprehensive data flow in MapReduce job?\"]}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Data Storage in HDFS\"}],\": Data is stored in multiple blocks across a distributed file system like HDFS.\"]}]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Starting a MapReduce Job\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"The \",[\"$\",\"strong\",null,{\"children\":\"JobTracker/ResourceManager\"}],\" assigns map tasks to various \",[\"$\",\"strong\",null,{\"children\":\"DataNodes\"}],\" where the data blocks are located. This minimizes data transfer by moving computation to where the data resides (data locality).\"]}],[\"$\",\"li\",null,{\"children\":\"Map tasks read the input data blocks from HDFS using TCP.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Map Phase\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"Each \",[\"$\",\"strong\",null,{\"children\":\"map task\"}],\" processes the data and produces intermediate key-value pairs.\"]}],[\"$\",\"li\",null,{\"children\":\"These intermediate results are often stored temporarily on the local disk of the map node.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Shuffle and Sort Phase\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"After all map tasks complete, the intermediate data is transferred to the nodes responsible for the reduce tasks. This involves moving data across the network using TCP.\"}],[\"$\",\"li\",null,{\"children\":\"The data is sorted by key during this phase to group all values associated with the same key together.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Reduce Phase\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Reduce tasks\"}],\" receive the sorted intermediate data, process it, and produce the final output.\"]}],[\"$\",\"li\",null,{\"children\":\"The final output of the reduce tasks is written to HDFS, again using TCP for reliable data transfer.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Final Output Storage\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"The final results from the reduce tasks are stored back into HDFS or another distributed file system.\"}]}]]}]]}],[\"$\",\"p\",null,{\"children\":\"To summarize, here's the refined process:\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Data blocks stored in HDFS.\"}],[\"$\",\"li\",null,{\"children\":\"Map tasks read data blocks from HDFS (using TCP).\"}],[\"$\",\"li\",null,{\"children\":\"Map tasks produce intermediate data and store it on local disk.\"}],[\"$\",\"li\",null,{\"children\":\"Intermediate data is transferred to reduce nodes during the shuffle and sort phase (using TCP).\"}],[\"$\",\"li\",null,{\"children\":\"Reduce tasks process the intermediate data and write the final results to HDFS (using TCP).\"}]]}],[\"$\",\"p\",null,{\"children\":\"It's important to note that the shuffle and sort phase is not a separate \\\"system\\\" but rather part of the overall MapReduce process, specifically handled by the MapReduce framework to ensure data is properly sorted and grouped before reaching the reduce phase.\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"question-is-there-any-centralized-component-in-a-mapreduce-system\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#question-is-there-any-centralized-component-in-a-mapreduce-system\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Question: Is there any centralized component in a MapReduce system?\"]}],[\"$\",\"p\",null,{\"children\":\"The MapReduce framework itself is not centralized; it operates in a distributed manner, coordinating various tasks across a cluster of nodes. Here's how it works:\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"JobTracker/ResourceManager\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"In the original Hadoop MapReduce (often referred to as MRv1), a central component called the JobTracker is responsible for managing the job and tracking the progress of each task.\"}],[\"$\",\"li\",null,{\"children\":\"In the newer Hadoop YARN (Yet Another Resource Negotiator) architecture, which is part of Hadoop 2.x and later (often referred to as MRv2), the ResourceManager is the central authority that manages resources and schedules jobs.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"TaskTrackers/NodeManagers\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"In MRv1, TaskTrackers are distributed across nodes in the cluster. They execute the map and reduce tasks as assigned by the JobTracker.\"}],[\"$\",\"li\",null,{\"children\":\"In YARN, NodeManagers run on each node and are responsible for launching and monitoring containers that execute the map and reduce tasks.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"MapReduce Job Execution\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"When a MapReduce job is submitted, the JobTracker/ResourceManager breaks it down into smaller tasks (map tasks and reduce tasks).\"}],[\"$\",\"li\",null,{\"children\":\"These tasks are distributed across the cluster to the nodes where the data is stored (data locality principle).\"}],[\"$\",\"li\",null,{\"children\":\"The map tasks process the input data and produce intermediate key-value pairs.\"}],[\"$\",\"li\",null,{\"children\":\"The shuffle and sort phase involves transferring intermediate data across the network to the nodes running the reduce tasks.\"}],[\"$\",\"li\",null,{\"children\":\"Reduce tasks process the intermediate data and produce the final output.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Fault Tolerance\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"The framework handles node failures by reassigning tasks to other nodes and ensures that the job completes successfully.\"}]}]]}]]}],[\"$\",\"p\",null,{\"children\":\"To summarize, while there are centralized components (JobTracker in MRv1 or ResourceManager in YARN) responsible for coordinating the overall job execution and resource management, the actual processing is distributed across the cluster. The MapReduce framework orchestrates this distributed processing to ensure efficient and reliable job completion.\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"does-the-resourcemanager-in-hadoop-support-the-master-slave-mechanism\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#does-the-resourcemanager-in-hadoop-support-the-master-slave-mechanism\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Does the ResourceManager in Hadoop support the master-slave mechanism?\"]}],[\"$\",\"p\",null,{\"children\":\"The ResourceManager in Hadoop YARN (Yet Another Resource Negotiator) is indeed a centralized component, but it operates within a master-slave architecture to provide high availability and fault tolerance. Here’s a detailed breakdown:\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"ResourceManager (RM)\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"The ResourceManager is the central authority responsible for resource allocation and job scheduling in a YARN cluster.\"}],[\"$\",\"li\",null,{\"children\":\"It consists of several components, including the Scheduler and the ApplicationManager.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"High Availability\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"To avoid a single point of failure, YARN can be configured for high availability (HA). In an HA setup, there are typically two ResourceManagers: an active ResourceManager and a standby ResourceManager.\"}],[\"$\",\"li\",null,{\"children\":\"The active RM handles all the resource management and job scheduling tasks, while the standby RM remains synchronized and can take over in case the active RM fails.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"NodeManagers (NMs)\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"NodeManagers are distributed across the cluster, running on each node.\"}],[\"$\",\"li\",null,{\"children\":\"They are responsible for launching and managing containers on the nodes, monitoring resource usage, and reporting back to the ResourceManager.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"ApplicationMaster (AM)\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"For each application submitted to the YARN cluster, an ApplicationMaster is launched.\"}],[\"$\",\"li\",null,{\"children\":\"The ApplicationMaster negotiates resources with the ResourceManager and works with the NodeManagers to execute and monitor tasks.\"}],[\"$\",\"li\",null,{\"children\":\"Each application has its own ApplicationMaster, which provides a level of decentralization for task management.\"}]]}]]}]]}],[\"$\",\"p\",null,{\"children\":\"Here's how it works in practice:\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Job Submission\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"When a job is submitted, the ResourceManager allocates a container for the ApplicationMaster of that job.\"}],[\"$\",\"li\",null,{\"children\":\"The ApplicationMaster then negotiates further resources with the ResourceManager to execute the map and reduce tasks.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Resource Allocation\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"The ResourceManager’s Scheduler allocates resources based on various policies (e.g., capacity, fairness).\"}],[\"$\",\"li\",null,{\"children\":\"NodeManagers communicate with the ResourceManager to report resource availability and task statuses.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Task Execution\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"The ApplicationMaster requests containers from the ResourceManager to run the tasks.\"}],[\"$\",\"li\",null,{\"children\":\"NodeManagers launch the containers and manage their execution.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Fault Tolerance\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"The ResourceManager ensures high availability with an active-standby configuration.\"}],[\"$\",\"li\",null,{\"children\":\"The ApplicationMaster can handle task failures by requesting new containers and re-executing failed tasks.\"}]]}]]}]]}],[\"$\",\"p\",null,{\"children\":\"In summary, while the ResourceManager is a centralized component in YARN, it operates within a master-slave architecture with mechanisms for high availability and decentralization of task management through ApplicationMasters and NodeManagers.\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"question-will-the-network-throughput-be-the-significant-bottleneck-in-the-mapreduce-job\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#question-will-the-network-throughput-be-the-significant-bottleneck-in-the-mapreduce-job\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Question: Will the network throughput be the significant bottleneck in the MapReduce job?\"]}],[\"$\",\"p\",null,{\"children\":\"Network throughput can indeed be a significant bottleneck in a MapReduce job, especially during the shuffle and sort phase where large volumes of intermediate data are transferred between nodes. To mitigate this, several strategies are employed:\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Data Locality\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"MapReduce frameworks try to schedule map tasks on the nodes where the data is stored or as close to those nodes as possible. This minimizes the need to transfer large amounts of input data over the network.\"}]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Intermediate Data Storage\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"Intermediate data generated by map tasks is often stored locally on the map node's disk. This local storage reduces the amount of data that needs to be transferred over the network initially.\"}]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Cluster Topology Awareness\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"The framework is aware of the cluster topology, such as the network bandwidth between racks and nodes. This awareness helps in optimizing data transfer during the shuffle and sort phase by grouping and scheduling tasks to minimize cross-rack network traffic.\"}]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Combiner Function\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"A combiner function can be used to perform local aggregation of intermediate data on the map node before sending it over the network. This reduces the volume of data that needs to be shuffled.\"}]}]]}]]}],[\"$\",\"p\",null,{\"children\":\"Regarding your question about Google setting GFS (Google File System) and MapReduce workers on the same physical machines:\"}],[\"$\",\"p\",null,{\"children\":\"Yes, Google’s original MapReduce implementation indeed places both GFS (or later Colossus, the successor to GFS) and MapReduce workers on the same physical machines. This co-location is done to:\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Improve Data Locality\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"By placing data storage (GFS) and data processing (MapReduce workers) on the same machines, the framework can leverage data locality to reduce network traffic. Map tasks can read input data directly from the local disk without needing to transfer it over the network.\"}]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Optimize Resource Utilization\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"This setup ensures better utilization of the cluster's resources. Machines are used for both storage and computation, balancing the workload and avoiding the need for a separate storage cluster and a separate compute cluster.\"}]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Reduce Latency\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"Co-locating storage and compute reduces the latency associated with data access, as the data is readily available on the same machine or at least within the same rack, minimizing the distance it needs to travel.\"}]}]]}]]}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"is-the-data-locality-a-major-consideration-nowadays\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#is-the-data-locality-a-major-consideration-nowadays\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Is the data locality a major consideration nowadays?\"]}],[\"$\",\"p\",null,{\"children\":\"Google's design for its distributed systems, including GFS (Google File System) and MapReduce, has evolved over time. However, the principle of co-locating storage and compute resources has remained a key strategy for optimizing data locality and minimizing network bottlenecks.\"}],[\"$\",\"p\",null,{\"children\":\"In the original MapReduce and GFS setup, co-locating the distributed file system and MapReduce workers on the same physical machines was a fundamental design choice. This approach improved performance by leveraging data locality, reducing the need to transfer large amounts of data over the network.\"}],[\"$\",\"p\",null,{\"children\":\"With advancements and the development of new systems like Colossus (the successor to GFS) and other distributed computing frameworks, Google has continued to optimize these principles. Some of the improvements and advancements include:\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Colossus (GFS successor)\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Colossus provides improvements in scalability, performance, and reliability over the original GFS. It is designed to handle even larger datasets and more complex workloads.\"}],[\"$\",\"li\",null,{\"children\":\"Colossus still emphasizes data locality to reduce network traffic, so co-locating storage and compute remains a beneficial practice.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Borg and Kubernetes\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Borg, Google’s cluster management system, and its successor Kubernetes, also emphasize efficient resource management and scheduling. These systems manage both storage and compute resources across large clusters.\"}],[\"$\",\"li\",null,{\"children\":\"Kubernetes, widely adopted outside of Google, provides mechanisms for co-locating pods (containers) with data, emphasizing locality and reducing network overhead.\"}]]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Cloud Infrastructure\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"In cloud environments like Google Cloud Platform (GCP), services such as Google Cloud Storage (GCS) and Google BigQuery provide highly optimized data storage and processing capabilities.\"}],[\"$\",\"li\",null,{\"children\":\"While these services may separate storage and compute physically, they are interconnected with high-speed, low-latency networks to minimize the performance impact. Data locality and efficient data transfer remain critical considerations.\"}]]}]]}]]}],[\"$\",\"p\",null,{\"children\":\"In summary, while the exact implementations and technologies have evolved, the principle of optimizing data locality by co-locating storage and compute resources continues to be a key strategy for enhancing performance and reducing network bottlenecks. Google’s infrastructure improvements have built on these principles, offering more advanced and scalable solutions for handling distributed data processing tasks.\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"why-does-mapreduce-use-batch-processing-instead-of-stream-processing-in-the-reduce-phase-allowing-reduce-nodes-to-avoid-waiting-for-all-map-nodes-to-finish-processing\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#why-does-mapreduce-use-batch-processing-instead-of-stream-processing-in-the-reduce-phase-allowing-reduce-nodes-to-avoid-waiting-for-all-map-nodes-to-finish-processing\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Why does MapReduce use batch processing instead of stream processing in the reduce phase, allowing reduce nodes to avoid waiting for all map nodes to finish processing?\"]}],[\"$\",\"p\",null,{\"children\":\"MapReduce, as a programming model, is inherently designed around batch processing. This design choice affects the reduce phase and how it handles data. Here are the main reasons why MapReduce typically doesn't use stream processing in the reduce phase:\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Synchronization and Completeness\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"The reduce phase needs to ensure that all intermediate data generated by the map phase is available before it begins. This requirement ensures that each reducer receives a complete set of data for each key it is responsible for. If reducers started processing data as it arrived (i.e., stream processing), there is a risk of incomplete data, leading to incorrect results.\"}]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Data Shuffling\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"In MapReduce, there is a critical step called the shuffle and sort phase that occurs between the map and reduce phases. During this step, the framework sorts and groups the intermediate data by key. Stream processing would complicate this step because it would require continuous sorting and grouping as data arrives, which is much more complex and less efficient compared to batch processing.\"}]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Fault Tolerance\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"MapReduce is designed to handle faults gracefully. If a map task fails, it can be re-executed without affecting the reduce tasks since reduce tasks only start after all map tasks are completed. This batch processing approach simplifies fault tolerance because the system can ensure that all necessary data is available and correct before reducers start processing.\"}]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Resource Optimization\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"By waiting until all map tasks are complete before starting the reduce phase, the system can optimize resource allocation. Resources can be dynamically allocated to map tasks first, and once they complete, those resources can be reassigned to reduce tasks. This staged approach can lead to better utilization of cluster resources.\"}]}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Implementation Simplicity\"}],\":\"]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":\"The original MapReduce model was designed with simplicity in mind. Batch processing in the reduce phase simplifies the implementation and logic of the MapReduce framework. Streaming data processing would require a more complex implementation to handle continuous data flow, synchronization issues, and potential partial results.\"}]}]]}]]}],[\"$\",\"p\",null,{\"children\":\"While MapReduce uses batch processing in the reduce phase for the reasons mentioned above, there are other frameworks and models designed specifically for stream processing, such as Apache Storm, Apache Flink, and Kafka Streams, which can handle continuous data streams and provide real-time processing capabilities. These frameworks address the challenges of stream processing and are better suited for scenarios where real-time data processing is required.\"}]]}]}],[\"$\",\"footer\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"divide-gray-200 text-sm font-medium leading-5 dark:divide-gray-700 xl:col-start-1 xl:row-start-2 xl:divide-y\",\"children\":[[\"$\",\"div\",null,{\"className\":\"py-4 xl:py-8\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xs uppercase tracking-wide text-gray-500 dark:text-gray-400\",\"children\":\"Tags\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap\",\"children\":[[\"$\",\"$Le\",null,{\"href\":\"/tags/distributed-systems\",\"className\":\"mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"children\":\"distributed-systems\"}],[\"$\",\"$Le\",null,{\"href\":\"/tags/algorithms\",\"className\":\"mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"children\":\"algorithms\"}],[\"$\",\"$Le\",null,{\"href\":\"/tags/mapreduce\",\"className\":\"mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"children\":\"mapreduce\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"flex justify-between py-4 xl:block xl:space-y-8 xl:py-8\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xs uppercase tracking-wide text-gray-500 dark:text-gray-400\",\"children\":\"Previous Article\"}],[\"$\",\"div\",null,{\"className\":\"text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"children\":[\"$\",\"$Le\",null,{\"className\":\"break-words\",\"href\":\"/blog/distributed-systems/systems/GFS\",\"children\":\"GFS\"}]}]]}],\"$undefined\"]}]]}],[\"$\",\"div\",null,{\"className\":\"pt-4 xl:pt-8\",\"children\":[\"$\",\"$Le\",null,{\"className\":\"text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"href\":\"/blog\",\"aria-label\":\"Back to the projects\",\"children\":\"← Back to the projects\"}]}]]}]]}]]}]}]]}]]\n"])</script><script>self.__next_f.push([1,"12:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"MapReduce | CodeWisdom\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Explores the MapReduce algorithm.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"5\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"6\",{\"rel\":\"canonical\",\"href\":\"https://codewisdom.io/blog/distributed-systems/algorithms/mapreduce\"}],[\"$\",\"link\",\"7\",{\"rel\":\"alternate\",\"type\":\"application/rss+xml\",\"href\":\"https://codewisdom.io/feed.xml\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:title\",\"content\":\"MapReduce\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:description\",\"content\":\"Explores the MapReduce algorithm.\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:url\",\"content\":\"https://codewisdom.io/blog/distributed-systems/algorithms/mapreduce\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:site_name\",\"content\":\"CodeWisdom\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:image\",\"content\":\"https://codewisdom.io/static/images/logo-round.png\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"15\",{\"property\":\"article:published_time\",\"content\":\"2024-10-31T00:00:00.000Z\"}],[\"$\",\"meta\",\"16\",{\"property\":\"article:modified_time\",\"content\":\"2024-10-31T00:00:00.000Z\"}],[\"$\",\"meta\",\"17\",{\"property\":\"article:author\",\"content\":\"Bowen Y\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:title\",\"content\":\"MapReduce\"}],[\"$\",\"meta\",\"20\",{\"name\":\"twitter:description\",\"content\":\"Explores the MapReduce algorithm.\"}],[\"$\",\"meta\",\"21\",{\"name\":\"twitter:image\",\"content\":\"https://codewisdom.io/static/images/logo-round.png\"}],[\"$\",\"meta\",\"22\",{\"name\":\"next-size-adjust\"}]]\n6:null\n"])</script></body></html>